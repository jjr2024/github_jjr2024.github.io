[
  {
    "objectID": "posts/perceptron-algo/perceptron-algo.html",
    "href": "posts/perceptron-algo/perceptron-algo.html",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "In this blog post, I implement and test the perceptron algorithm using randomly generated data. The algorithm attempts to categorize data points but is only able reach a loss of zero if the data is linearly separable. I implement the minibatch perceptron algorithm as well, which computes weight updates using multiple observations at a time. For linearly separable data, I find it’s possible to consistently achieve a loss of 0. For data that is not linearly separable, I don’t achieve a loss of 0 with the minibatch perceptron algorithm (where number of observations = number of points used to update at once) but instead see a leveling out at a loss of around 0.28.\nLink to Perceptron Python Code: https://github.com/jjr2024/github_jjr2024.github.io/blob/main/posts/perceptron-algo/perceptron.py\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\nThe below code, from our warmup, defines a function that generates a matrix and an array of random data.\n\nimport torch\nfrom matplotlib import pyplot as plt\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,2))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\nprint(y)\n\ntensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1])\n\n\nThe below code is a quick check to ensure the Perceptron code was written correctly. The output of tensor(True) shows Perceptron is probably fine.\n\np = Perceptron()\ns = p.score(X)\nl = p.loss(X, y)\nprint(l)\nprint(l == 0.5)\n\ntensor(0.5000)\ntensor(True)\n\n\nThe below code is our core training loop as a function. As long as the loss exceeds our loss threshold (which by default is 0), we perform the optimizer’s step function to update the weights and then try again. We only iterate up to our max iterations value, which is 1000 times by default.\nIf our final loss vector (loss_vec) value is 0, our program successfully found a weight vector that linearly separates our data.\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\nloss_vec = []\n\ndef train(X, y, max_iterations = 1000, loss_threshold = 0.0):\n    loss = 1.0\n\n    # for keeping track of loss values and number of iterations\n    j = 0\n    n = X.size()[0]\n\n    while loss &gt; loss_threshold and j &lt; max_iterations: # dangerous -- only terminates if data is linearly separable\n        j += 1\n        loss = p.loss(X, y) \n        loss_vec.append(loss)\n        \n        # pick a random data point\n        i = torch.randint(n, size = (1,))\n        x_i = X[[i],:]\n        y_i = y[i]\n        \n        # perform a perceptron update using the random data point\n        opt.step(x_i, y_i)\n    return loss_vec\n\nloss_vec = train(X,y)\nprint(\"Last Vector: \" + str(loss_vec[len(loss_vec)-1]))\nprint(p.w)\n\nLast Vector: tensor(0.)\ntensor([ 2.5406,  0.9279, -1.8141])\n\n\nThe above output shows the final vector in loss_vec is a 0, showing we managed to find a set of weights that minimize the loss. The weights are shown above below the last vector in loss_vec."
  },
  {
    "objectID": "posts/perceptron-algo/perceptron-algo.html#introduction",
    "href": "posts/perceptron-algo/perceptron-algo.html#introduction",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "In this blog post, I implement and test the perceptron algorithm using randomly generated data. The algorithm attempts to categorize data points but is only able reach a loss of zero if the data is linearly separable. I implement the minibatch perceptron algorithm as well, which computes weight updates using multiple observations at a time. For linearly separable data, I find it’s possible to consistently achieve a loss of 0. For data that is not linearly separable, I don’t achieve a loss of 0 with the minibatch perceptron algorithm (where number of observations = number of points used to update at once) but instead see a leveling out at a loss of around 0.28.\nLink to Perceptron Python Code: https://github.com/jjr2024/github_jjr2024.github.io/blob/main/posts/perceptron-algo/perceptron.py\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\nThe below code, from our warmup, defines a function that generates a matrix and an array of random data.\n\nimport torch\nfrom matplotlib import pyplot as plt\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,2))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\nprint(y)\n\ntensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1])\n\n\nThe below code is a quick check to ensure the Perceptron code was written correctly. The output of tensor(True) shows Perceptron is probably fine.\n\np = Perceptron()\ns = p.score(X)\nl = p.loss(X, y)\nprint(l)\nprint(l == 0.5)\n\ntensor(0.5000)\ntensor(True)\n\n\nThe below code is our core training loop as a function. As long as the loss exceeds our loss threshold (which by default is 0), we perform the optimizer’s step function to update the weights and then try again. We only iterate up to our max iterations value, which is 1000 times by default.\nIf our final loss vector (loss_vec) value is 0, our program successfully found a weight vector that linearly separates our data.\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\nloss_vec = []\n\ndef train(X, y, max_iterations = 1000, loss_threshold = 0.0):\n    loss = 1.0\n\n    # for keeping track of loss values and number of iterations\n    j = 0\n    n = X.size()[0]\n\n    while loss &gt; loss_threshold and j &lt; max_iterations: # dangerous -- only terminates if data is linearly separable\n        j += 1\n        loss = p.loss(X, y) \n        loss_vec.append(loss)\n        \n        # pick a random data point\n        i = torch.randint(n, size = (1,))\n        x_i = X[[i],:]\n        y_i = y[i]\n        \n        # perform a perceptron update using the random data point\n        opt.step(x_i, y_i)\n    return loss_vec\n\nloss_vec = train(X,y)\nprint(\"Last Vector: \" + str(loss_vec[len(loss_vec)-1]))\nprint(p.w)\n\nLast Vector: tensor(0.)\ntensor([ 2.5406,  0.9279, -1.8141])\n\n\nThe above output shows the final vector in loss_vec is a 0, showing we managed to find a set of weights that minimize the loss. The weights are shown above below the last vector in loss_vec."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/logistic-regression/logistic-regression.html",
    "href": "posts/logistic-regression/logistic-regression.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\nimport torch\nfrom matplotlib import pyplot as plt\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nLink to logistic.py on GitHub: https://github.com/jjr2024/github_jjr2024.github.io/blob/main/posts/logistic-regression/logistic.py"
  },
  {
    "objectID": "posts/logistic-regression/logistic-regression.html#introduction",
    "href": "posts/logistic-regression/logistic-regression.html#introduction",
    "title": "Implementing Logistic Regression",
    "section": "Introduction",
    "text": "Introduction\nIn this blog post, I drop down a level of abstraction from prior work in the class to implement the logistic regression model. I run multiple experiments to confirm the validity of my implementation and to see how the model reacts to changes in hyperparameters (specifically beta and number of features). I find that beta &gt; 0 allows for a faster convergence to a zero or near-zero loss and that when the number of features &gt; number of observations, the model is prone to overfitting."
  },
  {
    "objectID": "posts/logistic-regression/logistic-regression.html#part-a-implement-logistic-regression",
    "href": "posts/logistic-regression/logistic-regression.html#part-a-implement-logistic-regression",
    "title": "Implementing Logistic Regression",
    "section": "Part A: Implement Logistic Regression",
    "text": "Part A: Implement Logistic Regression\nThe below function generates data for a classication problem: n_points controls the number of observations generated. Noise determines the difficulty of the problem. p_dims controls the number of features.\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nThe below defines a function, train(), that trains the logistic regression model on X, y inputs and keeps track of the evolution of the loss over time via the loss_vec. Train() has parameters for the maximum number of update iterations (max_iterations), as well as for the learning rate (a) and the momentum rate (b).\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nloss_vec = []\n\nX, y = classification_data(n_points = 100, noise = 0.1)\n\ndef train(X, y, max_iterations = 1000, a = 0.1, b = 0.9):\n    for _ in range(max_iterations):\n        loss = LR.loss(X, y) \n        loss_vec.append(loss)\n        opt.step(X, y, a, b)\n\ntrain(X,y,1000,0.1,0.9)\n\nThe below is code defining two functions that assist in visualizing our results. plot_perceptron_data and draw_line visualize our datapoints and our weight vector, respectively.\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nplot_perceptron_data(X, y, ax)\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\ndraw_line(LR.w, 0, 1, ax, color = \"black\", linestyle = \"dashed\", label = \"weight vector w\")"
  },
  {
    "objectID": "posts/logistic-regression/logistic-regression.html#part-b-experiments",
    "href": "posts/logistic-regression/logistic-regression.html#part-b-experiments",
    "title": "Implementing Logistic Regression",
    "section": "Part B: Experiments",
    "text": "Part B: Experiments\n\nExperiment 1: Vanilla Gradient Descent\nIn this first experiment, our data has two features, alpha is small (0.2), and beta = 0. This experiment confirms that our code in Part A is likely correct.\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nloss_vec = []\n\nX, y = classification_data(n_points = 100, noise = 0.1, p_dims = 2)\n\ntrain(X,y,1000,a=0.2,b=0.0)\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"Loss\")\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(1, 1)\nplot_perceptron_data(X, y, ax)\ndraw_line(LR.w, 0, 1, ax, color = \"black\", linestyle = \"dashed\", label = \"weight vector w\")\n\n\n\n\n\n\n\n\nThe above visualization shows that our data is linearly separable and that our weight vector correctly classifies the data with 100% accuracy.\n\n\nExperiment 2: Benefits of Momentum\nIn this experiment, we set beta = 0.9 to see the benefits of momentum, with which we need fewer iterations to approach a loss of 0.\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nloss_vec2 = loss_vec\n\nloss_vec = []\n\n#No classification_data call so that we use same dataset\n\ntrain(X,y,1000,a=0.2,b=0.9)\n\nplt.plot(loss_vec, color = \"blue\")\nplt.plot(loss_vec2, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"blue\")\nplt.scatter(torch.arange(len(loss_vec2)), loss_vec2, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"Loss\")\n\n\n\n\n\n\n\n\nThe blue line indicates our loss vector when we train our model with b = 0.9. The grey line is the loss vector from Experiment 1. We see early on that momentum can work against us initially, but ultimately does lead to a faster zero or near-zero loss.\n\nfig, ax = plt.subplots(1, 1)\nplot_perceptron_data(X, y, ax)\ndraw_line(LR.w, 0, 1, ax, color = \"black\", linestyle = \"dashed\", label = \"weight vector w\")\n\n\n\n\n\n\n\n\nAgain we achieve 100% accuracy in classification, as shown by our weight vector accurately separating the two groups of data.\n\n\nExperiment 3: Overfitting\nIn this experiment, we set the number of observations (15) to be half the number of features (30). We also create two sets of classification data, each with 15 observations, to see how our logistic regression model overfits to the training data and sees a deterioration in accuracy on our test data.\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nloss_vec = []\n\nX_train, y_train = classification_data(n_points = 15, noise = 0.5, p_dims = 30)\nX_test, y_test = classification_data(n_points = 15, noise = 0.5, p_dims = 30)\n\n\ntrain(X_train,y_train,100,a=0.1,b=0.0)\n\n(LR.predict(X_train) == y_train).float().mean(), (LR.predict(X_test) == y_test).float().mean()\n\n(tensor(1.), tensor(1.))\n\n\nAbove, the first tensor, 1., reflects our 100% accuracy on our training. The second tensor reflects 93.33% accuracy in our testing data set.\nThe difference in accuracy rates reflects our model’s overfitting on the training set."
  },
  {
    "objectID": "posts/logistic-regression/logistic-regression.html#part-c-writing-conclusion",
    "href": "posts/logistic-regression/logistic-regression.html#part-c-writing-conclusion",
    "title": "Implementing Logistic Regression",
    "section": "Part C: Writing / Conclusion",
    "text": "Part C: Writing / Conclusion\nIn this blog post, I run experiments to confirm the validity of my implementation and to see how the model reacts to changes in hyperparameters (specifically beta and number of features). I find that beta &gt; 0 allows for a faster convergence to a zero or near-zero loss and that when the number of features &gt; number of observations, the model is prone to overfitting.\nIn writing this blog post, I learned about why momentum is useful for gradient descent and why in practice beta = 0.9 is common. I gained more familiarity turning mathematical equations into code. I had a more efficient process than previous posts because of more effective debugging, especially by knowing what to look for (the shape() function to different tensors was extremely useful to know when I did something wrong)."
  },
  {
    "objectID": "posts/final-project/quant_research.html",
    "href": "posts/final-project/quant_research.html",
    "title": "Quantitative Trading Model Using LSTM",
    "section": "",
    "text": "We applied machine learning methods to predict daily stock price movements in a basket of 10 US-listed energy companies. We found the most success using an LSTM model, achieving an accuracy of up to 61% on one stock (PSX). In line with prior literature, we compared our results to a benchmark established by a last value machine, which simply predicts the next day’s price to be the current day’s actual price. We tested our model on two different sets of observations, first on a year’s worth of historical data (historical test) and then on one week of recent trading data (live test). Comparing our LSTM results to our benchmark, we find mixed results. We achieve, on average, 53.57% accuracy vs. our benchmark’s 53.08% accuracy on the historical test. For the ten companies in our analysis, our model has superior for accuracy for 6 companies, has equal accuracy for 1 company, and has worse accuracy for 3 companies compared to our last value benchmark. This superior accuracy leads to higher simulated portfolio returns using our model compared to our benchmarks. Our live test accuracy of 49.99% was worse but still beat our benchmark’s accuracy of 48.33%.\n\n\n\nIn this blog post, we train machine learning models on historical stock market data to predict future stock price movements. This is a highly popular problem to address because of the potential for significant monetary gain. This is an important problem societally because stock markets are mechanisms of price discovery: they answer the question “What is a company worth?” Finding the right answer to that question allows society to correctly allocate more or less capital (money) to that company. On an individual level, this is an important problem to us as the authors because it’s the problem for all quant trading: making a profitable model.\nAn enormous body of literature within and without computer science exists for stock market prediction. Among the papers most relevant to our work are Gunduz (2021), Bhandari et al. (2022), and Zhang (2022).\nGunduz (2021) applies LSTM and ensemble learning (Light-GBM) models to predict the hourly directions of eight banking stocks in Borsa Istanbul. He achieved up to maximum success rate of 0.685 using individual features of bank stocks and LSTM.\nBhandari et al. (2022) apply single-layer and multi-layer LSTM models to the problem of predicting the S&P 500, the index of the largest 500 publicly traded companies in America. Their single-layer LTSM model with 150 neurons is their best performing specification. Their set of predicted values have an average correlation coefficient of 0.9976 with actual S&P index values.\nZhang (2022) finds the LSTM network model does not perform better than other models when applied to a short forecasting horizon (1 to 10 days). Zhang’s “other models” are linear regression, eXtreme gradient boosting (XGBoost), last value, and moving average.\nWe take some of the “best practices” we observe in the above papers, specifically benchmarking with last value and using MSE and R to assess our results. Unlike the mentioned papers, we will be focusing on single stocks and attempting to build a model that outperforms the last value benchmark.\n\n\n\nThe potential users are anyone interested in making profitable trades in the stock market. They are the individuals most likely to directly benefit from our work. Nonusers who could be affected by our work are those engaged in the stock market. The obvious affected nonusers are those on the opposite side of each trade as a user. In every trade, there’s a buyer and a seller, so in every trade, there’s a winner and a loser. These opposing nonusers are the individuals who are most likely to be harmed by the success of our program.\nUltimately, the point of the back and forth of markets is price discovery: to help society find the right prices of different companies. This leads to another nonuser effect: with better price discovery and more efficient markets, companies will raise money at prices that are closer to some “true” value, which is loosely defined as a value that best reflects the fundamental valuation of the company. Our model does not attempt to predict a true fundamental value for a company, but by making accurate predictions for the next day’s price, it should accelerate the market’s convergence to an appropriate value.\nA useful financial trading model should lead to a net societal benefit because better financial markets mean more or less money going to companies and therefore projects, leading to something closer to an “optimal” allocation of money in society.\nWe are personally motivated to work on this project because of personal interest, professional relevance, and the difficulty of the problem. All three of us personally invest in the stock market. Two of us (Donovan & James) are double majors in economics and have had experience working in the financial services industry. Andre is interested in pursuing a master’s in financial engineering after Middlebury. The problem itself is also inherently challenging: financial markets are constantly adapting and changing, making the findings of previous literature increasingly likely over time to be less applicable to today’s markets. This forces us to adopt new techniques. # Materials and Methods\n\n\n\nOur data was sourced from Yahoo Finance. We used the yfinance library to download historical stock price data for our 10 different stocks. We chose to focus on US-based oil companies. These companies are Exxon Mobil (XOM), Chevron (CVX), ConocoPhillips (COP), Enterprise Products Partners (EPD), Pioneer Natural Resources (PXD), EOG Resources (EOG), Duke Energy (DUK), Marathon Petroleum (MPC), Schlumberger (SLB), and Phillips 66 (PSX). We downloaded the data from May 6th, 2014 to May 6th, 2024.\nWithin the yfinance dataset we were given the following columns: Open, High, Low, Close, Adj Close, Volume.\nOpen is the opening price of the stock for the day. High is the highest price of the stock for the day. Low is the lowest price of the stock for the day. Close is the closing price of the stock for the day. Adj Close is the adjusted closing price of the stock for the day. Volume is the number of shares traded for the day.\nWe used the Close column as our target variable for our model. We also created the following features: SMA_20, SMA_50, Std_Dev, Z_Score, RSI, TTM_P/E which will be discussed below. Here’s a look at what the raw data looks like:\n\nimport yfinance as yf\n\nxom = yf.Ticker('XOM')\ndata = xom.history(start='2014-05-06', end='2024-05-07')\ndata.head()\n\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nVolume\nDividends\nStock Splits\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n2014-05-06 00:00:00-04:00\n66.050586\n66.501227\n65.928274\n66.095650\n9669800\n0.00\n0.0\n\n\n2014-05-07 00:00:00-04:00\n66.385376\n66.597816\n66.172931\n66.378937\n11007400\n0.00\n0.0\n\n\n2014-05-08 00:00:00-04:00\n66.366038\n66.494794\n65.773772\n65.870338\n8922500\n0.00\n0.0\n\n\n2014-05-09 00:00:00-04:00\n65.922184\n66.226810\n65.630524\n66.077736\n8948800\n0.69\n0.0\n\n\n2014-05-12 00:00:00-04:00\n66.324029\n66.336990\n65.805516\n66.259216\n8830500\n0.00\n0.0\n\n\n\n\n\n\n\n\nYou can find the full implementation of our data at lstm_data.py under the function prepare_data().\n\n\n\n\n\nWe used SMA_20, SMA_50, Std_Dev, Z_Score, RSI, Close, TTM_P/E as predictors for our models.\nThe SMA_20 and SMA_50 are the 20-day and 50-day simple moving averages of the stock price. This means that the average closing price of the stock over the last 20 and 50 days, respectively.\nThe Std_Dev is the standard deviation of the stock price meaning how much the stock price deviates from the mean.\nThe Z_Score is the z-score of the stock price meaning how many standard deviations the stock price is from the mean.\nThe RSI is the relative strength index of the stock price meaning how strong the stock price is relative to its past performance. It is calculated by taking the average of the gains and losses over a certain period of time.\nThe Close is the closing price of the stock per day.\nThe TTM_P/E is the trailing twelve months price-to-earnings ratio of the stock.\nWe used the next day’s Close price as the target variable for our model.\n\n\n\nWe collected 10 years of data from May 7th, 2014 to May 7th, 2024 and used a train-test split of 90-10 in order to train our model on the first 9 years worth of the data and test it on the remaining 1 year’s worth of data. We used a standard scaler for scaling our data in order to ensure that the data was normalized. We fit the scaler on the training data and then applied it to the test data to avoid any information leaking. We then combined the training data for each stock into one dataset. We used the closing price of the stock as the target variable for our model.\nHere’s what our data looks like after creating our features and scaling the data:\n\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\ntickers = ['XOM', 'CVX', 'COP', 'EPD', 'EOG', 'DUK', 'MPC', 'SLB', 'PSX', 'OXY']\nstart = '2014-05-06'\nend = '2024-05-07'\n\n# preps data, see lstm_data.py, prints size of each ticker's dataset\nX_train, y_train, X_test, y_test, X_scalers, y_scalers, batch_size = prepare_data(tickers, start_date=start, end_date=end, test_size=0.1)\nX_train\n\n(2041, 18)\n(2041, 18)\n(2041, 18)\n(2041, 18)\n(2041, 18)\n(2041, 18)\n(2041, 18)\n(2041, 18)\n(2041, 18)\n(2041, 18)\n\n\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nVolume\nDividends\nStock Splits\nSMA_10\nSMA_20\nSMA_50\nSMA_100\nSMA_250\nStd_Dev\nZ_Score\nRSI\nTTM_EPS\nTTM_P/E\nReturns\nTicker\n\n\n\n\n2015-05-01\n-0.139877\n-0.131185\n-0.112171\n-0.102371\n-0.540891\n-0.127836\n0.00000\n-0.149377\n-0.173825\n-0.178037\n-0.063809\n0.319177\n-0.679901\n1.186546\n0.879810\n0.650885\n-0.095652\n0.893759\nXOM\n\n\n2015-05-04\n-0.080169\n-0.096257\n-0.064154\n-0.092513\n-0.732495\n-0.127836\n0.00000\n-0.142218\n-0.166130\n-0.178686\n-0.064625\n0.316818\n-0.653451\n1.177769\n0.973560\n0.650885\n-0.095424\n0.133748\nXOM\n\n\n2015-05-05\n-0.059251\n-0.088369\n-0.080672\n-0.111473\n-0.577151\n-0.127836\n0.00000\n-0.135595\n-0.160600\n-0.179007\n-0.065519\n0.314254\n-0.642313\n0.791839\n0.672019\n0.650885\n-0.095861\n-0.329225\nXOM\n\n\n2015-05-06\n-0.071421\n-0.093251\n-0.108714\n-0.127400\n-0.639387\n-0.127836\n0.00000\n-0.132800\n-0.152615\n-0.179992\n-0.065441\n0.311766\n-0.751693\n0.518627\n0.427504\n0.650885\n-0.096229\n-0.281992\nXOM\n\n\n2015-05-07\n-0.134173\n-0.163112\n-0.142135\n-0.149774\n-0.767664\n-0.127836\n0.00000\n-0.132571\n-0.146911\n-0.181595\n-0.065840\n0.309079\n-0.859104\n0.047923\n0.100192\n0.650885\n-0.096744\n-0.387753\nXOM\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-06-02\n0.610803\n0.625792\n0.636749\n0.650958\n0.109535\n-0.105084\n-0.02214\n0.607044\n0.598059\n0.727834\n0.786385\n0.982249\n-0.739053\n0.869582\n0.136220\n1.924677\n-0.222355\n0.810058\nOXY\n\n\n2023-06-05\n0.714326\n0.672373\n0.676732\n0.633996\n-0.312158\n-0.105084\n-0.02214\n0.608629\n0.594259\n0.728931\n0.783608\n0.979744\n-0.834725\n0.776482\n0.011823\n1.924677\n-0.222464\n-0.165726\nOXY\n\n\n2023-06-06\n0.590332\n0.602501\n0.623814\n0.636336\n-0.325573\n-0.105084\n-0.02214\n0.610977\n0.595673\n0.730434\n0.780891\n0.977007\n-0.819762\n0.773031\n0.029482\n1.924677\n-0.222449\n0.003637\nOXY\n\n\n2023-06-07\n0.652329\n0.667133\n0.694371\n0.671429\n-0.292849\n-0.105084\n-0.02214\n0.613090\n0.598854\n0.730900\n0.777538\n0.974819\n-0.772194\n1.238762\n0.294437\n1.924677\n-0.222223\n0.292064\nOXY\n\n\n2023-06-08\n0.669287\n0.645055\n0.635925\n0.662629\n-0.147267\n2.262466\n-0.02214\n0.616022\n0.607809\n0.728131\n0.774355\n0.973557\n-0.847555\n1.077118\n0.218085\n1.924677\n-0.222280\n-0.093700\nOXY\n\n\n\n\n20410 rows × 19 columns\n\n\n\n\n\n\n\nOriginally, we used rather simplistic models like logistic regression, Random Forest, and SVM in order to predict stock price movements. We utilized Recursive Feature Elimination (RFE) in order to determine the optimal features for prediction for each model. However, we found that these models were not able to predict stock price movements consistently with much accuracy. We then decided to use a Long Short-Term Memory (LSTM) model to predict stock price movements. LSTM models are a type of recurrent neural network (RNN) with the addition of “gates” notably the input, forget and output gates. These gates allow for the model to determine what information to retain or discard at each timestep, mitigating the vanishing descent issue found in traditional recurrent neural networks. The LSTM model accounts for the shortfalls of an RNN by capturing long-term dependencies in the data.\nThe forget gate determines which information is either retained or discarded at each time step. It accepts the output from the previous time step \\(h_{t-1}\\) and the input \\(x_t\\) at the current time step. The forget gate is defined as:\n\\[f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\\]\nThe input gate determines which information is stored in the cell state. It avoids feeding the unimportant information into the current memory cell. It has three different components:\n\nGetting the state of the cell that must be updated.\nCreate a new cell state\nUpdate the cell state to the current cell state\n\nThese are defined as:\n\\[\\begin{aligned}\ni_{t} &= \\sigma(W_{t} \\cdot [h_{t-1}, x_{t}] + b_{i}) \\\\\n\\widetilde{C}_{t} &= \\tanh(W_{c} \\cdot [h_{t-1}, x_{t}] + b_{c}) \\\\\nC_{t} &= f_{t} \\ast C_{t-1} + i_{t} \\ast \\widetilde{C}_{t}\n\\end{aligned}\\]\nThe output gate determines how much of the newly created cell state will be discarded and how much will be passed to the output. It is defined as:\n\\[o_{t} = \\sigma(W_{o} \\cdot [h_{t-1}, x_{t}] + b_{o})\\]\nThis output information is firstly determined by a sigmoid layer, then the newly created cell state is processed by a tanh layer. The output is then multiplied by the sigmoid layer to determine the final output of the LSTM cell.\nWhich is defined as:\n\\[h_{t} = o_{t} \\ast \\tanh(C_{t})\\]\nTaking this all into account, the LSTM model is able to retain information from previous time steps and use it to predict future stock price movements while disregarding irrelevant information.\nThe implementation of our LSTM model can be found at: lstm_model.py\n\n\n\nWe first converted our wanted feature columns into a torch Variable to allow them to be differentiable. Then, we reshaped the data using torch.reshape() and torch.utils.data.DataLoader into [batch_size, seq_len, input_size].\n\nfeatures = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'Close', 'TTM_P/E']\n\nX_train_tensors = Variable(torch.Tensor(np.array(X_train[features])))\ny_train_tensors = Variable(torch.Tensor(y_train.values))\nX_train_final = torch.reshape(X_train_tensors, (X_train_tensors.shape[0], 1, X_train_tensors.shape[1]))\n\n# split data by ticker\ndata_loader_train = torch.utils.data.DataLoader(\n    torch.utils.data.TensorDataset(X_train_final, y_train_tensors),\n    batch_size=batch_size,\n    shuffle=True\n)\n\nnext(iter(data_loader_train))[0].shape\n\ntorch.Size([2041, 1, 7])\n\n\nWe trained our model using our own personal devices. We used the Adam optimizer with a learning rate of 0.001. We trained the model for 1000 epochs for each stock in our dataset (10 total) and used the torch.nn.MSELoss() loss function to train the model.\nMSE is defined as:\n\\[MSE = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_{i} - y_{i})^2\\]\nWhere \\(y_{i}\\) is the true price and \\(\\hat{y}_{i}\\) is the predicted price.\nAs mentioned previously our model was trained on 90% of the data and tested on the remaining 10%.\nIf the model predicted the next days price to be positive, we would purchase the stock at the closing price and sell it at the closing price the next day. If the model predicted the next days price to be negative, we would short the stock at the closing price and buy it back at the closing price the next day. We would then calculate the profit or loss percent change for each stock and compare it to the last value benchmark.\nBelow is our training code:\n\nnum_epochs = 1000 # 1000 epochs\nlearning_rate = 0.001 # 0.001 lr\n\ninput_size = X_train_final.shape[2] # number of features\nhidden_size = 32 # number of features in hidden state\nnum_layers = 1 # number of stacked lstm layers\nwindow = 1 # number of windows, leave at 1, basically can ignore\n\nnum_classes = 1 # number of output classes\n\nlstm = LSTMModel(num_classes, input_size, hidden_size, num_layers, seq_length=window, batch_size=batch_size) #our lstm class \ncriterion = torch.nn.MSELoss()    # mean-squared error for regression\noptimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate) # ADAM optimizer\n\n# training loop\nfor epoch in range(num_epochs):\n  for i, data in enumerate(data_loader_train):\n    X_, y_ = data\n    outputs = lstm.forward(X_) #forward pass\n    optimizer.zero_grad() #calculate the gradient, manually setting to 0\n  \n    # obtain the loss function\n    loss = criterion(outputs, y_.reshape(y_.size(0)*y_.size(1), 1))\n  \n    loss.backward() #calculates the loss of the loss function\n  \n    optimizer.step() #improve from loss, i.e backprop\n    # if (i + 1) % 50 == 0:\n    #     print(f\"Epoch {epoch}, batch {i:&gt;3}, loss on batch: {loss.item():.3f}\")\n  if epoch % 100 == 0:\n    print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))\n\nEpoch: 0, loss: 0.91365\nEpoch: 100, loss: 0.00494\nEpoch: 200, loss: 0.00502\nEpoch: 300, loss: 0.00508\nEpoch: 400, loss: 0.00451\nEpoch: 500, loss: 0.00478\nEpoch: 600, loss: 0.00479\nEpoch: 700, loss: 0.00491\nEpoch: 800, loss: 0.00514\nEpoch: 900, loss: 0.00504\n\n\n\n\n\nWe evaluated our model by comparing the cumulative predicted stock price returns and accuracy to the actual cumulative stock price returns and accuracy and the cumulative last value benchmark returns and accuracy. The last value benchmark is defined as using the previous days value as the prediction for the current day. We would buy the stock at the current day’s close price and sell at the next day’s close price if the predicted returns were positive and do nothing if the predicted returns were negative. We followed the same principle in calculating actual cumulative stock returns and accuracy, and the cumulative last value benchmark returns and accuracy.\nWe define accuracy for our purposes as percentage of times the model correctly predicts an upward or downward movement in the share price of a company.\nConsider a simple test case where the model predicts the stock price to go up and the stock price actually goes up. In this case, the model is correct. If the model predicts the stock price to go up and the stock price actually goes down, the model is incorrect. We calculate the accuracy of the model by dividing the number of correct predictions by the total number of predictions.\nAccuracy per Stock\n\n\nCode\nfor i in range(10):\n    cum_strat_returns, cum_stock_returns, cum_lv_returns, accuracy, lv_accuracy, prediction_correl, lv_prediction_correl = evaluate_lstm(lstm, X_test[i], y_test[i], X_scalers[i], y_scalers[i], features)\n    if i == 0:\n        cum_strat_returns_list = np.array([cum_strat_returns])\n        cum_stock_returns_list = np.array([cum_stock_returns])\n        cum_lv_returns_list = np.array([cum_lv_returns])\n        accuracy_list = np.array([accuracy])\n        lv_accuracy_list = np.array([lv_accuracy])\n        correl_list = np.array([prediction_correl])\n        lv_correl_list = np.array([lv_prediction_correl])\n    else:\n        cum_strat_returns_list = np.append(cum_strat_returns_list, np.array([cum_strat_returns]), axis=0)\n        cum_stock_returns_list = np.append(cum_stock_returns_list, np.array([cum_stock_returns]), axis=0)\n        cum_lv_returns_list = np.append(cum_lv_returns_list, np.array([cum_lv_returns]), axis=0)\n        accuracy_list = np.append(accuracy_list, np.array([accuracy]), axis=0)\n        lv_accuracy_list = np.append(lv_accuracy_list, np.array([lv_accuracy]), axis=0)\n        correl_list = np.append(correl_list, np.array([prediction_correl]), axis=0)\n        lv_correl_list = np.append(lv_correl_list, np.array([lv_prediction_correl]))\n\n\nXOM Accuracy: 0.5418502202643172, Correlation: 0.972313389714224, Last Value Accuracy: 0.5242290748898678, Last Value Correlation: 0.9780145037734742\nCVX Accuracy: 0.5154185022026432, Correlation: 0.9613024878319475, Last Value Accuracy: 0.5066079295154186, Last Value Correlation: 0.9635678490020599\nCOP Accuracy: 0.5550660792951542, Correlation: 0.9689221084107935, Last Value Accuracy: 0.5418502202643172, Last Value Correlation: 0.9783667790929887\nEPD Accuracy: 0.5374449339207048, Correlation: 0.9860702298831613, Last Value Accuracy: 0.5506607929515418, Last Value Correlation: 0.9916250412456891\nEOG Accuracy: 0.5506607929515418, Correlation: 0.9686894982563453, Last Value Accuracy: 0.5462555066079295, Last Value Correlation: 0.9721381776147026\nDUK Accuracy: 0.4933920704845815, Correlation: 0.9693304786167968, Last Value Accuracy: 0.4889867841409692, Last Value Correlation: 0.9698313657418474\nMPC Accuracy: 0.5770925110132159, Correlation: 0.9716364908547462, Last Value Accuracy: 0.5550660792951542, Last Value Correlation: 0.9941712442141354\nSLB Accuracy: 0.4669603524229075, Correlation: 0.9745432552946375, Last Value Accuracy: 0.4669603524229075, Last Value Correlation: 0.9752418496648181\nPSX Accuracy: 0.6079295154185022, Correlation: 0.9801502512089683, Last Value Accuracy: 0.6123348017621145, Last Value Correlation: 0.995576915466044\nOXY Accuracy: 0.5110132158590308, Correlation: 0.9636470842152328, Last Value Accuracy: 0.5154185022026432, Last Value Correlation: 0.9638295520210561\n\n\nAverage Accuracy\nBelow shows the overall accuracy, summed across our 10 stocks, vs the Last Value Benchmark.\nWe find that the average accuracy of our model slightly outperforms the last value benchmark, but our correlation slighty underperforms the last value benchmark.\n\n\nCode\nprint(f'Avg Accuracy: {accuracy_list.mean()}, Avg Correlation: {correl_list.mean()}, Avg LV Accuracy: {lv_accuracy_list.mean()}, Avg LV Correlation: {lv_correl_list.mean()}')\n\n\nAvg Accuracy: 0.5356828193832598, Avg Correlation: 0.9716605274286853, Avg LV Accuracy: 0.5308370044052864, Avg LV Correlation: 0.9782363277836815\n\n\nCumulative Returns\nThe code below shows the comparison between our strategy returns, the baseline stock returns, and the last value benchmark returns.\nWe find that our strategy outperforms the baseline stock returns and the last value benchmark returns.\n\n\nCode\ndf_strat_returns = pd.DataFrame(cum_strat_returns_list.transpose(), columns=tickers)\ndf_strat_returns['Cum_Strat_Returns'] = df_strat_returns.mean(axis=1)\ndf_strat_returns.index = X_test[0].index\n\ndf_stock_returns = pd.DataFrame(cum_stock_returns_list.transpose(), columns=tickers)\ndf_stock_returns['Cum_Stock_Returns'] = df_stock_returns.mean(axis=1)\ndf_stock_returns.index = X_test[0].index\n\ndf_lv_returns = pd.DataFrame(cum_lv_returns_list.transpose(), columns=tickers)\ndf_lv_returns['Cum_LV_Returns'] = df_lv_returns.mean(axis=1)\ndf_lv_returns.index = X_test[0].index\n\ntotal_strat_returns = df_strat_returns['Cum_Strat_Returns'].iloc[-1]\ntotal_stock_returns = df_stock_returns['Cum_Stock_Returns'].iloc[-1]\ntotal_lv_returns = df_lv_returns['Cum_LV_Returns'].iloc[-1]\n\nprint(f'1 Year Portfolio Returns: {total_strat_returns}')\nprint(f'1 Year Stock Returns: {total_stock_returns}')\nprint(f'1 Year LV Returns: {total_lv_returns}')\n\nplt.figure(figsize=(10,5))\nplt.plot(df_strat_returns['Cum_Strat_Returns'], label='Strategy Returns')\nplt.plot(df_stock_returns['Cum_Stock_Returns'], label='Stock Returns')\nplt.plot(df_lv_returns['Cum_LV_Returns'], label='Last Value Benchmark')\nplt.legend();\n\n\n1 Year Portfolio Returns: 1.2360575366734956\n1 Year Stock Returns: 1.2066049840833908\n1 Year LV Returns: 1.195690607517653\n\n\n\n\n\n\n\n\nFigure 1: A comparison of cumulative returns between our strategy returns, baseline stock returns, and the last value benchmark.\n\n\n\n\n\n\n\n\n\nFor fun, we decided to do a live mock test for the past week (2024/05/07 - 2024/05/16) to see how are model does on current data. We followed the same procedures as above except we trained on 10 years of data prior to our test week.\n\n\n(2469, 15)\n(2469, 15)\n(2469, 15)\n(2469, 15)\n(2469, 15)\n(2469, 15)\n(2469, 15)\n(2469, 15)\n(2469, 15)\n(2469, 15)\n\n\n\n\nEpoch: 0, loss: 0.89484\nEpoch: 100, loss: 0.00404\nEpoch: 200, loss: 0.00378\nEpoch: 300, loss: 0.00369\nEpoch: 400, loss: 0.00369\nEpoch: 500, loss: 0.00373\nEpoch: 600, loss: 0.00389\nEpoch: 700, loss: 0.00382\nEpoch: 800, loss: 0.00376\nEpoch: 900, loss: 0.00399\n\n\n\n\n['lstm_live.joblib']\n\n\n::: {#cell-26 .cell 0=‘e’ 1=‘c’ 2=‘h’ 3=‘o’ 4=‘:’ 5=‘f’ 6=‘a’ 7=‘l’ 8=‘s’ 9=‘e’ execution_count=67}\nlstm_live = load('lstm_live.joblib')\n:::\nHere are the accuracies and correlations for each stock based on our strategy and last value:\n\nfor i in range(10):\n    cum_strat_returns, cum_stock_returns, cum_lv_returns, accuracy, lv_accuracy, prediction_correl, lv_prediction_correl = evaluate_lstm(lstm_live, X_test_list[i], y_test_list[i], X_scalers[i], y_scalers[i], features)\n    if i == 0:\n        cum_strat_returns_list = np.array([cum_strat_returns])\n        cum_stock_returns_list = np.array([cum_stock_returns])\n        cum_lv_returns_list = np.array([cum_lv_returns])\n        accuracy_list = np.array([accuracy])\n        lv_accuracy_list = np.array([lv_accuracy])\n        correl_list = np.array([prediction_correl])\n    else:\n        cum_strat_returns_list = np.append(cum_strat_returns_list, np.array([cum_strat_returns]), axis=0)\n        cum_stock_returns_list = np.append(cum_stock_returns_list, np.array([cum_stock_returns]), axis=0)\n        cum_lv_returns_list = np.append(cum_lv_returns_list, np.array([cum_lv_returns]), axis=0)\n        accuracy_list = np.append(accuracy_list, np.array([accuracy]), axis=0)\n        lv_accuracy_list = np.append(lv_accuracy_list, np.array([lv_accuracy]), axis=0)\n        correl_list = np.append(correl_list, np.array([prediction_correl]), axis=0)\n\nXOM Accuracy: 0.5, Correlation: 0.45596059959860424, Last Value Accuracy: 0.3333333333333333, Last Value Correlation: 0.442960265038012\nCVX Accuracy: 0.6666666666666666, Correlation: 0.33422337988058554, Last Value Accuracy: 0.5, Last Value Correlation: 0.34341958688787927\nCOP Accuracy: 0.6666666666666666, Correlation: 0.7971408777137727, Last Value Accuracy: 0.6666666666666666, Last Value Correlation: 0.7558464387577114\nEPD Accuracy: 0.3333333333333333, Correlation: 0.8684150010738954, Last Value Accuracy: 0.3333333333333333, Last Value Correlation: 0.8603779889931645\nEOG Accuracy: 0.3333333333333333, Correlation: 0.2363276728915621, Last Value Accuracy: 0.3333333333333333, Last Value Correlation: 0.25582456053077646\nDUK Accuracy: 0.3333333333333333, Correlation: -0.09651392314802673, Last Value Accuracy: 0.5, Last Value Correlation: -0.159262707461855\nMPC Accuracy: 0.6666666666666666, Correlation: 0.7663651315254851, Last Value Accuracy: 0.6666666666666666, Last Value Correlation: 0.7016596578784171\nSLB Accuracy: 0.5, Correlation: 0.42526094830678546, Last Value Accuracy: 0.5, Last Value Correlation: 0.44817487130500694\nPSX Accuracy: 0.6666666666666666, Correlation: 0.19340309591226992, Last Value Accuracy: 0.8333333333333334, Last Value Correlation: 0.27552121319099077\nOXY Accuracy: 0.3333333333333333, Correlation: 0.40172522417713447, Last Value Accuracy: 0.16666666666666666, Last Value Correlation: 0.4083298375864765\n\n\nHere are the average accuracies and correlations:\n\n\nCode\nprint(f'Avg Accuracy: {accuracy_list.mean()}, Avg Correlation: {correl_list.mean()}, Avg LV Accuracy: {lv_accuracy_list.mean()}, Avg LV Correlation: {lv_prediction_correl.mean()}')\n\n\nAvg Accuracy: 0.4999999999999999, Avg Correlation: 0.43823080079320686, Avg LV Accuracy: 0.4833333333333333, Avg LV Correlation: 0.4083298375864765\n\n\nHere are the returns for the strategy, stocks, and last value:\n\n\nCode\ndf_strat_returns = pd.DataFrame(cum_strat_returns_list.transpose(), columns=tickers)\ndf_strat_returns['Cum_Strat_Returns'] = df_strat_returns.mean(axis=1)\ndf_strat_returns.index = X_test_list[0].index\n\ndf_stock_returns = pd.DataFrame(cum_stock_returns_list.transpose(), columns=tickers)\ndf_stock_returns['Cum_Stock_Returns'] = df_stock_returns.mean(axis=1)\ndf_stock_returns.index = X_test_list[0].index\n\ndf_lv_returns = pd.DataFrame(cum_lv_returns_list.transpose(), columns=tickers)\ndf_lv_returns['Cum_LV_Returns'] = df_lv_returns.mean(axis=1)\ndf_lv_returns.index = X_test_list[0].index\n\ntotal_strat_returns = df_strat_returns['Cum_Strat_Returns'].iloc[-1]\ntotal_stock_returns = df_stock_returns['Cum_Stock_Returns'].iloc[-1]\ntotal_lv_returns = df_lv_returns['Cum_LV_Returns'].iloc[-1]\n\nprint(f'1 Week Portfolio Returns: {total_strat_returns}')\nprint(f'1 Week Stock Returns: {total_stock_returns}')\nprint(f'1 Week LV Returns: {total_lv_returns}')\n\nplt.figure(figsize=(10,5))\nplt.plot(df_strat_returns['Cum_Strat_Returns'], label='Strategy Returns')\nplt.plot(df_stock_returns['Cum_Stock_Returns'], label='Stock Returns')\nplt.plot(df_lv_returns['Cum_LV_Returns'], label='Last Value Benchmark')\nplt.legend();\n\n\n1 Week Portfolio Returns: 0.9981978730664818\n1 Week Stock Returns: 0.9968738574121445\n1 Week LV Returns: 0.9952073325941001\n\n\n\n\n\n\n\n\nFigure 2: A comparison of cumulative returns between our strategy returns, baseline stock returns, and the last value benchmark for 2024-05-07 to 2024-05-16.\n\n\n\n\n\n\n\n\nOur project was a success in the sense that we constructed a model that is profitable and more accurate than our benchmarks. Our original goal was to be better than “random chance” but this last value machine provides a more reasonable benchmark given that stocks tend to rise more than they fall over long time horizons (so simply saying “an accuracy above 0.5 is a success” is not reasonable). On our historical data test, we also achieved an average correlation coefficient of 0.9717 between our predicted prices and actual prices, which slightly lags the best results from Bhandari et al. (2022) and is below our last value benchmark at 0.9782. We didn’t take the same approach as we initially expected of using alternative data, instead using more conventional features, but the ultimate goal was accuracy and profitability, so we aren’t concerned by this change in methodology. We had substantially lower accuracy than Gunduz (2021) , who achieved accuracy of up to 0.675. There are many factors that could have contributed to this difference, including time frames (Gunduz used hourly data vs. our daily data), company geographies (Gunduz studied companies on the Borsa Istanbul), and number of features (Gunduz created a substantially greater number of features).\nThere are a two main key assumptions worth noting. Our tests above make two simplifying assumptions about trading. First, we assume the entire portfolio enters every trade, which any reasonable asset manager would think is incredibly reckless and is a major risk management failure. Second, we assume we are able to buy and sell stocks exactly at their closing price on a given day. This isn’t as problematic an assumption as the first, but it’s still an assumption that may not reflect real-world circumstances, especially when trading small stocks with low trading volumes or, more generally, when trading with enough capital to influence stock prices.\nIf we had more time, data, and computational resources, we would have explored creating and filtering a substantially greater number of features. We also would have liked to have worked with larger baskets of companies. We chose energy companies based on intuition that training a model on data from the same industry would result in better predictions.\n\n\n\nAndre did research on RFE using logistic regression, random forest, and support vector machine before pivoting to an LSTM axiao_research.ipynb. He wrote the source code for the data preparation in lstm_data.py, the LSTM model in lstm_model.py, and the evaluation. He wrote the code for the plots for comparing cumulative returns and the code for calculating the accuracy of the strategy and the benchmarks.\nDonovan provided the initial research and the code for calculating the features SMA_20, SMA_50, RSI, Z_Score, and Std_Dev. He provided visualizations for the moving averages and performed inital tests using logistic regression in dwood_test.ipynb. He wrote the data section.\nJames worked on an early analysis using Google Trends data in prelim-analysis.ipynb, which we pivoted away from after realizing the limited supply of daily data. He created the presentation and wrote the abstract, the introduction, the values, and the conclusion sections. He also wrote the code to calculate the correlation coefficients between predicted prices and actual prices.\n\n\n\nWhile all three of us read up on relevant research and prior literature, I put together the introduction and so had to spend more time on this aspect of our project. I learned about the value of the prior literature–until I read the papers cited, we didn’t have any benchmark. We didn’t know what the best practices were to assess our work. Furthermore, we worked very effectively as a team. We didn’t meet frequently, but each time we met, we made substantial progress either in the project design, code, or writeup. I learned to get a bit better at getting up to speed on others’ code, trying to ask the right questions and spending the time myself with the program.\nI’m happy with what we’ve achieved in this project. I wanted a high-quality, easy-to-understand project that I can refer back to for future projects, and I think this is exactly that. The project is relatively simple in concept, even though the implementation wasn’t. I was originally interested in cleaning up messy data, but that ultimately wasn’t a problem because of the convenience of the Yahoo Finance API. I was also interested in rigorously testing our results. While we could have implemented more tests, I’m happy with what we have in terms of assessing our results across multiple dimensions (returns, accuracy, and correlation).\nA key takeaway for me is that constructing a model thoughtfully for real-world use is extremely difficult. We’re happy with our results, but our work isn’t reflective of what should actually be done in a portfolio. By averaging out our returns across 10 companies, we conceptually buy ourselves the ability to say we “only put 10% of the portfolio in each trade for each company,” but 10% is still a monster of an allocation. We also went into this project knowing it would be difficult to get great results, but we were still surprised by just how difficult it was to achieve similar results as the works cited.\nWorking on this blog post was probably one of the most realistic projects I’ve done in a CS class. We spent a significant amount of time figuring out a direction for our project to take, how to best collaborate, and how to present our findings in an organized fashion. This was definitely a challenging project, and my main takeaway from it for my career is the learning and experience I got from collaborating with my group."
  },
  {
    "objectID": "posts/final-project/quant_research.html#abstract",
    "href": "posts/final-project/quant_research.html#abstract",
    "title": "Quantitative Trading Model Using LSTM",
    "section": "",
    "text": "We applied machine learning methods to predict daily stock price movements in a basket of 10 US-listed energy companies. We found the most success using an LSTM model, achieving an accuracy of up to 61% on one stock (PSX). In line with prior literature, we compared our results to a benchmark established by a last value machine, which simply predicts the next day’s price to be the current day’s actual price. We tested our model on two different sets of observations, first on a year’s worth of historical data (historical test) and then on one week of recent trading data (live test). Comparing our LSTM results to our benchmark, we find mixed results. We achieve, on average, 53.57% accuracy vs. our benchmark’s 53.08% accuracy on the historical test. For the ten companies in our analysis, our model has superior for accuracy for 6 companies, has equal accuracy for 1 company, and has worse accuracy for 3 companies compared to our last value benchmark. This superior accuracy leads to higher simulated portfolio returns using our model compared to our benchmarks. Our live test accuracy of 49.99% was worse but still beat our benchmark’s accuracy of 48.33%."
  },
  {
    "objectID": "posts/final-project/quant_research.html#introduction",
    "href": "posts/final-project/quant_research.html#introduction",
    "title": "Quantitative Trading Model Using LSTM",
    "section": "",
    "text": "In this blog post, we train machine learning models on historical stock market data to predict future stock price movements. This is a highly popular problem to address because of the potential for significant monetary gain. This is an important problem societally because stock markets are mechanisms of price discovery: they answer the question “What is a company worth?” Finding the right answer to that question allows society to correctly allocate more or less capital (money) to that company. On an individual level, this is an important problem to us as the authors because it’s the problem for all quant trading: making a profitable model.\nAn enormous body of literature within and without computer science exists for stock market prediction. Among the papers most relevant to our work are Gunduz (2021), Bhandari et al. (2022), and Zhang (2022).\nGunduz (2021) applies LSTM and ensemble learning (Light-GBM) models to predict the hourly directions of eight banking stocks in Borsa Istanbul. He achieved up to maximum success rate of 0.685 using individual features of bank stocks and LSTM.\nBhandari et al. (2022) apply single-layer and multi-layer LSTM models to the problem of predicting the S&P 500, the index of the largest 500 publicly traded companies in America. Their single-layer LTSM model with 150 neurons is their best performing specification. Their set of predicted values have an average correlation coefficient of 0.9976 with actual S&P index values.\nZhang (2022) finds the LSTM network model does not perform better than other models when applied to a short forecasting horizon (1 to 10 days). Zhang’s “other models” are linear regression, eXtreme gradient boosting (XGBoost), last value, and moving average.\nWe take some of the “best practices” we observe in the above papers, specifically benchmarking with last value and using MSE and R to assess our results. Unlike the mentioned papers, we will be focusing on single stocks and attempting to build a model that outperforms the last value benchmark."
  },
  {
    "objectID": "posts/final-project/quant_research.html#values",
    "href": "posts/final-project/quant_research.html#values",
    "title": "Quantitative Trading Model Using LSTM",
    "section": "",
    "text": "The potential users are anyone interested in making profitable trades in the stock market. They are the individuals most likely to directly benefit from our work. Nonusers who could be affected by our work are those engaged in the stock market. The obvious affected nonusers are those on the opposite side of each trade as a user. In every trade, there’s a buyer and a seller, so in every trade, there’s a winner and a loser. These opposing nonusers are the individuals who are most likely to be harmed by the success of our program.\nUltimately, the point of the back and forth of markets is price discovery: to help society find the right prices of different companies. This leads to another nonuser effect: with better price discovery and more efficient markets, companies will raise money at prices that are closer to some “true” value, which is loosely defined as a value that best reflects the fundamental valuation of the company. Our model does not attempt to predict a true fundamental value for a company, but by making accurate predictions for the next day’s price, it should accelerate the market’s convergence to an appropriate value.\nA useful financial trading model should lead to a net societal benefit because better financial markets mean more or less money going to companies and therefore projects, leading to something closer to an “optimal” allocation of money in society.\nWe are personally motivated to work on this project because of personal interest, professional relevance, and the difficulty of the problem. All three of us personally invest in the stock market. Two of us (Donovan & James) are double majors in economics and have had experience working in the financial services industry. Andre is interested in pursuing a master’s in financial engineering after Middlebury. The problem itself is also inherently challenging: financial markets are constantly adapting and changing, making the findings of previous literature increasingly likely over time to be less applicable to today’s markets. This forces us to adopt new techniques. # Materials and Methods"
  },
  {
    "objectID": "posts/final-project/quant_research.html#our-data",
    "href": "posts/final-project/quant_research.html#our-data",
    "title": "Quantitative Trading Model Using LSTM",
    "section": "",
    "text": "Our data was sourced from Yahoo Finance. We used the yfinance library to download historical stock price data for our 10 different stocks. We chose to focus on US-based oil companies. These companies are Exxon Mobil (XOM), Chevron (CVX), ConocoPhillips (COP), Enterprise Products Partners (EPD), Pioneer Natural Resources (PXD), EOG Resources (EOG), Duke Energy (DUK), Marathon Petroleum (MPC), Schlumberger (SLB), and Phillips 66 (PSX). We downloaded the data from May 6th, 2014 to May 6th, 2024.\nWithin the yfinance dataset we were given the following columns: Open, High, Low, Close, Adj Close, Volume.\nOpen is the opening price of the stock for the day. High is the highest price of the stock for the day. Low is the lowest price of the stock for the day. Close is the closing price of the stock for the day. Adj Close is the adjusted closing price of the stock for the day. Volume is the number of shares traded for the day.\nWe used the Close column as our target variable for our model. We also created the following features: SMA_20, SMA_50, Std_Dev, Z_Score, RSI, TTM_P/E which will be discussed below. Here’s a look at what the raw data looks like:\n\nimport yfinance as yf\n\nxom = yf.Ticker('XOM')\ndata = xom.history(start='2014-05-06', end='2024-05-07')\ndata.head()\n\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nVolume\nDividends\nStock Splits\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n2014-05-06 00:00:00-04:00\n66.050586\n66.501227\n65.928274\n66.095650\n9669800\n0.00\n0.0\n\n\n2014-05-07 00:00:00-04:00\n66.385376\n66.597816\n66.172931\n66.378937\n11007400\n0.00\n0.0\n\n\n2014-05-08 00:00:00-04:00\n66.366038\n66.494794\n65.773772\n65.870338\n8922500\n0.00\n0.0\n\n\n2014-05-09 00:00:00-04:00\n65.922184\n66.226810\n65.630524\n66.077736\n8948800\n0.69\n0.0\n\n\n2014-05-12 00:00:00-04:00\n66.324029\n66.336990\n65.805516\n66.259216\n8830500\n0.00\n0.0\n\n\n\n\n\n\n\n\nYou can find the full implementation of our data at lstm_data.py under the function prepare_data()."
  },
  {
    "objectID": "posts/final-project/quant_research.html#our-approach",
    "href": "posts/final-project/quant_research.html#our-approach",
    "title": "Quantitative Trading Model Using LSTM",
    "section": "",
    "text": "We used SMA_20, SMA_50, Std_Dev, Z_Score, RSI, Close, TTM_P/E as predictors for our models.\nThe SMA_20 and SMA_50 are the 20-day and 50-day simple moving averages of the stock price. This means that the average closing price of the stock over the last 20 and 50 days, respectively.\nThe Std_Dev is the standard deviation of the stock price meaning how much the stock price deviates from the mean.\nThe Z_Score is the z-score of the stock price meaning how many standard deviations the stock price is from the mean.\nThe RSI is the relative strength index of the stock price meaning how strong the stock price is relative to its past performance. It is calculated by taking the average of the gains and losses over a certain period of time.\nThe Close is the closing price of the stock per day.\nThe TTM_P/E is the trailing twelve months price-to-earnings ratio of the stock.\nWe used the next day’s Close price as the target variable for our model.\n\n\n\nWe collected 10 years of data from May 7th, 2014 to May 7th, 2024 and used a train-test split of 90-10 in order to train our model on the first 9 years worth of the data and test it on the remaining 1 year’s worth of data. We used a standard scaler for scaling our data in order to ensure that the data was normalized. We fit the scaler on the training data and then applied it to the test data to avoid any information leaking. We then combined the training data for each stock into one dataset. We used the closing price of the stock as the target variable for our model.\nHere’s what our data looks like after creating our features and scaling the data:\n\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\ntickers = ['XOM', 'CVX', 'COP', 'EPD', 'EOG', 'DUK', 'MPC', 'SLB', 'PSX', 'OXY']\nstart = '2014-05-06'\nend = '2024-05-07'\n\n# preps data, see lstm_data.py, prints size of each ticker's dataset\nX_train, y_train, X_test, y_test, X_scalers, y_scalers, batch_size = prepare_data(tickers, start_date=start, end_date=end, test_size=0.1)\nX_train\n\n(2041, 18)\n(2041, 18)\n(2041, 18)\n(2041, 18)\n(2041, 18)\n(2041, 18)\n(2041, 18)\n(2041, 18)\n(2041, 18)\n(2041, 18)\n\n\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nVolume\nDividends\nStock Splits\nSMA_10\nSMA_20\nSMA_50\nSMA_100\nSMA_250\nStd_Dev\nZ_Score\nRSI\nTTM_EPS\nTTM_P/E\nReturns\nTicker\n\n\n\n\n2015-05-01\n-0.139877\n-0.131185\n-0.112171\n-0.102371\n-0.540891\n-0.127836\n0.00000\n-0.149377\n-0.173825\n-0.178037\n-0.063809\n0.319177\n-0.679901\n1.186546\n0.879810\n0.650885\n-0.095652\n0.893759\nXOM\n\n\n2015-05-04\n-0.080169\n-0.096257\n-0.064154\n-0.092513\n-0.732495\n-0.127836\n0.00000\n-0.142218\n-0.166130\n-0.178686\n-0.064625\n0.316818\n-0.653451\n1.177769\n0.973560\n0.650885\n-0.095424\n0.133748\nXOM\n\n\n2015-05-05\n-0.059251\n-0.088369\n-0.080672\n-0.111473\n-0.577151\n-0.127836\n0.00000\n-0.135595\n-0.160600\n-0.179007\n-0.065519\n0.314254\n-0.642313\n0.791839\n0.672019\n0.650885\n-0.095861\n-0.329225\nXOM\n\n\n2015-05-06\n-0.071421\n-0.093251\n-0.108714\n-0.127400\n-0.639387\n-0.127836\n0.00000\n-0.132800\n-0.152615\n-0.179992\n-0.065441\n0.311766\n-0.751693\n0.518627\n0.427504\n0.650885\n-0.096229\n-0.281992\nXOM\n\n\n2015-05-07\n-0.134173\n-0.163112\n-0.142135\n-0.149774\n-0.767664\n-0.127836\n0.00000\n-0.132571\n-0.146911\n-0.181595\n-0.065840\n0.309079\n-0.859104\n0.047923\n0.100192\n0.650885\n-0.096744\n-0.387753\nXOM\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-06-02\n0.610803\n0.625792\n0.636749\n0.650958\n0.109535\n-0.105084\n-0.02214\n0.607044\n0.598059\n0.727834\n0.786385\n0.982249\n-0.739053\n0.869582\n0.136220\n1.924677\n-0.222355\n0.810058\nOXY\n\n\n2023-06-05\n0.714326\n0.672373\n0.676732\n0.633996\n-0.312158\n-0.105084\n-0.02214\n0.608629\n0.594259\n0.728931\n0.783608\n0.979744\n-0.834725\n0.776482\n0.011823\n1.924677\n-0.222464\n-0.165726\nOXY\n\n\n2023-06-06\n0.590332\n0.602501\n0.623814\n0.636336\n-0.325573\n-0.105084\n-0.02214\n0.610977\n0.595673\n0.730434\n0.780891\n0.977007\n-0.819762\n0.773031\n0.029482\n1.924677\n-0.222449\n0.003637\nOXY\n\n\n2023-06-07\n0.652329\n0.667133\n0.694371\n0.671429\n-0.292849\n-0.105084\n-0.02214\n0.613090\n0.598854\n0.730900\n0.777538\n0.974819\n-0.772194\n1.238762\n0.294437\n1.924677\n-0.222223\n0.292064\nOXY\n\n\n2023-06-08\n0.669287\n0.645055\n0.635925\n0.662629\n-0.147267\n2.262466\n-0.02214\n0.616022\n0.607809\n0.728131\n0.774355\n0.973557\n-0.847555\n1.077118\n0.218085\n1.924677\n-0.222280\n-0.093700\nOXY\n\n\n\n\n20410 rows × 19 columns\n\n\n\n\n\n\n\nOriginally, we used rather simplistic models like logistic regression, Random Forest, and SVM in order to predict stock price movements. We utilized Recursive Feature Elimination (RFE) in order to determine the optimal features for prediction for each model. However, we found that these models were not able to predict stock price movements consistently with much accuracy. We then decided to use a Long Short-Term Memory (LSTM) model to predict stock price movements. LSTM models are a type of recurrent neural network (RNN) with the addition of “gates” notably the input, forget and output gates. These gates allow for the model to determine what information to retain or discard at each timestep, mitigating the vanishing descent issue found in traditional recurrent neural networks. The LSTM model accounts for the shortfalls of an RNN by capturing long-term dependencies in the data.\nThe forget gate determines which information is either retained or discarded at each time step. It accepts the output from the previous time step \\(h_{t-1}\\) and the input \\(x_t\\) at the current time step. The forget gate is defined as:\n\\[f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\\]\nThe input gate determines which information is stored in the cell state. It avoids feeding the unimportant information into the current memory cell. It has three different components:\n\nGetting the state of the cell that must be updated.\nCreate a new cell state\nUpdate the cell state to the current cell state\n\nThese are defined as:\n\\[\\begin{aligned}\ni_{t} &= \\sigma(W_{t} \\cdot [h_{t-1}, x_{t}] + b_{i}) \\\\\n\\widetilde{C}_{t} &= \\tanh(W_{c} \\cdot [h_{t-1}, x_{t}] + b_{c}) \\\\\nC_{t} &= f_{t} \\ast C_{t-1} + i_{t} \\ast \\widetilde{C}_{t}\n\\end{aligned}\\]\nThe output gate determines how much of the newly created cell state will be discarded and how much will be passed to the output. It is defined as:\n\\[o_{t} = \\sigma(W_{o} \\cdot [h_{t-1}, x_{t}] + b_{o})\\]\nThis output information is firstly determined by a sigmoid layer, then the newly created cell state is processed by a tanh layer. The output is then multiplied by the sigmoid layer to determine the final output of the LSTM cell.\nWhich is defined as:\n\\[h_{t} = o_{t} \\ast \\tanh(C_{t})\\]\nTaking this all into account, the LSTM model is able to retain information from previous time steps and use it to predict future stock price movements while disregarding irrelevant information.\nThe implementation of our LSTM model can be found at: lstm_model.py\n\n\n\nWe first converted our wanted feature columns into a torch Variable to allow them to be differentiable. Then, we reshaped the data using torch.reshape() and torch.utils.data.DataLoader into [batch_size, seq_len, input_size].\n\nfeatures = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'Close', 'TTM_P/E']\n\nX_train_tensors = Variable(torch.Tensor(np.array(X_train[features])))\ny_train_tensors = Variable(torch.Tensor(y_train.values))\nX_train_final = torch.reshape(X_train_tensors, (X_train_tensors.shape[0], 1, X_train_tensors.shape[1]))\n\n# split data by ticker\ndata_loader_train = torch.utils.data.DataLoader(\n    torch.utils.data.TensorDataset(X_train_final, y_train_tensors),\n    batch_size=batch_size,\n    shuffle=True\n)\n\nnext(iter(data_loader_train))[0].shape\n\ntorch.Size([2041, 1, 7])\n\n\nWe trained our model using our own personal devices. We used the Adam optimizer with a learning rate of 0.001. We trained the model for 1000 epochs for each stock in our dataset (10 total) and used the torch.nn.MSELoss() loss function to train the model.\nMSE is defined as:\n\\[MSE = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_{i} - y_{i})^2\\]\nWhere \\(y_{i}\\) is the true price and \\(\\hat{y}_{i}\\) is the predicted price.\nAs mentioned previously our model was trained on 90% of the data and tested on the remaining 10%.\nIf the model predicted the next days price to be positive, we would purchase the stock at the closing price and sell it at the closing price the next day. If the model predicted the next days price to be negative, we would short the stock at the closing price and buy it back at the closing price the next day. We would then calculate the profit or loss percent change for each stock and compare it to the last value benchmark.\nBelow is our training code:\n\nnum_epochs = 1000 # 1000 epochs\nlearning_rate = 0.001 # 0.001 lr\n\ninput_size = X_train_final.shape[2] # number of features\nhidden_size = 32 # number of features in hidden state\nnum_layers = 1 # number of stacked lstm layers\nwindow = 1 # number of windows, leave at 1, basically can ignore\n\nnum_classes = 1 # number of output classes\n\nlstm = LSTMModel(num_classes, input_size, hidden_size, num_layers, seq_length=window, batch_size=batch_size) #our lstm class \ncriterion = torch.nn.MSELoss()    # mean-squared error for regression\noptimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate) # ADAM optimizer\n\n# training loop\nfor epoch in range(num_epochs):\n  for i, data in enumerate(data_loader_train):\n    X_, y_ = data\n    outputs = lstm.forward(X_) #forward pass\n    optimizer.zero_grad() #calculate the gradient, manually setting to 0\n  \n    # obtain the loss function\n    loss = criterion(outputs, y_.reshape(y_.size(0)*y_.size(1), 1))\n  \n    loss.backward() #calculates the loss of the loss function\n  \n    optimizer.step() #improve from loss, i.e backprop\n    # if (i + 1) % 50 == 0:\n    #     print(f\"Epoch {epoch}, batch {i:&gt;3}, loss on batch: {loss.item():.3f}\")\n  if epoch % 100 == 0:\n    print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))\n\nEpoch: 0, loss: 0.91365\nEpoch: 100, loss: 0.00494\nEpoch: 200, loss: 0.00502\nEpoch: 300, loss: 0.00508\nEpoch: 400, loss: 0.00451\nEpoch: 500, loss: 0.00478\nEpoch: 600, loss: 0.00479\nEpoch: 700, loss: 0.00491\nEpoch: 800, loss: 0.00514\nEpoch: 900, loss: 0.00504\n\n\n\n\n\nWe evaluated our model by comparing the cumulative predicted stock price returns and accuracy to the actual cumulative stock price returns and accuracy and the cumulative last value benchmark returns and accuracy. The last value benchmark is defined as using the previous days value as the prediction for the current day. We would buy the stock at the current day’s close price and sell at the next day’s close price if the predicted returns were positive and do nothing if the predicted returns were negative. We followed the same principle in calculating actual cumulative stock returns and accuracy, and the cumulative last value benchmark returns and accuracy.\nWe define accuracy for our purposes as percentage of times the model correctly predicts an upward or downward movement in the share price of a company.\nConsider a simple test case where the model predicts the stock price to go up and the stock price actually goes up. In this case, the model is correct. If the model predicts the stock price to go up and the stock price actually goes down, the model is incorrect. We calculate the accuracy of the model by dividing the number of correct predictions by the total number of predictions.\nAccuracy per Stock\n\n\nCode\nfor i in range(10):\n    cum_strat_returns, cum_stock_returns, cum_lv_returns, accuracy, lv_accuracy, prediction_correl, lv_prediction_correl = evaluate_lstm(lstm, X_test[i], y_test[i], X_scalers[i], y_scalers[i], features)\n    if i == 0:\n        cum_strat_returns_list = np.array([cum_strat_returns])\n        cum_stock_returns_list = np.array([cum_stock_returns])\n        cum_lv_returns_list = np.array([cum_lv_returns])\n        accuracy_list = np.array([accuracy])\n        lv_accuracy_list = np.array([lv_accuracy])\n        correl_list = np.array([prediction_correl])\n        lv_correl_list = np.array([lv_prediction_correl])\n    else:\n        cum_strat_returns_list = np.append(cum_strat_returns_list, np.array([cum_strat_returns]), axis=0)\n        cum_stock_returns_list = np.append(cum_stock_returns_list, np.array([cum_stock_returns]), axis=0)\n        cum_lv_returns_list = np.append(cum_lv_returns_list, np.array([cum_lv_returns]), axis=0)\n        accuracy_list = np.append(accuracy_list, np.array([accuracy]), axis=0)\n        lv_accuracy_list = np.append(lv_accuracy_list, np.array([lv_accuracy]), axis=0)\n        correl_list = np.append(correl_list, np.array([prediction_correl]), axis=0)\n        lv_correl_list = np.append(lv_correl_list, np.array([lv_prediction_correl]))\n\n\nXOM Accuracy: 0.5418502202643172, Correlation: 0.972313389714224, Last Value Accuracy: 0.5242290748898678, Last Value Correlation: 0.9780145037734742\nCVX Accuracy: 0.5154185022026432, Correlation: 0.9613024878319475, Last Value Accuracy: 0.5066079295154186, Last Value Correlation: 0.9635678490020599\nCOP Accuracy: 0.5550660792951542, Correlation: 0.9689221084107935, Last Value Accuracy: 0.5418502202643172, Last Value Correlation: 0.9783667790929887\nEPD Accuracy: 0.5374449339207048, Correlation: 0.9860702298831613, Last Value Accuracy: 0.5506607929515418, Last Value Correlation: 0.9916250412456891\nEOG Accuracy: 0.5506607929515418, Correlation: 0.9686894982563453, Last Value Accuracy: 0.5462555066079295, Last Value Correlation: 0.9721381776147026\nDUK Accuracy: 0.4933920704845815, Correlation: 0.9693304786167968, Last Value Accuracy: 0.4889867841409692, Last Value Correlation: 0.9698313657418474\nMPC Accuracy: 0.5770925110132159, Correlation: 0.9716364908547462, Last Value Accuracy: 0.5550660792951542, Last Value Correlation: 0.9941712442141354\nSLB Accuracy: 0.4669603524229075, Correlation: 0.9745432552946375, Last Value Accuracy: 0.4669603524229075, Last Value Correlation: 0.9752418496648181\nPSX Accuracy: 0.6079295154185022, Correlation: 0.9801502512089683, Last Value Accuracy: 0.6123348017621145, Last Value Correlation: 0.995576915466044\nOXY Accuracy: 0.5110132158590308, Correlation: 0.9636470842152328, Last Value Accuracy: 0.5154185022026432, Last Value Correlation: 0.9638295520210561\n\n\nAverage Accuracy\nBelow shows the overall accuracy, summed across our 10 stocks, vs the Last Value Benchmark.\nWe find that the average accuracy of our model slightly outperforms the last value benchmark, but our correlation slighty underperforms the last value benchmark.\n\n\nCode\nprint(f'Avg Accuracy: {accuracy_list.mean()}, Avg Correlation: {correl_list.mean()}, Avg LV Accuracy: {lv_accuracy_list.mean()}, Avg LV Correlation: {lv_correl_list.mean()}')\n\n\nAvg Accuracy: 0.5356828193832598, Avg Correlation: 0.9716605274286853, Avg LV Accuracy: 0.5308370044052864, Avg LV Correlation: 0.9782363277836815\n\n\nCumulative Returns\nThe code below shows the comparison between our strategy returns, the baseline stock returns, and the last value benchmark returns.\nWe find that our strategy outperforms the baseline stock returns and the last value benchmark returns.\n\n\nCode\ndf_strat_returns = pd.DataFrame(cum_strat_returns_list.transpose(), columns=tickers)\ndf_strat_returns['Cum_Strat_Returns'] = df_strat_returns.mean(axis=1)\ndf_strat_returns.index = X_test[0].index\n\ndf_stock_returns = pd.DataFrame(cum_stock_returns_list.transpose(), columns=tickers)\ndf_stock_returns['Cum_Stock_Returns'] = df_stock_returns.mean(axis=1)\ndf_stock_returns.index = X_test[0].index\n\ndf_lv_returns = pd.DataFrame(cum_lv_returns_list.transpose(), columns=tickers)\ndf_lv_returns['Cum_LV_Returns'] = df_lv_returns.mean(axis=1)\ndf_lv_returns.index = X_test[0].index\n\ntotal_strat_returns = df_strat_returns['Cum_Strat_Returns'].iloc[-1]\ntotal_stock_returns = df_stock_returns['Cum_Stock_Returns'].iloc[-1]\ntotal_lv_returns = df_lv_returns['Cum_LV_Returns'].iloc[-1]\n\nprint(f'1 Year Portfolio Returns: {total_strat_returns}')\nprint(f'1 Year Stock Returns: {total_stock_returns}')\nprint(f'1 Year LV Returns: {total_lv_returns}')\n\nplt.figure(figsize=(10,5))\nplt.plot(df_strat_returns['Cum_Strat_Returns'], label='Strategy Returns')\nplt.plot(df_stock_returns['Cum_Stock_Returns'], label='Stock Returns')\nplt.plot(df_lv_returns['Cum_LV_Returns'], label='Last Value Benchmark')\nplt.legend();\n\n\n1 Year Portfolio Returns: 1.2360575366734956\n1 Year Stock Returns: 1.2066049840833908\n1 Year LV Returns: 1.195690607517653\n\n\n\n\n\n\n\n\nFigure 1: A comparison of cumulative returns between our strategy returns, baseline stock returns, and the last value benchmark."
  },
  {
    "objectID": "posts/final-project/quant_research.html#live-mock-testing",
    "href": "posts/final-project/quant_research.html#live-mock-testing",
    "title": "Quantitative Trading Model Using LSTM",
    "section": "",
    "text": "For fun, we decided to do a live mock test for the past week (2024/05/07 - 2024/05/16) to see how are model does on current data. We followed the same procedures as above except we trained on 10 years of data prior to our test week.\n\n\n(2469, 15)\n(2469, 15)\n(2469, 15)\n(2469, 15)\n(2469, 15)\n(2469, 15)\n(2469, 15)\n(2469, 15)\n(2469, 15)\n(2469, 15)\n\n\n\n\nEpoch: 0, loss: 0.89484\nEpoch: 100, loss: 0.00404\nEpoch: 200, loss: 0.00378\nEpoch: 300, loss: 0.00369\nEpoch: 400, loss: 0.00369\nEpoch: 500, loss: 0.00373\nEpoch: 600, loss: 0.00389\nEpoch: 700, loss: 0.00382\nEpoch: 800, loss: 0.00376\nEpoch: 900, loss: 0.00399\n\n\n\n\n['lstm_live.joblib']\n\n\n::: {#cell-26 .cell 0=‘e’ 1=‘c’ 2=‘h’ 3=‘o’ 4=‘:’ 5=‘f’ 6=‘a’ 7=‘l’ 8=‘s’ 9=‘e’ execution_count=67}\nlstm_live = load('lstm_live.joblib')\n:::\nHere are the accuracies and correlations for each stock based on our strategy and last value:\n\nfor i in range(10):\n    cum_strat_returns, cum_stock_returns, cum_lv_returns, accuracy, lv_accuracy, prediction_correl, lv_prediction_correl = evaluate_lstm(lstm_live, X_test_list[i], y_test_list[i], X_scalers[i], y_scalers[i], features)\n    if i == 0:\n        cum_strat_returns_list = np.array([cum_strat_returns])\n        cum_stock_returns_list = np.array([cum_stock_returns])\n        cum_lv_returns_list = np.array([cum_lv_returns])\n        accuracy_list = np.array([accuracy])\n        lv_accuracy_list = np.array([lv_accuracy])\n        correl_list = np.array([prediction_correl])\n    else:\n        cum_strat_returns_list = np.append(cum_strat_returns_list, np.array([cum_strat_returns]), axis=0)\n        cum_stock_returns_list = np.append(cum_stock_returns_list, np.array([cum_stock_returns]), axis=0)\n        cum_lv_returns_list = np.append(cum_lv_returns_list, np.array([cum_lv_returns]), axis=0)\n        accuracy_list = np.append(accuracy_list, np.array([accuracy]), axis=0)\n        lv_accuracy_list = np.append(lv_accuracy_list, np.array([lv_accuracy]), axis=0)\n        correl_list = np.append(correl_list, np.array([prediction_correl]), axis=0)\n\nXOM Accuracy: 0.5, Correlation: 0.45596059959860424, Last Value Accuracy: 0.3333333333333333, Last Value Correlation: 0.442960265038012\nCVX Accuracy: 0.6666666666666666, Correlation: 0.33422337988058554, Last Value Accuracy: 0.5, Last Value Correlation: 0.34341958688787927\nCOP Accuracy: 0.6666666666666666, Correlation: 0.7971408777137727, Last Value Accuracy: 0.6666666666666666, Last Value Correlation: 0.7558464387577114\nEPD Accuracy: 0.3333333333333333, Correlation: 0.8684150010738954, Last Value Accuracy: 0.3333333333333333, Last Value Correlation: 0.8603779889931645\nEOG Accuracy: 0.3333333333333333, Correlation: 0.2363276728915621, Last Value Accuracy: 0.3333333333333333, Last Value Correlation: 0.25582456053077646\nDUK Accuracy: 0.3333333333333333, Correlation: -0.09651392314802673, Last Value Accuracy: 0.5, Last Value Correlation: -0.159262707461855\nMPC Accuracy: 0.6666666666666666, Correlation: 0.7663651315254851, Last Value Accuracy: 0.6666666666666666, Last Value Correlation: 0.7016596578784171\nSLB Accuracy: 0.5, Correlation: 0.42526094830678546, Last Value Accuracy: 0.5, Last Value Correlation: 0.44817487130500694\nPSX Accuracy: 0.6666666666666666, Correlation: 0.19340309591226992, Last Value Accuracy: 0.8333333333333334, Last Value Correlation: 0.27552121319099077\nOXY Accuracy: 0.3333333333333333, Correlation: 0.40172522417713447, Last Value Accuracy: 0.16666666666666666, Last Value Correlation: 0.4083298375864765\n\n\nHere are the average accuracies and correlations:\n\n\nCode\nprint(f'Avg Accuracy: {accuracy_list.mean()}, Avg Correlation: {correl_list.mean()}, Avg LV Accuracy: {lv_accuracy_list.mean()}, Avg LV Correlation: {lv_prediction_correl.mean()}')\n\n\nAvg Accuracy: 0.4999999999999999, Avg Correlation: 0.43823080079320686, Avg LV Accuracy: 0.4833333333333333, Avg LV Correlation: 0.4083298375864765\n\n\nHere are the returns for the strategy, stocks, and last value:\n\n\nCode\ndf_strat_returns = pd.DataFrame(cum_strat_returns_list.transpose(), columns=tickers)\ndf_strat_returns['Cum_Strat_Returns'] = df_strat_returns.mean(axis=1)\ndf_strat_returns.index = X_test_list[0].index\n\ndf_stock_returns = pd.DataFrame(cum_stock_returns_list.transpose(), columns=tickers)\ndf_stock_returns['Cum_Stock_Returns'] = df_stock_returns.mean(axis=1)\ndf_stock_returns.index = X_test_list[0].index\n\ndf_lv_returns = pd.DataFrame(cum_lv_returns_list.transpose(), columns=tickers)\ndf_lv_returns['Cum_LV_Returns'] = df_lv_returns.mean(axis=1)\ndf_lv_returns.index = X_test_list[0].index\n\ntotal_strat_returns = df_strat_returns['Cum_Strat_Returns'].iloc[-1]\ntotal_stock_returns = df_stock_returns['Cum_Stock_Returns'].iloc[-1]\ntotal_lv_returns = df_lv_returns['Cum_LV_Returns'].iloc[-1]\n\nprint(f'1 Week Portfolio Returns: {total_strat_returns}')\nprint(f'1 Week Stock Returns: {total_stock_returns}')\nprint(f'1 Week LV Returns: {total_lv_returns}')\n\nplt.figure(figsize=(10,5))\nplt.plot(df_strat_returns['Cum_Strat_Returns'], label='Strategy Returns')\nplt.plot(df_stock_returns['Cum_Stock_Returns'], label='Stock Returns')\nplt.plot(df_lv_returns['Cum_LV_Returns'], label='Last Value Benchmark')\nplt.legend();\n\n\n1 Week Portfolio Returns: 0.9981978730664818\n1 Week Stock Returns: 0.9968738574121445\n1 Week LV Returns: 0.9952073325941001\n\n\n\n\n\n\n\n\nFigure 2: A comparison of cumulative returns between our strategy returns, baseline stock returns, and the last value benchmark for 2024-05-07 to 2024-05-16."
  },
  {
    "objectID": "posts/final-project/quant_research.html#concluding-discussion",
    "href": "posts/final-project/quant_research.html#concluding-discussion",
    "title": "Quantitative Trading Model Using LSTM",
    "section": "",
    "text": "Our project was a success in the sense that we constructed a model that is profitable and more accurate than our benchmarks. Our original goal was to be better than “random chance” but this last value machine provides a more reasonable benchmark given that stocks tend to rise more than they fall over long time horizons (so simply saying “an accuracy above 0.5 is a success” is not reasonable). On our historical data test, we also achieved an average correlation coefficient of 0.9717 between our predicted prices and actual prices, which slightly lags the best results from Bhandari et al. (2022) and is below our last value benchmark at 0.9782. We didn’t take the same approach as we initially expected of using alternative data, instead using more conventional features, but the ultimate goal was accuracy and profitability, so we aren’t concerned by this change in methodology. We had substantially lower accuracy than Gunduz (2021) , who achieved accuracy of up to 0.675. There are many factors that could have contributed to this difference, including time frames (Gunduz used hourly data vs. our daily data), company geographies (Gunduz studied companies on the Borsa Istanbul), and number of features (Gunduz created a substantially greater number of features).\nThere are a two main key assumptions worth noting. Our tests above make two simplifying assumptions about trading. First, we assume the entire portfolio enters every trade, which any reasonable asset manager would think is incredibly reckless and is a major risk management failure. Second, we assume we are able to buy and sell stocks exactly at their closing price on a given day. This isn’t as problematic an assumption as the first, but it’s still an assumption that may not reflect real-world circumstances, especially when trading small stocks with low trading volumes or, more generally, when trading with enough capital to influence stock prices.\nIf we had more time, data, and computational resources, we would have explored creating and filtering a substantially greater number of features. We also would have liked to have worked with larger baskets of companies. We chose energy companies based on intuition that training a model on data from the same industry would result in better predictions."
  },
  {
    "objectID": "posts/final-project/quant_research.html#group-contributions-statement",
    "href": "posts/final-project/quant_research.html#group-contributions-statement",
    "title": "Quantitative Trading Model Using LSTM",
    "section": "",
    "text": "Andre did research on RFE using logistic regression, random forest, and support vector machine before pivoting to an LSTM axiao_research.ipynb. He wrote the source code for the data preparation in lstm_data.py, the LSTM model in lstm_model.py, and the evaluation. He wrote the code for the plots for comparing cumulative returns and the code for calculating the accuracy of the strategy and the benchmarks.\nDonovan provided the initial research and the code for calculating the features SMA_20, SMA_50, RSI, Z_Score, and Std_Dev. He provided visualizations for the moving averages and performed inital tests using logistic regression in dwood_test.ipynb. He wrote the data section.\nJames worked on an early analysis using Google Trends data in prelim-analysis.ipynb, which we pivoted away from after realizing the limited supply of daily data. He created the presentation and wrote the abstract, the introduction, the values, and the conclusion sections. He also wrote the code to calculate the correlation coefficients between predicted prices and actual prices."
  },
  {
    "objectID": "posts/final-project/quant_research.html#personal-reflection",
    "href": "posts/final-project/quant_research.html#personal-reflection",
    "title": "Quantitative Trading Model Using LSTM",
    "section": "",
    "text": "While all three of us read up on relevant research and prior literature, I put together the introduction and so had to spend more time on this aspect of our project. I learned about the value of the prior literature–until I read the papers cited, we didn’t have any benchmark. We didn’t know what the best practices were to assess our work. Furthermore, we worked very effectively as a team. We didn’t meet frequently, but each time we met, we made substantial progress either in the project design, code, or writeup. I learned to get a bit better at getting up to speed on others’ code, trying to ask the right questions and spending the time myself with the program.\nI’m happy with what we’ve achieved in this project. I wanted a high-quality, easy-to-understand project that I can refer back to for future projects, and I think this is exactly that. The project is relatively simple in concept, even though the implementation wasn’t. I was originally interested in cleaning up messy data, but that ultimately wasn’t a problem because of the convenience of the Yahoo Finance API. I was also interested in rigorously testing our results. While we could have implemented more tests, I’m happy with what we have in terms of assessing our results across multiple dimensions (returns, accuracy, and correlation).\nA key takeaway for me is that constructing a model thoughtfully for real-world use is extremely difficult. We’re happy with our results, but our work isn’t reflective of what should actually be done in a portfolio. By averaging out our returns across 10 companies, we conceptually buy ourselves the ability to say we “only put 10% of the portfolio in each trade for each company,” but 10% is still a monster of an allocation. We also went into this project knowing it would be difficult to get great results, but we were still surprised by just how difficult it was to achieve similar results as the works cited.\nWorking on this blog post was probably one of the most realistic projects I’ve done in a CS class. We spent a significant amount of time figuring out a direction for our project to take, how to best collaborate, and how to present our findings in an organized fashion. This was definitely a challenging project, and my main takeaway from it for my career is the learning and experience I got from collaborating with my group."
  },
  {
    "objectID": "posts/final-project/dwood_test.html#donovan-test-code-just-to-get-started",
    "href": "posts/final-project/dwood_test.html#donovan-test-code-just-to-get-started",
    "title": "Donovan test code just to get started",
    "section": "Donovan test code just to get started",
    "text": "Donovan test code just to get started\n\nimport yfinance as yf\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n# Define the ticker and the time period\nticker = 'AAPL'\nstart_date = '2019-01-01'\nend_date = '2020-01-01'\n\n# Fetch data\ndata = yf.download(ticker, start=start_date, end=end_date)\nprint(data.head())\n\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n                 Open       High        Low      Close  Adj Close     Volume\nDate                                                                        \n2019-01-02  38.722500  39.712502  38.557499  39.480000  37.845047  148158800\n2019-01-03  35.994999  36.430000  35.500000  35.547501  34.075401  365248800\n2019-01-04  36.132500  37.137501  35.950001  37.064999  35.530048  234428400\n2019-01-07  37.174999  37.207500  36.474998  36.982498  35.450970  219111200\n2019-01-08  37.389999  37.955002  37.130001  37.687500  36.126774  164101200\n\n\n\n# Moving Average \n\nshort_window = 40\nlong_window = 100\n\ndata['Short_MAvg'] = data['Close'].rolling(window=short_window, min_periods=1).mean()\ndata['Long_MAvg'] = data['Close'].rolling(window=long_window, min_periods=1).mean()\n\n\n# Basic trading signal based on crossover \n\n# Create a 'Signal' column\ndata['Signal'] = 0\ndata['Signal'] = np.where(data['Short_MAvg'] &gt; data['Long_MAvg'], 1, 0)\n\n# Generate trading orders\ndata['Position'] = data['Signal'].diff()\n\n\n# Basic Back Test\n\n# Plot the closing prices and moving averages\nplt.figure(figsize=(14,7))\nplt.plot(data['Close'], label='Close Price', alpha=0.5)\nplt.plot(data['Short_MAvg'], label=f'{short_window}-Day MA', alpha=0.75)\nplt.plot(data['Long_MAvg'], label=f'{long_window}-Day MA', alpha=0.75)\n\n# Plot buy signals\nplt.plot(data[data['Position'] == 1].index, data['Short_MAvg'][data['Position'] == 1], '^', markersize=10, color='g', lw=0, label='Buy Signal')\n\n# Plot sell signals\nplt.plot(data[data['Position'] == -1].index, data['Short_MAvg'][data['Position'] == -1], 'v', markersize=10, color='r', lw=0, label='Sell Signal')\n\nplt.title('AAPL Stock Price and Moving Average Crossovers')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nMessing around with mean reversion\n\nimport yfinance as yf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nticker = 'AAPL'\nstart_date = '2019-01-01'\nend_date = '2020-01-01'\n\n\ndata = yf.download(ticker, start=start_date, end=end_date)\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n# Calculate the moving average and the standard deviation\nwindow = 20\ndata['Moving_Average'] = data['Close'].rolling(window=window).mean()\ndata['Std_Dev'] = data['Close'].rolling(window=window).std()\n\n# Calculate the z-score\ndata['Z_Score'] = (data['Close'] - data['Moving_Average']) / data['Std_Dev']\n\n# Define thresholds for buying and selling\nthreshold_buy = -1.5  # Buy signal threshold\nthreshold_sell = 1.5  # Sell signal threshold\n\n# Generate signals\ndata['Signal'] = 0\ndata['Signal'][data['Z_Score'] &gt; threshold_sell] = -1  # Sell signal\ndata['Signal'][data['Z_Score'] &lt; threshold_buy] = 1  # Buy signal\n\n# Generate trading positions\ndata['Position'] = data['Signal'].replace(0, np.nan).ffill().fillna(0)\n\n/var/folders/6z/mh947m1s1m17ylsnt3y4rkfc0000gn/T/ipykernel_33078/834088593.py:15: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data['Signal'][data['Z_Score'] &gt; threshold_sell] = -1  # Sell signal\n/var/folders/6z/mh947m1s1m17ylsnt3y4rkfc0000gn/T/ipykernel_33078/834088593.py:16: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data['Signal'][data['Z_Score'] &lt; threshold_buy] = 1  # Buy signal\n\n\n\n# Plot the results\nplt.figure(figsize=(14,7))\nplt.plot(data['Close'], label='Close Price', alpha=0.5)\nplt.plot(data['Moving_Average'], label='Moving Average', alpha=0.75)\nplt.fill_between(data.index, data['Moving_Average'] - data['Std_Dev'], data['Moving_Average'] + data['Std_Dev'], color='gray', alpha=0.3, label='Standard Deviation Range')\n\n# Highlight the buy and sell signals\nplt.plot(data[data['Position'] == 1].index, data['Close'][data['Position'] == 1], '^', markersize=10, color='g', lw=0, label='Buy Signal')\nplt.plot(data[data['Position'] == -1].index, data['Close'][data['Position'] == -1], 'v', markersize=10, color='r', lw=0, label='Sell Signal')\n\nplt.title('AAPL Stock Price and Mean Reversion Signals')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nNow will actually do some machine learning stuff. Add some random features and try to make it work\n\nimport yfinance as yf\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\n#Trying random forest here just for fun, could use LR or something else too\n\n\nticker = 'AAPL'\ndata = yf.download(ticker, start=\"2010-01-01\", end=\"2020-01-01\")\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n# Calculate moving averages\ndata['SMA_20'] = data['Close'].rolling(window=20).mean()\ndata['SMA_50'] = data['Close'].rolling(window=50).mean()\n\n# Calculate RSI\ndelta = data['Close'].diff()\nup = delta.clip(lower=0)\ndown = -1 * delta.clip(upper=0)\nema_up = up.ewm(com=13, adjust=False).mean()\nema_down = down.ewm(com=13, adjust=False).mean()\nrs = ema_up / ema_down\n\ndata['RSI'] = 100 - (100 / (1 + rs))\n\n# Calculate the daily returns\ndata['Returns'] = data['Close'].pct_change()\n\n# Drop any NaNs\ndata.dropna(inplace=True)\n\n\n# Data if it goes up or down\n\ndata['Target'] = (data['Returns'].shift(-1) &gt; 0).astype(int)\n\n\nfeatures = ['SMA_20', 'SMA_50', 'RSI', 'Returns']\nX = data[features]\ny = data['Target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\nRandomForestClassifier(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(random_state=42)\n\n\n\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\nAccuracy: 0.5080971659919028\n\n\n\ndata['Predicted_Signal'] = model.predict(X)\ndata['Strategy_Returns'] = data['Returns'] * data['Predicted_Signal'].shift(1)\ncumulative_strategy_returns = (data['Strategy_Returns'] + 1).cumprod()\n\nplt.figure(figsize=(10,5))\nplt.plot(cumulative_strategy_returns, label='Strategy Returns')\nplt.legend()\nplt.show()\n\n# Not fully correct but good starting point. Just wanted to start familiarizing myself with the library and the data."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/classifying-penguins/ClassifyingPenguins.html",
    "href": "posts/classifying-penguins/ClassifyingPenguins.html",
    "title": "Classifying Penguins",
    "section": "",
    "text": "Blog Post 1: Classifying Palmer Penguins\nAbstract:\nThis post seeks to identify what qualitative and quantative features we should select to achieve a 100% accuracy rate using a machine learning model (specifically a logistic regression model) to predict a penguin’s species. We exhaustively test combinations of available features to find the combination that leads to the highest accuracy on training data. The features found to lead to a model with 100% accuracy on the test dataset were the island a penguin was on, the penguins’ culmen length, and their culmen depth.\nI have neither given nor received unauthorized aid on this assignment - James Ohr\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\ntrain.head()\n\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\nThe below code preprocesses the data: it assigns a unique numerical value to each unique category in the “Species” column, removes variables that are not of interest, removes observations where the value for “Sex” is “.”, and removes any observations with missing variable values.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nX_train.head()\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\n0\n1\n0\n1\n0\n1\n1\n0\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\n0\n1\n0\n1\n0\n1\n0\n1\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\n1\n0\n0\n1\n0\n1\n0\n1\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\n1\n0\n0\n1\n0\n1\n1\n0\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\n0\n1\n0\n1\n0\n1\n0\n1\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\n#Creates a table showing means by island\ntrain.groupby([\"Island\", \"Species\"]).aggregate('mean')\n\nC:\\Users\\james\\AppData\\Local\\Temp\\ipykernel_33480\\3605326083.py:3: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  train.groupby([\"Island\", \"Species\"]).aggregate('mean')\n\n\n\n\n\n\n\n\n\n\n\nSample Number\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\n\n\nIsland\nSpecies\n\n\n\n\n\n\n\n\n\n\n\nBiscoe\nAdelie Penguin (Pygoscelis adeliae)\n66.030303\n38.845455\n18.475758\n188.636364\n3711.363636\n8.788643\n-25.920138\n\n\nGentoo penguin (Pygoscelis papua)\n60.244898\n47.073196\n14.914433\n216.752577\n5039.948454\n8.247341\n-26.149389\n\n\nDream\nAdelie Penguin (Pygoscelis adeliae)\n90.622222\n38.826667\n18.306667\n190.133333\n3728.888889\n8.933945\n-25.769529\n\n\nChinstrap penguin (Pygoscelis antarctica)\n35.631579\n48.826316\n18.366667\n196.000000\n3743.421053\n9.331004\n-24.553401\n\n\nTorgersen\nAdelie Penguin (Pygoscelis adeliae)\n67.547619\n39.229268\n18.468293\n191.195122\n3712.804878\n8.846768\n-25.715095\n\n\n\n\n\n\n\n\nThe above table shows that each island has a different combination of penguin species. It’s worth nothing that within each island, the different penguin species seem to have notable differences in culmen length and flipper length. The only island on which body mass and culmen depth distinguish its two species of resident penguins is Biscoe.\n\n#Creates a scatterplot whose axes are culmen depth and body mass, with points colored by species\nsns.scatterplot(data=train, x=\"Culmen Depth (mm)\", y=\"Body Mass (g)\", hue=\"Species\")\n\n\n\n\n\n\n\n\nThe above figure shows that there’s a clear divide between Gentoo penguins and the other two penguin species in body mass–Gentoo penguins are heavier regardless of culmen depth. But controlling for the species, there’s a clear positive linear relationship between body mass and culmen depth. Adelie and Chinstrap penguins seem to have the same linear relationship between body mass and culmen depth.\n\n#Creates a scatterplot whose axes are flipper length and body mass, with points colored by island\nsns.scatterplot(data=train, x=\"Flipper Length (mm)\", y=\"Body Mass (g)\", hue=\"Island\")\n\n\n\n\n\n\n\n\nThe above figure shows that regardless of the island a penguin is from, there’s a linear relationship between body mass and flipper length. But while penguins on Biscoe seem to have a greater range of body mass and flipper length, whereas penguins on Dream and Torgersen occupy a similar, tighter range.\nTraining the Model\nNow we choose three features of the data to achieve 100% testing accuracy. To do so, we use a nested for-loop to construct every possible combination of 1 qualitative feature and 2 quantitative features to use in training a logistic regression model. The outer for-loop sets a qualitative variable to use. The inner for-loop take a pair of quantitative variables from the all_quant_cols array.\nIncluded in the inner loop is an if-statement that ensures we keep track of the features that lead to the highest score. Those features are stored in the best_cols variable.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nbest_cols = [] #keeps track of the best features to use\nmax_score = 0 #keeps track of highest score so far\n\nall_qual_cols = ['Clutch Completion', 'Island']\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\nLR = LogisticRegression(max_iter=10000)\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair)\n    LR.fit(X_train[cols], y_train)\n    print(cols)\n    print(LR.score(X_train[cols], y_train))\n    if LR.score(X_train[cols], y_train) &gt; max_score:\n      max_score = LR.score(X_train[cols], y_train)\n      best_cols = cols\n\nLR.fit(X_train[best_cols], y_train)\n\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n0.96484375\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Flipper Length (mm)']\n0.95703125\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Body Mass (g)']\n0.9453125\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n0.81640625\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Body Mass (g)']\n0.76953125\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Flipper Length (mm)', 'Body Mass (g)']\n0.63671875\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n0.99609375\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Flipper Length (mm)']\n0.9765625\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Body Mass (g)']\n0.9765625\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n0.8828125\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Depth (mm)', 'Body Mass (g)']\n0.8359375\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Flipper Length (mm)', 'Body Mass (g)']\n0.75390625\n\n\nLogisticRegression(max_iter=10000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(max_iter=10000)\n\n\nCross-Validation\nThe below code uses a scikit-learn method to calculate cross-validation scores with five folds. The results are largely promising, with the lowest accuracy rate being 96%.\n\nfrom sklearn.model_selection import cross_val_score\n\ncv_scores_LR = cross_val_score(LR, X_train[best_cols], y_train, cv=5)\ncv_scores_LR\n\narray([0.98076923, 1.        , 1.        , 0.96078431, 1.        ])\n\n\nTesting\nThe below code loads test.csv, preprocesses the data with prepare_data, and scores the LR on the test data. The result is 100% accuracy.\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[best_cols], y_test)\n\n1.0\n\n\nDecision Regions\n\n#This section of code retrains the model on cols, which has the same features as best_col, just\n#with the columns rearranged so that Matplotlib doesn't return an error later in the next code block.\n\ncols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nLR = LogisticRegression(max_iter=10000)\nLR.fit(X_train[cols], y_train)\n\nLogisticRegression(max_iter=10000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(max_iter=10000)\n\n\nThe below code defines a function, plot_regions, that creates multiple charts (the # is determined by how many qualitative variables there are), each with the same axes. The colors of different regions correspond to the species predictions made by the model, whereas the colors of observations correspond to actual species values.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # Creates a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nplot_regions(LR, X_train[cols], y_train)\n\n\n\n\n\n\n\n\nAbove are the decision regions for the training data. We effectively control for the island a penguin is from to determine their species based on their culmen depth and length. The only inaccurately labeled point is a penguin on Dream with a slightly shorter culmen than its depth and would predict.\n\nplot_regions(LR, X_test[cols], y_test)\n\n\n\n\n\n\n\n\nOur 100% accuracy is reflected here–every plotted observation is correctly categorized into the region with the corresponding color.\nConfusion Matrix\nBelow is code to create a confusion matrix for our results. Given that there were no false positives or false negatives, we see a confusion matrix with non-zeros only on the diagonal.\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR.predict(X_test[cols])\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]], dtype=int64)\n\n\nDiscussion\nIn this post, we found that, with a logistic regression model, the qualitative feature “Island” combined with culmen depth and culmen length can predict with great accuracy (100% accuracy on our test data) whether the penguin is of the Adelie, Chinstrap, or Gentoo species.\nThe original figures I created earlier in the post actually weren’t helpful in finding the best features. The body mass and flipper length relationship in particular was something that intuitively seemed right to me, but it wasn’t actually useful in helping predict penguin species.\nThrough this blog post, I’ve learned some basic features of scikit-learn, seaborn, Pandas, and Matplotlib. I’m not sure if I’m just not sufficiently comfortable with Matplotlib, but it gave me more trouble than the rest of the entire project. I had to re-train the model on a re-arranged set of column names (instead of best_cols) just to get it to stop giving me the same error. The most valuable aspect of it, I think, was just getting a better hang of the syntax and gaining familiarity with the tools we use in the class, especially scikit-learn."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Applying Machine Learning to Stock Price Prediction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDonovan test code just to get started\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNewton’s Method for Logistic Regression\n\n\n\n\n\nA blog post about implementing and testing Newton’s method for logistic regression.\n\n\n\n\n\nMay 5, 2024\n\n\nJames Ohr\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Logistic Regression\n\n\n\n\n\nA blog post about implementing and testing the logistic regression algorithm.\n\n\n\n\n\nMay 2, 2024\n\n\nJames Ohr\n\n\n\n\n\n\n\n\n\n\n\n\nSparse Kernel Machines\n\n\n\n\n\nA blog post about implementing and testing sparse kernelized logistic regression.\n\n\n\n\n\nMay 1, 2024\n\n\nJames Ohr\n\n\n\n\n\n\n\n\n\n\n\n\nQuantitative Trading Model Using LSTM\n\n\n\n\n\nFinal Project for CS451\n\n\n\n\n\nApr 22, 2024\n\n\nAndre Xiao, James Ohr, Donovan Wood\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing the Perceptron Algorithm\n\n\n\n\n\nA blog post about implementing and testing the perceptron algorithm.\n\n\n\n\n\nApr 6, 2024\n\n\nJames Ohr\n\n\n\n\n\n\n\n\n\n\n\n\nDecision Making\n\n\n\n\n\nA blog post about optimal decisionmaking in the context of bank loans.\n\n\n\n\n\nMar 30, 2024\n\n\nJames Ohr\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Penguins\n\n\n\n\n\nA blog post about classifying penguin species based on physical characteristics.\n\n\n\n\n\nMar 29, 2024\n\n\nJames Ohr\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/decision-making/DecisionMaking.html",
    "href": "posts/decision-making/DecisionMaking.html",
    "title": "Decision Making",
    "section": "",
    "text": "Introduction: In this post, I analyze data, construct a logistic regression model, and make predictions on a test dataset to understand a simplified but relatively realistic decisionmaking process to find whether to make a loan to a borrower given information on their personal finances, their requested loan, and their employment history. I find a bank can maximize its profits by lending to a small number of higher-income individuals with smaller requested loans, longer credit histories, and more years of employment. Loans for different purposes have varying default rates and are correspondingly approved at different rates–medical loans in particular are more often rejected but have the highest default rates. Using a definition of fairness related to minimizing harm, I argue that it’s reasonable for a bank to have lower approval ratings for medical loans so it can minimize harm to its own stakeholders.\nI have neither given nor received unauthorized aid on this assignment - James Ohr\n\n\n\n#Importing the data and storing it in variable df_train\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\n\n\n\n\ndf_train.head()\n\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n\n\n\n\n\n\nThe above table helps me get a sense of the variables in the dataset and their types.\n\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ng = sns.displot(df_train, x=\"loan_percent_income\", hue=\"loan_status\", element=\"step\", legend=False)\nplt.legend(title='Loan Status', loc='upper right', labels=['Default', 'No Default'])\ng.set(xlabel=\"Loan Proportion of Income\")\nplt.show(g)\n\n\n\n\n\n\n\n\nThe above figure shows that there are * significantly more borrowers who didn’t default than those who did * individuals who receive a loan less than 30% of their income are unlikely to default on their loan * individuals who receive a loan over 30% of their income are significantly more likely to default than not.\n\ng = sns.displot(df_train, x=\"loan_int_rate\", hue=\"loan_intent\", kind=\"kde\")\ng.legend.set_title(\"Loan Purpose\")\ng.set(xlabel=\"Loan Interest Rate (%)\")\nplt.show(g)\n\n\n\n\n\n\n\n\nThe above figure shows an interesting pattern across every single type of loan. Each distribution is approximately bimodal (although some have three local maxima). It’s unsurprising, but there are far more people under the second peak, i.e., the higher interest rate peak, for each type of loan.\n\ndf_train.groupby(\"loan_intent\").mean(numeric_only=True)\n\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_cred_hist_length\n\n\nloan_intent\n\n\n\n\n\n\n\n\n\n\n\n\nDEBTCONSOLIDATION\n27.588798\n66693.453327\n4.759419\n9620.901149\n10.983305\n0.287458\n0.170869\n5.695548\n\n\nEDUCATION\n26.597620\n63847.711917\n4.440192\n9460.015604\n10.965465\n0.173396\n0.169352\n5.141603\n\n\nHOMEIMPROVEMENT\n28.981737\n73082.079600\n5.103754\n10348.725017\n11.160075\n0.264645\n0.166733\n6.430048\n\n\nMEDICAL\n27.950982\n61314.583868\n4.782062\n9242.269907\n11.051946\n0.263289\n0.172825\n5.913547\n\n\nPERSONAL\n28.288339\n68070.502495\n4.897997\n9549.427178\n11.009814\n0.193739\n0.168671\n6.151316\n\n\nVENTURE\n27.588643\n66098.818162\n4.877869\n9516.417425\n10.940866\n0.148678\n0.170130\n5.744040\n\n\n\n\n\n\n\n\nBorrowers intending to use loans for home improvement tend to: * have higher incomes * receive greater loans * tend to have higher default rates, which are in line with default rates of loans for medical and debt consolidation.\nMedical loan borrowers tend to have the lowest incomes, the highest loan-to-income ratios, and the lowest loan amounts. The first observation is most surprising–I would have expected education loan borrowers to have the lowest incomes (maybe this has to do with co-signers).\n\n\n\nThe below code pre-processes our data for use in the model. We drop the “loan_grade” variable, which the instructions disallow for building th emodel. We separate out “loan status” into an outcome variable, y. We transform categorical variables from strings into numbers.\n\n#Data pre-processing\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\ndef prepare_data(df):\n  df = df_train.drop([\"loan_grade\"], axis = 1)\n  df = df.dropna()\n  le.fit(df_train[\"person_home_ownership\"])\n  df[\"person_home_ownership\"] = le.transform(df[\"person_home_ownership\"])\n  le.fit(df_train[\"loan_intent\"])\n  df[\"loan_intent\"] = le.transform(df[\"loan_intent\"])\n  y = df[\"loan_status\"]\n  df = df.drop([\"loan_status\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(df_train)\ndf_train = df_train.dropna()\n\nBelow, I chose the features for the model and fitted our model to our training data. I chose income and loan amount because they’re the two components of the loan-to-income ratio, which seemed to be relatively revealing in the above chart showing loan status vs. loan-to-income. Adding employment length didn’t actually change the accuracy of the model, but I added it just because intuitively I thought it might have an impact even though the table above didn’t show anything particularly interesting regarding the variable.\n\n#Choosing features\nfrom sklearn.linear_model import LogisticRegression\n\nLR = LogisticRegression()\n\ncols = [\"person_income\", \"loan_amnt\", \"person_emp_length\"]\n\nLR.fit(X_train[cols], y_train)\nLR.score(X_train[cols], y_train)\nLR.coef_[0]\n\narray([-4.05735465e-05,  1.06559046e-04, -2.48736069e-08])\n\n\n\n\n\nBelow we have code that (1) finds the profit the bank makes from all of its loans (we ignore any NAs) and (2) identifies the threshold that maximizes profit.\n\n#This function, taken from the week 2 lecture notes and just slightly modified to allow for a third x variable, calculates the risk score for a borrower. \ndef linear_score(w, x0, x1, x2):\n    return w[0]*x0 + w[1]*x1 + w[2]*x2\n\n\n#Predict makes binary predictions for data using a supplied score function with weights w and a supplied threshold. Taken from lecture notes from week 2.\n#We begin with a 0 threshold but later on test others to find an optimal threshold\n\nt = 0\n\ndef predict(score_fun, w, threshold, df):\n    \"\"\"\n    make binary predictions for data df using a supplied score function with weights w and supplied threshold. \n    \"\"\"\n    scores = score_fun(w, df[\"person_income\"], df[\"loan_amnt\"], df[\"person_emp_length\"])\n    return 1*(scores &gt; threshold)\n\ndf_train[\"decision\"] = predict(linear_score, LR.coef_[0], t, df_train)\n(df_train[\"decision\"] == df_train[\"loan_status\"]).mean()\n\n0.8080062862880342\n\n\n\n# Creating a funciton to find profit\n#The loan_int_rate variable is expressed as a percentage, so we divide it by 100 to make it a regular proportion instead in both helper variables\n\n#Helper function to calculate profit for when loans are repaid, using the provided formula\ndef calculateGain(loan_amnt, loan_int_rate):\n    return np.sum(loan_amnt*(1 + 0.25*loan_int_rate/100)**10 - loan_amnt) \n\n#Helper function to calculate loss for when loans are defaulted on, using the provided formula\ndef calculateLoss(loan_amnt, loan_int_rate):\n    return np.sum(loan_amnt*(1 + 0.25*loan_int_rate/100)**3 - 1.7*loan_amnt)\n    \ndef find_profit(df):\n    #Below df_repaid and df_default are created to select only the data points the model chooses\n    df_repaid = df[df[\"loan_status\"] == 0]\n    df_repaid = df_repaid[df_repaid[\"decision\"] == 0]\n    df_default = df[df[\"loan_status\"] == 1]\n    df_default = df_default[df_default[\"decision\"] == 0]\n    \n    return calculateGain(df_repaid[\"loan_amnt\"], df_repaid[\"loan_int_rate\"]) + calculateLoss(df_default[\"loan_amnt\"], df_default[\"loan_int_rate\"])\n\nfind_profit(df_train)\n\n25068803.47746343\n\n\nThe below code runs find_profit on every integer from -100 to 100 to try to find the threshold that gets us the highest profit. We find that a threshold of -1 corresponds to the highest profit value, which is about $25 million. To find this threshold, we use the idxmax() function, which gets us the first occurance of a maximum over an axis. In this case, it helps us find whatever threshold corresponds to the highest profit.\n\niterations = 200\npredictions = []\nfor i in range(iterations):\n    threshold = (-iterations/2)+(i)\n    df_train[\"decision\"] = predict(linear_score, LR.coef_[0], threshold, df_train)\n    predictions.append((threshold, find_profit(df_train)))\n\n\npredictions_df = pd.DataFrame(data=predictions)\npredictions_df.columns =['Threshold', 'Profit']\n\nsns.relplot(data=predictions_df, x=\"Threshold\", y=\"Profit\")\npredictions_df['Threshold'][predictions_df['Profit'].idxmax()], predictions_df['Profit'].max()\n\n\n\n\n\n\n\n\nWe obtain from our output above the chart (-1.0, 25755817.698476546) that the profit-maximizing threshold is -1.\n\n\n\n\n#In these code block, we find for the training set the total profit of our model, the profit per approved borrower, and the accuracy rate.\nfinal_threshold = -1\n\ndf_train[\"decision\"] = predict(linear_score, LR.coef_[0], final_threshold, df_train)\nborrowers_count = df_train[df_train[\"decision\"]==0].count()[\"loan_int_rate\"]\nfind_profit(df_train), find_profit(df_train)/borrowers_count, (df_train[\"decision\"] == df_train[\"loan_status\"]).mean()\n\n(25755817.698476546, 1819.0421426990993, 0.6927576723272362)\n\n\n\n#In these code block, we find for the test set the total profit of our model, the profit per approved borrower, and the accuracy rate.\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\n\ndf_test[\"decision\"] = predict(linear_score, LR.coef_[0], final_threshold, df_test)\nborrowers_count = df_test[df_test[\"decision\"]==0].count()[\"loan_int_rate\"]\nfind_profit(df_test), find_profit(df_test)/borrowers_count, (df_test[\"decision\"] == df_test[\"loan_status\"]).mean()\n\n(5965836.794978274, 1632.2398891869423, 0.6898879852692957)\n\n\nWhat is the expected profit per borrower on the test set? Is it similar to your profit on the training set?\nThe profit per borrower was lower in the test dataset ($1,632) than the training dataset ($1,819), but the accuracy was similar, where accuracy is defined as the % of the time our default prediction actually matched the borrower’s default status.\nTraining accuracy: 69.3%\nTesting accuracy: 69.0%\n\n\n\n\ndf_test.groupby([\"loan_intent\"])[[\"loan_status\", \"decision\"]].mean(numeric_only=True)\n\n\n\n\n\n\n\n\n\nloan_status\ndecision\n\n\nloan_intent\n\n\n\n\n\n\nDEBTCONSOLIDATION\n0.279497\n0.386847\n\n\nEDUCATION\n0.167421\n0.401961\n\n\nHOMEIMPROVEMENT\n0.246088\n0.270270\n\n\nMEDICAL\n0.281553\n0.406958\n\n\nPERSONAL\n0.219227\n0.385445\n\n\nVENTURE\n0.145701\n0.361086\n\n\n\n\n\n\n\n\nThe above table shows us the default rates by loan category, as well as what proportion of borrowers were approved by our model. Medical loans have the highest rejection rates and the highest default rates.\n\ndf_test.groupby([\"decision\"]).mean(numeric_only=True)\n\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_cred_hist_length\n\n\ndecision\n\n\n\n\n\n\n\n\n\n\n\n\n0\n28.064977\n82754.051932\n5.244782\n8949.870785\n10.709193\n0.123554\n0.115412\n6.052670\n\n\n1\n27.189894\n39522.220049\n4.133252\n10859.555827\n11.528797\n0.381011\n0.263394\n5.496333\n\n\n\n\n\n\n\n\nThe above table shows us the different average statistics of approved vs. rejected borrowers. Approved borrowers tend to be slightly older, have much higher income, have longer employment histories, ask for smaller loans, ask for loans that represent a smaller proportion of their income, and have longer credit histories.\n\nbins = np.array([1,20,40,60,80,100])\ndf_test[\"person_age\"] = pd.cut(df_test[\"person_age\"].astype('Int64'), bins, include_lowest=True)\n\n\ng = sns.barplot(data=df_test, x=\"person_age\", y=\"decision\", color='blue')\ng.set(xlabel=\"Person Age Range\", ylabel=\"% of Age Range Rejected (w/ Confidence Intervals)\")\nplt.show(g)\n\n\n\n\n\n\n\n\nThe above table shows the average decision rating for each age range (bucketed by 20 years). The Y axis shows what % of the age range is rejected. The lines on the bars indicate 95% confidence intervals.\nQuestions: 1. Is it more difficult for people in certain age groups to access credit under your proposed system? 2. Is it more difficult for people to get loans in order to pay for medical expenses? 2a. How does this compare with the actual rate of default in that group? 2b. What about people seeking loans for business ventures or education? 3. How does a person’s income level impact the ease with which they can access credit under your decision system?\nResponse:\nFor the purpose of this discussion, we will consider a loan “rejected” if the model deems the loan too-high risk. All statistics are based on the test dataset.\n\nWhile the 20-40 and 40-60 age ranges seem to have about the same access to credit, the 60-80 age group has a significantly higher chance of being rejected–i.e., have lower access to credit. Younger borrowers seem to have the greatest variability in access to credit but generally have high access\nCompared to loans for other purposes, loans for medical expenses are frequently considered too high-risk to make according to our model. Our model is, in a sense, generous: 28.2% of medical borrowers defaulted, but 40.7% were considered too high-risk. In contrast, business venture loans were denied 36.1% of the time with 14.6% risk of default, so the default rate and rejection rate were even further apart. For loans for education, 40.2% of borrowers were rejected, while 16.7% of borrowers actually defaulted. So even though a 40.7% rejection rate for medical borrowers is higher than loans for other purposes, the rejection rate seems reasonable when we consider the high default rate of medical loans.\nThe higher the income, the lower the perceived risk by the model. This is clearest in the simple table above that groups individuals by the “decision” column results. The borrowers the model would accept have about 13 more months of work experience, request loans that are smaller by about $2,000, and have credit histories that are 6 months longer. But the starkest differences are in the income of the borrower and (2) the percentage of income the loan represented, which is directly related to income. The income of approved borrowers is about double the income of rejected borrowers. The loan-to-income ratio is about three times higher for rejected borrowers than approved borrowers.\n\n\n\n\nDiscussion on Fairness\nConsidering that people seeking loans for medical expense have high rates of default, is it fair that it is more difficult for them to obtain access to credit?\nFairness\nFairness in decision-making is to make choices that minimize harm to all parties, with particular regard to parties with low/no control.\nControl\nThe bank has full control over whether to make the loan, so they clearly have substantial control. The borrower has far less control but not no control–excluding cases in which people have a serious illness / injury that prevents them from working, they have some level of control over their personal finances, particularly their income. Still, the borrower has likely little control over the size of the loan they need given the nature of medical expenses and, relatedly, the percentage of income their loan constitutes. So in this way, it’s unlikely that a perfectly fair decision can be made. But the bank has to make a decision, and so the goal should be to achieve a target that gets as close to fairness as possible.\nHarm\nIt is not only the borrower who can experience harm in this scenario by being denied a loan. The bank experiences financial harm by lending to a borrower with excessively high risk. This harm is harder to see immediately–it’s distributed in the form of risk or loss across shareholders, management, employees, depositors, lenders, etc, but it’s harm nonetheless. And if a bank were to repeatedly make loans with negative expected value, it risks serious harm to those stakeholders (e.g., people losing savings in the bank’s stock; employees losing jobs; depositors losing uninsured deposits).\nIt’s possible the bank could just make loans with lower expected value instead, not negative expected value. But even that could mean a decline in the business over time as competitors generate greater profits and reinvest those profits to become more competitive, harming this bank’s business and hence its stakeholders.\nIn conclusion, balancing the benefit to the bank’s stakeholders with the potential harm to rejected borrowers, I conclude that it is fair for medical loan borrowers to have more difficulty accessing credit.\nTo reach a conclusion about whether this specific model and its decisions are fair, we would have to make assumptions about the level of harm being done to rejected borrowers and the benefit to stakeholders in the bank from those borrowers being rejected.\nReflection on Blog Post\nI found that it is possible to construct a logistic regression model that can predict borrower outcomes with relative accuracy. One of the most interesting findings was that in this case, to maximize our target variable, we actually didn’t optimize accuracy. A threshold of 0 yields around 80% accuracy on the training set, far higher than our profit-maximizing threshold of -1, which yields 69% accuracy on the training set. I learned more about how to visualize data and had to grapple with the best ways to visualize so many variables. I ultimately concluded that I would keep my graphs relatively simple and show a more complete story via tables. I learned to work with subsets of my own data: Part D in particular was tricky for me because I had trouble finding a way to construct the find_profit function.\nUnlike the Classifying Penguins post, there was a lot less guidance, which I think was helpful in forcing me to edit and therefore engage further with the example code given previously (e.g., pre-processing the data at the start of Part C)"
  },
  {
    "objectID": "posts/decision-making/DecisionMaking.html#part-a-grab-the-data",
    "href": "posts/decision-making/DecisionMaking.html#part-a-grab-the-data",
    "title": "Decision Making",
    "section": "",
    "text": "#Importing the data and storing it in variable df_train\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)"
  },
  {
    "objectID": "posts/decision-making/DecisionMaking.html#part-b-explore-the-data",
    "href": "posts/decision-making/DecisionMaking.html#part-b-explore-the-data",
    "title": "Decision Making",
    "section": "",
    "text": "df_train.head()\n\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n\n\n\n\n\n\nThe above table helps me get a sense of the variables in the dataset and their types.\n\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ng = sns.displot(df_train, x=\"loan_percent_income\", hue=\"loan_status\", element=\"step\", legend=False)\nplt.legend(title='Loan Status', loc='upper right', labels=['Default', 'No Default'])\ng.set(xlabel=\"Loan Proportion of Income\")\nplt.show(g)\n\n\n\n\n\n\n\n\nThe above figure shows that there are * significantly more borrowers who didn’t default than those who did * individuals who receive a loan less than 30% of their income are unlikely to default on their loan * individuals who receive a loan over 30% of their income are significantly more likely to default than not.\n\ng = sns.displot(df_train, x=\"loan_int_rate\", hue=\"loan_intent\", kind=\"kde\")\ng.legend.set_title(\"Loan Purpose\")\ng.set(xlabel=\"Loan Interest Rate (%)\")\nplt.show(g)\n\n\n\n\n\n\n\n\nThe above figure shows an interesting pattern across every single type of loan. Each distribution is approximately bimodal (although some have three local maxima). It’s unsurprising, but there are far more people under the second peak, i.e., the higher interest rate peak, for each type of loan.\n\ndf_train.groupby(\"loan_intent\").mean(numeric_only=True)\n\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_cred_hist_length\n\n\nloan_intent\n\n\n\n\n\n\n\n\n\n\n\n\nDEBTCONSOLIDATION\n27.588798\n66693.453327\n4.759419\n9620.901149\n10.983305\n0.287458\n0.170869\n5.695548\n\n\nEDUCATION\n26.597620\n63847.711917\n4.440192\n9460.015604\n10.965465\n0.173396\n0.169352\n5.141603\n\n\nHOMEIMPROVEMENT\n28.981737\n73082.079600\n5.103754\n10348.725017\n11.160075\n0.264645\n0.166733\n6.430048\n\n\nMEDICAL\n27.950982\n61314.583868\n4.782062\n9242.269907\n11.051946\n0.263289\n0.172825\n5.913547\n\n\nPERSONAL\n28.288339\n68070.502495\n4.897997\n9549.427178\n11.009814\n0.193739\n0.168671\n6.151316\n\n\nVENTURE\n27.588643\n66098.818162\n4.877869\n9516.417425\n10.940866\n0.148678\n0.170130\n5.744040\n\n\n\n\n\n\n\n\nBorrowers intending to use loans for home improvement tend to: * have higher incomes * receive greater loans * tend to have higher default rates, which are in line with default rates of loans for medical and debt consolidation.\nMedical loan borrowers tend to have the lowest incomes, the highest loan-to-income ratios, and the lowest loan amounts. The first observation is most surprising–I would have expected education loan borrowers to have the lowest incomes (maybe this has to do with co-signers)."
  },
  {
    "objectID": "posts/decision-making/DecisionMaking.html#part-c-build-a-model",
    "href": "posts/decision-making/DecisionMaking.html#part-c-build-a-model",
    "title": "Decision Making",
    "section": "",
    "text": "The below code pre-processes our data for use in the model. We drop the “loan_grade” variable, which the instructions disallow for building th emodel. We separate out “loan status” into an outcome variable, y. We transform categorical variables from strings into numbers.\n\n#Data pre-processing\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\ndef prepare_data(df):\n  df = df_train.drop([\"loan_grade\"], axis = 1)\n  df = df.dropna()\n  le.fit(df_train[\"person_home_ownership\"])\n  df[\"person_home_ownership\"] = le.transform(df[\"person_home_ownership\"])\n  le.fit(df_train[\"loan_intent\"])\n  df[\"loan_intent\"] = le.transform(df[\"loan_intent\"])\n  y = df[\"loan_status\"]\n  df = df.drop([\"loan_status\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(df_train)\ndf_train = df_train.dropna()\n\nBelow, I chose the features for the model and fitted our model to our training data. I chose income and loan amount because they’re the two components of the loan-to-income ratio, which seemed to be relatively revealing in the above chart showing loan status vs. loan-to-income. Adding employment length didn’t actually change the accuracy of the model, but I added it just because intuitively I thought it might have an impact even though the table above didn’t show anything particularly interesting regarding the variable.\n\n#Choosing features\nfrom sklearn.linear_model import LogisticRegression\n\nLR = LogisticRegression()\n\ncols = [\"person_income\", \"loan_amnt\", \"person_emp_length\"]\n\nLR.fit(X_train[cols], y_train)\nLR.score(X_train[cols], y_train)\nLR.coef_[0]\n\narray([-4.05735465e-05,  1.06559046e-04, -2.48736069e-08])"
  },
  {
    "objectID": "posts/decision-making/DecisionMaking.html#part-d-find-a-threshold",
    "href": "posts/decision-making/DecisionMaking.html#part-d-find-a-threshold",
    "title": "Decision Making",
    "section": "",
    "text": "Below we have code that (1) finds the profit the bank makes from all of its loans (we ignore any NAs) and (2) identifies the threshold that maximizes profit.\n\n#This function, taken from the week 2 lecture notes and just slightly modified to allow for a third x variable, calculates the risk score for a borrower. \ndef linear_score(w, x0, x1, x2):\n    return w[0]*x0 + w[1]*x1 + w[2]*x2\n\n\n#Predict makes binary predictions for data using a supplied score function with weights w and a supplied threshold. Taken from lecture notes from week 2.\n#We begin with a 0 threshold but later on test others to find an optimal threshold\n\nt = 0\n\ndef predict(score_fun, w, threshold, df):\n    \"\"\"\n    make binary predictions for data df using a supplied score function with weights w and supplied threshold. \n    \"\"\"\n    scores = score_fun(w, df[\"person_income\"], df[\"loan_amnt\"], df[\"person_emp_length\"])\n    return 1*(scores &gt; threshold)\n\ndf_train[\"decision\"] = predict(linear_score, LR.coef_[0], t, df_train)\n(df_train[\"decision\"] == df_train[\"loan_status\"]).mean()\n\n0.8080062862880342\n\n\n\n# Creating a funciton to find profit\n#The loan_int_rate variable is expressed as a percentage, so we divide it by 100 to make it a regular proportion instead in both helper variables\n\n#Helper function to calculate profit for when loans are repaid, using the provided formula\ndef calculateGain(loan_amnt, loan_int_rate):\n    return np.sum(loan_amnt*(1 + 0.25*loan_int_rate/100)**10 - loan_amnt) \n\n#Helper function to calculate loss for when loans are defaulted on, using the provided formula\ndef calculateLoss(loan_amnt, loan_int_rate):\n    return np.sum(loan_amnt*(1 + 0.25*loan_int_rate/100)**3 - 1.7*loan_amnt)\n    \ndef find_profit(df):\n    #Below df_repaid and df_default are created to select only the data points the model chooses\n    df_repaid = df[df[\"loan_status\"] == 0]\n    df_repaid = df_repaid[df_repaid[\"decision\"] == 0]\n    df_default = df[df[\"loan_status\"] == 1]\n    df_default = df_default[df_default[\"decision\"] == 0]\n    \n    return calculateGain(df_repaid[\"loan_amnt\"], df_repaid[\"loan_int_rate\"]) + calculateLoss(df_default[\"loan_amnt\"], df_default[\"loan_int_rate\"])\n\nfind_profit(df_train)\n\n25068803.47746343\n\n\nThe below code runs find_profit on every integer from -100 to 100 to try to find the threshold that gets us the highest profit. We find that a threshold of -1 corresponds to the highest profit value, which is about $25 million. To find this threshold, we use the idxmax() function, which gets us the first occurance of a maximum over an axis. In this case, it helps us find whatever threshold corresponds to the highest profit.\n\niterations = 200\npredictions = []\nfor i in range(iterations):\n    threshold = (-iterations/2)+(i)\n    df_train[\"decision\"] = predict(linear_score, LR.coef_[0], threshold, df_train)\n    predictions.append((threshold, find_profit(df_train)))\n\n\npredictions_df = pd.DataFrame(data=predictions)\npredictions_df.columns =['Threshold', 'Profit']\n\nsns.relplot(data=predictions_df, x=\"Threshold\", y=\"Profit\")\npredictions_df['Threshold'][predictions_df['Profit'].idxmax()], predictions_df['Profit'].max()\n\n\n\n\n\n\n\n\nWe obtain from our output above the chart (-1.0, 25755817.698476546) that the profit-maximizing threshold is -1."
  },
  {
    "objectID": "posts/decision-making/DecisionMaking.html#part-e-evaluate-your-model-from-the-banks-perspective",
    "href": "posts/decision-making/DecisionMaking.html#part-e-evaluate-your-model-from-the-banks-perspective",
    "title": "Decision Making",
    "section": "",
    "text": "#In these code block, we find for the training set the total profit of our model, the profit per approved borrower, and the accuracy rate.\nfinal_threshold = -1\n\ndf_train[\"decision\"] = predict(linear_score, LR.coef_[0], final_threshold, df_train)\nborrowers_count = df_train[df_train[\"decision\"]==0].count()[\"loan_int_rate\"]\nfind_profit(df_train), find_profit(df_train)/borrowers_count, (df_train[\"decision\"] == df_train[\"loan_status\"]).mean()\n\n(25755817.698476546, 1819.0421426990993, 0.6927576723272362)\n\n\n\n#In these code block, we find for the test set the total profit of our model, the profit per approved borrower, and the accuracy rate.\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\n\ndf_test[\"decision\"] = predict(linear_score, LR.coef_[0], final_threshold, df_test)\nborrowers_count = df_test[df_test[\"decision\"]==0].count()[\"loan_int_rate\"]\nfind_profit(df_test), find_profit(df_test)/borrowers_count, (df_test[\"decision\"] == df_test[\"loan_status\"]).mean()\n\n(5965836.794978274, 1632.2398891869423, 0.6898879852692957)\n\n\nWhat is the expected profit per borrower on the test set? Is it similar to your profit on the training set?\nThe profit per borrower was lower in the test dataset ($1,632) than the training dataset ($1,819), but the accuracy was similar, where accuracy is defined as the % of the time our default prediction actually matched the borrower’s default status.\nTraining accuracy: 69.3%\nTesting accuracy: 69.0%"
  },
  {
    "objectID": "posts/decision-making/DecisionMaking.html#part-f-evaluate-your-model-from-the-borrowers-perspective",
    "href": "posts/decision-making/DecisionMaking.html#part-f-evaluate-your-model-from-the-borrowers-perspective",
    "title": "Decision Making",
    "section": "",
    "text": "df_test.groupby([\"loan_intent\"])[[\"loan_status\", \"decision\"]].mean(numeric_only=True)\n\n\n\n\n\n\n\n\n\nloan_status\ndecision\n\n\nloan_intent\n\n\n\n\n\n\nDEBTCONSOLIDATION\n0.279497\n0.386847\n\n\nEDUCATION\n0.167421\n0.401961\n\n\nHOMEIMPROVEMENT\n0.246088\n0.270270\n\n\nMEDICAL\n0.281553\n0.406958\n\n\nPERSONAL\n0.219227\n0.385445\n\n\nVENTURE\n0.145701\n0.361086\n\n\n\n\n\n\n\n\nThe above table shows us the default rates by loan category, as well as what proportion of borrowers were approved by our model. Medical loans have the highest rejection rates and the highest default rates.\n\ndf_test.groupby([\"decision\"]).mean(numeric_only=True)\n\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_cred_hist_length\n\n\ndecision\n\n\n\n\n\n\n\n\n\n\n\n\n0\n28.064977\n82754.051932\n5.244782\n8949.870785\n10.709193\n0.123554\n0.115412\n6.052670\n\n\n1\n27.189894\n39522.220049\n4.133252\n10859.555827\n11.528797\n0.381011\n0.263394\n5.496333\n\n\n\n\n\n\n\n\nThe above table shows us the different average statistics of approved vs. rejected borrowers. Approved borrowers tend to be slightly older, have much higher income, have longer employment histories, ask for smaller loans, ask for loans that represent a smaller proportion of their income, and have longer credit histories.\n\nbins = np.array([1,20,40,60,80,100])\ndf_test[\"person_age\"] = pd.cut(df_test[\"person_age\"].astype('Int64'), bins, include_lowest=True)\n\n\ng = sns.barplot(data=df_test, x=\"person_age\", y=\"decision\", color='blue')\ng.set(xlabel=\"Person Age Range\", ylabel=\"% of Age Range Rejected (w/ Confidence Intervals)\")\nplt.show(g)\n\n\n\n\n\n\n\n\nThe above table shows the average decision rating for each age range (bucketed by 20 years). The Y axis shows what % of the age range is rejected. The lines on the bars indicate 95% confidence intervals.\nQuestions: 1. Is it more difficult for people in certain age groups to access credit under your proposed system? 2. Is it more difficult for people to get loans in order to pay for medical expenses? 2a. How does this compare with the actual rate of default in that group? 2b. What about people seeking loans for business ventures or education? 3. How does a person’s income level impact the ease with which they can access credit under your decision system?\nResponse:\nFor the purpose of this discussion, we will consider a loan “rejected” if the model deems the loan too-high risk. All statistics are based on the test dataset.\n\nWhile the 20-40 and 40-60 age ranges seem to have about the same access to credit, the 60-80 age group has a significantly higher chance of being rejected–i.e., have lower access to credit. Younger borrowers seem to have the greatest variability in access to credit but generally have high access\nCompared to loans for other purposes, loans for medical expenses are frequently considered too high-risk to make according to our model. Our model is, in a sense, generous: 28.2% of medical borrowers defaulted, but 40.7% were considered too high-risk. In contrast, business venture loans were denied 36.1% of the time with 14.6% risk of default, so the default rate and rejection rate were even further apart. For loans for education, 40.2% of borrowers were rejected, while 16.7% of borrowers actually defaulted. So even though a 40.7% rejection rate for medical borrowers is higher than loans for other purposes, the rejection rate seems reasonable when we consider the high default rate of medical loans.\nThe higher the income, the lower the perceived risk by the model. This is clearest in the simple table above that groups individuals by the “decision” column results. The borrowers the model would accept have about 13 more months of work experience, request loans that are smaller by about $2,000, and have credit histories that are 6 months longer. But the starkest differences are in the income of the borrower and (2) the percentage of income the loan represented, which is directly related to income. The income of approved borrowers is about double the income of rejected borrowers. The loan-to-income ratio is about three times higher for rejected borrowers than approved borrowers."
  },
  {
    "objectID": "posts/decision-making/DecisionMaking.html#part-g-write-and-reflect",
    "href": "posts/decision-making/DecisionMaking.html#part-g-write-and-reflect",
    "title": "Decision Making",
    "section": "",
    "text": "Discussion on Fairness\nConsidering that people seeking loans for medical expense have high rates of default, is it fair that it is more difficult for them to obtain access to credit?\nFairness\nFairness in decision-making is to make choices that minimize harm to all parties, with particular regard to parties with low/no control.\nControl\nThe bank has full control over whether to make the loan, so they clearly have substantial control. The borrower has far less control but not no control–excluding cases in which people have a serious illness / injury that prevents them from working, they have some level of control over their personal finances, particularly their income. Still, the borrower has likely little control over the size of the loan they need given the nature of medical expenses and, relatedly, the percentage of income their loan constitutes. So in this way, it’s unlikely that a perfectly fair decision can be made. But the bank has to make a decision, and so the goal should be to achieve a target that gets as close to fairness as possible.\nHarm\nIt is not only the borrower who can experience harm in this scenario by being denied a loan. The bank experiences financial harm by lending to a borrower with excessively high risk. This harm is harder to see immediately–it’s distributed in the form of risk or loss across shareholders, management, employees, depositors, lenders, etc, but it’s harm nonetheless. And if a bank were to repeatedly make loans with negative expected value, it risks serious harm to those stakeholders (e.g., people losing savings in the bank’s stock; employees losing jobs; depositors losing uninsured deposits).\nIt’s possible the bank could just make loans with lower expected value instead, not negative expected value. But even that could mean a decline in the business over time as competitors generate greater profits and reinvest those profits to become more competitive, harming this bank’s business and hence its stakeholders.\nIn conclusion, balancing the benefit to the bank’s stakeholders with the potential harm to rejected borrowers, I conclude that it is fair for medical loan borrowers to have more difficulty accessing credit.\nTo reach a conclusion about whether this specific model and its decisions are fair, we would have to make assumptions about the level of harm being done to rejected borrowers and the benefit to stakeholders in the bank from those borrowers being rejected.\nReflection on Blog Post\nI found that it is possible to construct a logistic regression model that can predict borrower outcomes with relative accuracy. One of the most interesting findings was that in this case, to maximize our target variable, we actually didn’t optimize accuracy. A threshold of 0 yields around 80% accuracy on the training set, far higher than our profit-maximizing threshold of -1, which yields 69% accuracy on the training set. I learned more about how to visualize data and had to grapple with the best ways to visualize so many variables. I ultimately concluded that I would keep my graphs relatively simple and show a more complete story via tables. I learned to work with subsets of my own data: Part D in particular was tricky for me because I had trouble finding a way to construct the find_profit function.\nUnlike the Classifying Penguins post, there was a lot less guidance, which I think was helpful in forcing me to edit and therefore engage further with the example code given previously (e.g., pre-processing the data at the start of Part C)"
  },
  {
    "objectID": "posts/final-project/axiao_research.html",
    "href": "posts/final-project/axiao_research.html",
    "title": "Applying Machine Learning to Stock Price Prediction",
    "section": "",
    "text": "What problem did we address?\nWhat approach(es) did we use to address it?\nWhat are the big-picture results?\n\n\n\n\n\nPrompt: “Your introduction should describe the big-picture problem that you aimed to address in your project. What’s the problem to be solved, and why is it important? Who has tried solving this problem already, and what did they do? I would expect most introductions to reference no fewer than 2 scholarly studies that attempted similar tasks, although 5 is probably a better target.”\n\nIn this blog post, we train machine learning models on historical stock market data to predict future stock price movements. This is a highly popular problem to address because of the potential for significant monetary gain. This is an important problem societally because stock markets are mechanisms of price discovery: they answer the question “What is a company worth?” Finding the right answer to that question allows society to correctly allocate more or less capital (money) to that company. On an individual level, this is an important problem to us as the authors because it’s the problem for all quant trading: making a profitable model.\nAn enormous body of literature within and without computer science exists for stock market prediction. Among the papers most relevant to our work are Bhandari et al. (2022) and Zhang (2022).\nGunduz (2021) applies LSTM and ensemble learning (Light-GBM) models to predict the hourly directions of eight banking stocks in Borsa Istanbul. He achieved up to maximum success rate of 0.685 using individual features of bank stocks and LSTM.\nBhandari et al. (2022) apply single-layer and multi-layer LSTM models to the problem of predicting the S&P 500, the index of the largest 500 publicly traded companies in America. Their single-layer LTSM model with 150 neurons is their best performing specification. Their set of predicted values have an average correlation coefficient of 0.9976 with actual S&P index values.\nZhang (2022) finds the LSTM network model does not perform better than other models when applied to a short forecasting horizon (1 to 10 days). Zhang’s “other models” are linear regression, eXtreme gradient boosting (XGBoost), last value, and moving average.\nWe take some of the “best practices” we observe in the above papers, specifically benchmarking with last value and calculating accuracy with R, RMSE, and MAPE. Unlike the mentioned papers, we will be focusing on single stocks and attempting to build a model that outperforms the last value benchmark.\n\n\n\n\nWho are the potential users of your project? Who, other than your users, might still be affected by your project?\nWho benefits from technology that solves the problem you address?\nWho could be harmed from technology that solves the problem you well address?\nWhat is your personal reason for working on this problem?\nBased on your reflection, would the world be a more equitable, just, joyful, peaceful, or sustainable place based on the technology that you implemented?\n\n\nimport yfinance as yf\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom yahoofinancials import YahooFinancials as YF\n\n\n# Define the ticker and the time period\nticker = 'TSLA'\nstart_date = '2019-01-01'\nend_date = '2020-01-01'\n\n# Fetch TSLA data\ntsla = yf.download(ticker, start=start_date, end=end_date)\n\nprint(tsla.head())\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n                 Open       High        Low      Close  Adj Close     Volume\nDate                                                                        \n2019-01-02  20.406668  21.008667  19.920000  20.674667  20.674667  174879000\n2019-01-03  20.466667  20.626667  19.825333  20.024000  20.024000  104478000\n2019-01-04  20.400000  21.200001  20.181999  21.179333  21.179333  110911500\n2019-01-07  21.448000  22.449333  21.183332  22.330667  22.330667  113268000\n2019-01-08  22.797333  22.934000  21.801332  22.356667  22.356667  105127500\n\n\n\n# Moving Average \nshort_window = 40\nlong_window = 100\n\ntsla['Short_MAvg'] = tsla['Close'].rolling(window=short_window, min_periods=1).mean()\ntsla['Long_MAvg'] = tsla['Close'].rolling(window=long_window, min_periods=1).mean()\n\n\n# Basic trading signal based on crossover \n\n# Create a 'Signal' column\ntsla['Signal'] = 0\ntsla['Signal'] = np.where(tsla['Short_MAvg'] &gt; tsla['Long_MAvg'], 1, 0)\n\n\n# Generate trading orders\ntsla['Position'] = tsla['Signal'].diff()\n\n\n# Basic Back Test\n\n# Plot the closing prices and moving averages\nplt.figure(figsize=(14,7))\nplt.plot(tsla['Close'], label='Close Price', alpha=0.5)\nplt.plot(tsla['Short_MAvg'], label=f'{short_window}-Day MA', alpha=0.75)\nplt.plot(tsla['Long_MAvg'], label=f'{long_window}-Day MA', alpha=0.75)\n\n# Plot buy signals\nplt.plot(tsla[tsla['Position'] == 1].index, tsla['Short_MAvg'][tsla['Position'] == 1], '^', markersize=10, color='g', lw=0, label='Buy Signal')\n\n# Plot sell signals\nplt.plot(tsla[tsla['Position'] == -1].index, tsla['Short_MAvg'][tsla['Position'] == -1], 'v', markersize=10, color='r', lw=0, label='Sell Signal')\n\nplt.title('AAPL Stock Price and Moving Average Crossovers')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\ntsla['Std_Dev'] = tsla['Close'].rolling(window=short_window, min_periods=1).std()\n\n# Calculate the z-score\ntsla['Z_Score'] = (tsla['Close'] - tsla['Short_MAvg']) / tsla['Std_Dev']\n\n\n# Define thresholds for buying and selling\nthreshold_buy = -1.5  # Buy signal threshold\nthreshold_sell = 1.5  # Sell signal threshold\n\n# Generate signals\ntsla['Signal'] = 0\ntsla['Signal'][tsla['Z_Score'] &gt; threshold_sell] = -1  # Sell signal\ntsla['Signal'][tsla['Z_Score'] &lt; threshold_buy] = 1  # Buy signal\n\n# Generate trading positions\ntsla['Position'] = tsla['Signal'].replace(0, np.nan).ffill().fillna(0)\n\nC:\\Users\\james\\AppData\\Local\\Temp\\ipykernel_70572\\3954209935.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  tsla['Signal'][tsla['Z_Score'] &gt; threshold_sell] = -1  # Sell signal\nC:\\Users\\james\\AppData\\Local\\Temp\\ipykernel_70572\\3954209935.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  tsla['Signal'][tsla['Z_Score'] &lt; threshold_buy] = 1  # Buy signal\n\n\n\n# Plot the results\nplt.figure(figsize=(14,7))\nplt.plot(tsla['Close'], label='Close Price', alpha=0.5)\nplt.plot(tsla['Short_MAvg'], label='Moving Average', alpha=0.75)\nplt.fill_between(tsla.index, tsla['Short_MAvg'] - tsla['Std_Dev'], tsla['Short_MAvg'] + tsla['Std_Dev'], color='gray', alpha=0.3, label='Standard Deviation Range')\n\n# Highlight the buy and sell signals\nplt.plot(tsla[tsla['Position'] == 1].index, tsla['Close'][tsla['Position'] == 1], '^', markersize=10, color='g', lw=0, label='Buy Signal')\nplt.plot(tsla[tsla['Position'] == -1].index, tsla['Close'][tsla['Position'] == -1], 'v', markersize=10, color='r', lw=0, label='Sell Signal')\n\nplt.title(f'{ticker} Stock Price and Mean Reversion Signals')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n\nticker = 'TSLA'\nstart_date = '2010-01-01'\nend_date = '2020-01-01'\n\nmodel = LogisticRegression()\n\ndata = yf.download(ticker, start=start_date, end=end_date)\n\n# Calculate moving averages and std\ndata['SMA_20'] = data['Close'].rolling(window=20).mean()\ndata['SMA_50'] = data['Close'].rolling(window=50).mean()\ndata['Std_Dev'] = data['Close'].rolling(window=20).std()\n\n# Calculate the z-score\ndata['Z_Score'] = (data['Close'] - data['SMA_20']) / data['Std_Dev']\n\n# Calculate RSI\ndelta = data['Close'].diff()\nup = delta.clip(lower=0)\ndown = -1 * delta.clip(upper=0)\nema_up = up.ewm(com=13, adjust=False).mean()\nema_down = down.ewm(com=13, adjust=False).mean()\nrs = ema_up / ema_down\n\ndata['RSI'] = 100 - (100 / (1 + rs))\n\n# Calculate the daily returns\ndata['Returns'] = data['Close'].pct_change()\n\n# Drop any NaNs\ndata.dropna(inplace=True)\n\n# If stock price goes up or down\ndata['Target'] = (data['Returns'].shift(-1) &gt; 0).astype(int)\n\n# Wanted features for X and y\nfeatures = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'Returns']\nX = data[features]\ny = data['Target']\n\n# Split data into first 80% and last 20%\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\n# Calculate cumulative strategy returns on test data\nX_test['Predicted_Signal'] = y_pred\nX_test['Strategy_Returns'] = X_test['Returns'] * X_test['Predicted_Signal'].shift(1)\ncumulative_strategy_returns = (X_test['Strategy_Returns'] + 1).cumprod()\n\n# Calculate cumulative returns for the market\nspy = yf.download('SPY', start=X_test.index[0].date().strftime('%Y-%m-%d'), end=end_date)\nspy['Returns'] = spy['Close'].pct_change()\ncumulative_market_returns = (spy['Returns'] + 1).cumprod()\n\nplt.figure(figsize=(10,5))\nplt.plot(cumulative_strategy_returns, label='Strategy Returns')\nplt.plot(cumulative_market_returns, label='Market Returns')\nplt.legend()\nplt.show()\n\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n\n\nAccuracy: 0.4946695095948827\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\ndef train(model, ticker, start_date, end_date):\n    data = yf.download(ticker, start=start_date, end=end_date)\n    \n    # Calculate moving averages and std\n    data['SMA_20'] = data['Close'].rolling(window=20).mean()\n    data['SMA_50'] = data['Close'].rolling(window=50).mean()\n    data['Std_Dev'] = data['Close'].rolling(window=20).std()\n\n    # Calculate the z-score\n    data['Z_Score'] = (data['Close'] - data['SMA_20']) / data['Std_Dev']\n\n    # Calculate RSI\n    delta = data['Close'].diff()\n    up = delta.clip(lower=0)\n    down = -1 * delta.clip(upper=0)\n    ema_up = up.ewm(com=13, adjust=False).mean()\n    ema_down = down.ewm(com=13, adjust=False).mean()\n    rs = ema_up / ema_down\n\n    data['RSI'] = 100 - (100 / (1 + rs))\n\n    # Calculate the daily returns\n    data['Returns'] = data['Close'].pct_change()\n\n    # Drop any NaNs\n    data.dropna(inplace=True)\n\n    # If stock price goes up or down\n    data['Target'] = (data['Returns'].shift(-1) &gt; 0).astype(int)\n\n    # Wanted features for X and y\n    features = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'Returns']\n    X = data[features]\n    y = data['Target']\n\n    # Split data into first 80% and last 20%\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(\"Accuracy:\", accuracy)\n\n    # Calculate cumulative strategy returns on test data\n    X_test['Predicted_Signal'] = y_pred\n    X_test['Strategy_Returns'] = X_test['Returns'] * X_test['Predicted_Signal'].shift(1)\n    cumulative_strategy_returns = (X_test['Strategy_Returns'] + 1).cumprod()\n\n    # Calculate cumulative returns for the market\n    spy = yf.download('SPY', start=X_test.index[0].date().strftime('%Y-%m-%d'), end=end_date)\n    spy['Returns'] = spy['Close'].pct_change()\n    cumulative_market_returns = (spy['Returns'] + 1).cumprod()\n    \n    plt.figure(figsize=(10,5))\n    plt.plot(cumulative_strategy_returns, label='Strategy Returns')\n    plt.plot(cumulative_market_returns, label='Market Returns')\n    plt.legend()\n    plt.show()\n\n\n\nCode\n%load_ext autoreload\n%autoreload 2\nfrom strategy import MeanReversion\n\n\n\nticker = 'TSLA'\nmarket = 'SPY'\nstart = '2014-01-01'\nend = '2024-01-01'\nMR = MeanReversion(ticker, start, end, market)\n\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n\n\n\nRF = RandomForestClassifier(n_estimators=100, random_state=42)\nX_test = MR.evaluate(model=RF)\n\nAccuracy: 0.5263157894736842\n\n\n\n\n\n\n\n\n\n\nLR = LogisticRegression()\nX_test = MR.evaluate(model=LR)\n\nAccuracy: 0.5041322314049587\n\n\n\n\n\n\n\n\n\n\nSVM = SVC()\nX_test = MR.evaluate(model = SVM)\n\nAccuracy: 0.5337552742616034\n\n\n\n\n\n\n\n\n\nNote: Different models work better for different stocks.\nBelow, we calculate the “risk-free rate,” used as a reference for all returns in financial markets. It is considered the return on an investment that is considered to have zero risk (short-term US treasuries). We use the risk-free rate to calculate the Sharpe ratio, which is a widely-used measure of an investment or a strategy’s “risk-adjusted” performance.\n\ndef deannualize(annual_rate, periods=365):\n    return (1 + annual_rate) ** (1/periods) - 1\n\ndef get_risk_free_rate(start_date, end_date):\n    # download 3-month us treasury bills rates\n    annualized = yf.download('^IRX', start_date, end_date)['Close']\n    annualized = annualized / 100\n    \n    # de-annualize\n    daily = annualized.apply(deannualize)\n\n    # create dataframe\n    return pd.DataFrame({\"annualized\": annualized, \"daily\": daily})\n\n\n\n\n\nfrom pytrends.request import TrendReq\nfrom pytrends import dailydata\n\n\npytrends = TrendReq(hl = 'en-US', tz=360)\n\n\npytrends.build_payload(kw_list=['Microsoft', 'Tesla', 'Apple'], timeframe='2010-01-01 2020-01-01')\n\n\n\n\n\ntickers = ['XOM', 'CVX', 'COP', 'NEE', 'SO', 'EOG', 'DUK', 'MPC', 'SLB', 'PSX']\n\ndef prepare_data(tickers, start_date, end_date):\n    '''\n    Combines data of all tickers into a single dataframe for X_train. X_test is a list of dataframes for each ticker.\n    '''\n    X_train_list = []\n    y_train_list = []\n    X_test_list = []\n    y_test_list = []\n    for t in tickers:\n        data = yf.download(t, start=start_date, end=end_date)\n        # Calculate moving averages and std\n        data['SMA_20'] = data['Close'].rolling(window=20).mean()\n        data['SMA_50'] = data['Close'].rolling(window=50).mean()\n        data['Std_Dev'] = data['Close'].rolling(window=20).std()\n\n        # Calculate the z-score\n        data['Z_Score'] = (data['Close'] - data['SMA_20']) / data['Std_Dev']\n\n        # Calculate RSI\n        delta = data['Close'].diff()\n        up = delta.clip(lower=0)\n        down = -1 * delta.clip(upper=0)\n        ema_up = up.ewm(com=13, adjust=False).mean()\n        ema_down = down.ewm(com=13, adjust=False).mean()\n        rs = ema_up / ema_down\n\n        data['RSI'] = 100 - (100 / (1 + rs))\n\n        # Calculate the daily returns\n        data['Returns'] = data['Close'].pct_change()\n\n        # Drop any NaNs\n        data.dropna(inplace=True)\n\n        # If stock price goes up or down\n        data['Target'] = (data['Returns'].shift(-1) &gt; 0).astype(int)\n        data['Ticker'] = t\n        features = ['Ticker', 'SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'Returns']\n        X = data[features]\n        y = data['Target']\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n        X_train_list.append(X_train)\n        y_train_list.append(y_train)\n        X_test_list.append(X_test)\n        y_test_list.append(y_test)\n\n    return pd.concat(X_train_list, ignore_index=True), pd.concat(y_train_list, ignore_index=True), X_test_list, y_test_list\n\n\ndef evaluate(model, X_test_, y_test_, features, market_data):\n    '''\n    Compares returns to the market for a single ticker.\n    '''\n    X_test = X_test_.copy()\n    y_test = y_test_.copy()\n    \n    y_pred = model.predict(X_test[features])\n    accuracy = accuracy_score(y_test, y_pred)\n    print(f\"{X_test.Ticker.iloc[0]} Accuracy:\", accuracy)\n\n    # Calculate cumulative strategy returns on test data\n    X_test['Predicted_Signal'] = y_pred\n    X_test['Strategy_Returns'] = X_test['Returns'] * X_test['Predicted_Signal'].shift(1)\n    cumulative_strategy_returns = (X_test['Strategy_Returns'].fillna(0) + 1).cumprod()\n    returns = X_test.loc[X_test.index, 'Returns']\n    returns.iloc[0] = 0\n    cumulative_stock_returns = (returns + 1).cumprod()\n\n    # Calculate cumulative returns for the market\n    market_data['Returns'] = market_data['Close'].pct_change()\n    #cumulative_market_returns = (market_data['Returns'].fillna(0) + 1).cumprod()\n\n    plt.figure(figsize=(10,5))\n    plt.plot(cumulative_strategy_returns, label='Strategy Returns', alpha=0.5)\n    #plt.plot(cumulative_market_returns, label='Market Returns')\n    plt.plot(cumulative_stock_returns, label='Stock Returns', alpha=0.5)\n    plt.title(f'{X_test.Ticker.iloc[0]} Returns')\n    plt.legend()\n    plt.show()\n\n    return X_test\n\n\nstart = '2014-01-01'\nend = '2024-01-01'\nX_train, y_train, X_test, y_test = prepare_data(tickers, start, end)\n\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n\n\n\nfrom sklearn.svm import LinearSVC\n\n#model = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel = SVC()\n#model = LogisticRegression()\nfeatures = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'Returns']\nmodel = model.fit(X_train[features], y_train)\n\nmarket_data = yf.download('SPY', start=start, end=end)\nX_test_vec = []\n\nfor i in range(10):\n    market = market_data[market_data.index &gt;= X_test[i].index[0]].copy()\n    X_test_vec.append(evaluate(model, X_test[i], y_test[i], features, market_data=market))\n\n[*********************100%%**********************]  1 of 1 completed\n\n\nXOM Accuracy: 0.52834008097166\nCVX Accuracy: 0.48380566801619435\nCOP Accuracy: 0.5323886639676113\nNEE Accuracy: 0.48380566801619435\nSO Accuracy: 0.4939271255060729\nEOG Accuracy: 0.5425101214574899\nDUK Accuracy: 0.5182186234817814\nMPC Accuracy: 0.5303643724696356\nSLB Accuracy: 0.520242914979757\nPSX Accuracy: 0.5161943319838057\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nX_test_vec[0]['Predicted_Signal'].mean()\n\n0.5040485829959515\n\n\n\n\n\n\n\n\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.svm import LinearSVC\nfeatures = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI']\nrfe = RFECV(LinearSVC(dual='auto'), cv = 5)\n\n\nrfe = rfe.fit(X_train[features], y_train)\n\n\nrfe.ranking_\n\narray([2, 3, 1, 1, 4])\n\n\n\nmarket_data = yf.download('SPY', start=start, end=end)\nmarket = market_data[market_data.index &gt;= X_test[i].index[0]].copy()\nX_test_vec = []\nfor i in range(10):\n    X_test_vec.append(evaluate(rfe, X_test[i], y_test[i], features, market_data=market))\n\n[*********************100%%**********************]  1 of 1 completed\n\n\nXOM Accuracy: 0.5182186234817814\nCVX Accuracy: 0.46963562753036436\nCOP Accuracy: 0.5222672064777328\nNEE Accuracy: 0.4898785425101215\nSO Accuracy: 0.4939271255060729\nEOG Accuracy: 0.46963562753036436\nDUK Accuracy: 0.5141700404858299\nMPC Accuracy: 0.5323886639676113\nSLB Accuracy: 0.5242914979757085\nPSX Accuracy: 0.5323886639676113\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\nfrom torch.autograd import Variable \n\n\n%load_ext autoreload\n%autoreload 2\nfrom lstm_model import LSTMModel\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\nticker = 'XOM'\nstart_date = '2014-01-01'\nend_date = '2024-01-01'\n\n#model = LogisticRegression()\n\ndata = yf.download(ticker, start=start_date, end=end_date)\n\n# Calculate moving averages and std\ndata['SMA_20'] = data['Close'].rolling(window=20).mean()\ndata['SMA_50'] = data['Close'].rolling(window=50).mean()\ndata['Std_Dev'] = data['Close'].rolling(window=20).std()\n\n# Calculate the z-score\ndata['Z_Score'] = (data['Close'] - data['SMA_20']) / data['Std_Dev']\n\n# Calculate RSI\ndelta = data['Close'].diff()\nup = delta.clip(lower=0)\ndown = -1 * delta.clip(upper=0)\nema_up = up.ewm(com=13, adjust=False).mean()\nema_down = down.ewm(com=13, adjust=False).mean()\nrs = ema_up / ema_down\n\ndata['RSI'] = 100 - (100 / (1 + rs))\n\n# Calculate the daily returns\ndata['Returns'] = data['Close'].pct_change()\n\n# If stock price goes up or down\ndata['Target'] = data['Close'].shift(-1)\n\ndata.dropna(inplace=True)\n\n# Wanted features for X and y\nfeatures = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'Close', 'Returns']\nX = data.loc[:, features]\ny = data.iloc[:, (data.shape[1]-1):(data.shape[1])]\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nmm1 = MinMaxScaler()\nss1 = StandardScaler()\n# mm2 = MinMaxScaler()\n# ss2 = StandardScaler()\n\nX_ss = pd.DataFrame(ss1.fit_transform(X), index=X.index, columns=X.columns)\ny_mm = pd.DataFrame(mm1.fit_transform(y), index=y.index, columns=y.columns)\n\n# Split data into first 80% and last 20%\nX_train, X_test, y_train, y_test = train_test_split(X_ss, y_mm, test_size=0.2, random_state=42, shuffle=False)\n# X_train_ss = pd.DataFrame(ss1.fit_transform(X_train), index=X_train.index, columns=X.columns)\n# y_train_mm = pd.DataFrame(mm1.fit_transform(y_train), index=y_train.index, columns=y.columns)\n# X_test_ss = pd.DataFrame(ss2.fit_transform(X_test), index=X_test.index, columns=X.columns)\n# y_test_mm = pd.DataFrame(mm2.fit_transform(y_test), index=y_test.index, columns=y.columns)\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\nfeatures_ = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI']\nX_train_tensors = Variable(torch.Tensor(np.array(X_train[features_])))\nX_test_tensors = Variable(torch.Tensor(np.array(X_test[features_])))\n\ny_train_tensors = Variable(torch.Tensor(y_train.values))\ny_test_tensors = Variable(torch.Tensor(y_test.values))\n\nX_train_final = torch.reshape(X_train_tensors,   (X_train_tensors.shape[0], 1, X_train_tensors.shape[1]))\nX_test_final = torch.reshape(X_test_tensors,  (X_test_tensors.shape[0], 1, X_test_tensors.shape[1]))\n\n\nprint(\"Training Shape\", X_train_final.shape, y_train_tensors.shape)\nprint(\"Testing Shape\", X_test_final.shape, y_test_tensors.shape) \n\nTraining Shape torch.Size([1972, 1, 5]) torch.Size([1972, 1])\nTesting Shape torch.Size([494, 1, 5]) torch.Size([494, 1])\n\n\n\nnum_epochs = 1000 #1000 epochs\nlearning_rate = 0.001 #0.001 lr\n\ninput_size = 5 #number of features\nhidden_size = 2 #number of features in hidden state\nnum_layers = 1 #number of stacked lstm layers\n\nnum_classes = 1 #number of output classes \n\n\nlstm = LSTMModel(num_classes, input_size, hidden_size, num_layers, X_train_final.shape[1]) #our lstm class \n\n\ncriterion = torch.nn.MSELoss()    # mean-squared error for regression\noptimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n\n\nfor epoch in range(num_epochs):\n  outputs = lstm.forward(X_train_final) #forward pass\n  optimizer.zero_grad() #calculate the gradient, manually setting to 0\n \n  # obtain the loss function\n  loss = criterion(outputs, y_train_tensors)\n \n  loss.backward() #calculates the loss of the loss function\n \n  optimizer.step() #improve from loss, i.e backprop\n  if epoch % 100 == 0:\n    print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item())) \n\nEpoch: 0, loss: 0.16941\nEpoch: 100, loss: 0.01794\nEpoch: 200, loss: 0.00689\nEpoch: 300, loss: 0.00291\nEpoch: 400, loss: 0.00080\nEpoch: 500, loss: 0.00038\nEpoch: 600, loss: 0.00029\nEpoch: 700, loss: 0.00026\nEpoch: 800, loss: 0.00023\nEpoch: 900, loss: 0.00022\n\n\n\ntrain_predict = lstm(X_train_final)\ndata_predict = train_predict.data.numpy()\n\n\ndata_predict = mm1.inverse_transform(data_predict) #reverse transformation\n\n\nX_train = pd.DataFrame(ss1.inverse_transform(X_train), index=X_train.index, columns=features)\n\n\nX_train['Predicted_Price'] = data_predict\nX_train['Actual_Signal'] = (X_train['Returns'].shift(-1) &gt; 0).astype(int)\nX_train['Predicted_Returns'] = X_train['Predicted_Price'].pct_change()\nX_train['Predicted_Signal'] = (X_train['Predicted_Returns'].shift(-1) &gt; 0).astype(int)\nX_train['Strategy_Returns'] = X_train['Returns'] * X_train['Predicted_Signal'].shift(1)\ncumulative_strategy_returns = (X_train['Strategy_Returns'].fillna(0) + 1).cumprod()\nreturns = X_train.loc[X_train.index, 'Returns']\nreturns.iloc[0] = 0\ncumulative_stock_returns = (returns + 1).cumprod()\naccuracy = (X_train['Actual_Signal'] == X_train['Predicted_Signal']).mean()\nprint(f'Accuracy: {accuracy}')\n# Calculate cumulative returns for the market\n#market_data['Returns'] = market_data['Close'].pct_change()\n#cumulative_market_returns = (market_data['Returns'].fillna(0) + 1).cumprod()\n\nplt.figure(figsize=(10,5))\nplt.plot(cumulative_strategy_returns, label='Strategy Returns', alpha=0.5)\n#plt.plot(cumulative_market_returns, label='Market Returns')\nplt.plot(cumulative_stock_returns, label='Stock Returns', alpha=0.5)\nplt.legend();\n\nAccuracy: 0.9523326572008114\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10,5))\nplt.plot(X_train['Predicted_Price'], label='Predicted Price')\nplt.plot(X_train['Close'], label='Actual Price')\nplt.legend();\n\n\n\n\n\n\n\n\n\ntest_predict = lstm(X_test_final)\ndata_predict = test_predict.data.numpy()\n\n\ndata_predict = mm1.inverse_transform(data_predict)\nX_test = pd.DataFrame(ss1.inverse_transform(X_test), index=X_test.index, columns=features)\n\n\nX_test['Actual_Signal'] = (X_test['Returns'].shift(-1) &gt; 0).astype(int)\nX_test['Predicted_Price'] = data_predict\nX_test['Predicted_Returns'] = X_test['Predicted_Price'].pct_change()\nX_test['Predicted_Signal'] = (X_test['Predicted_Returns'].shift(-1) &gt; 0).astype(int)\nX_test['Strategy_Returns'] = X_test['Returns'] * X_test['Predicted_Signal'].shift(1)\ncumulative_strategy_returns = (X_test['Strategy_Returns'].fillna(0) + 1).cumprod()\nreturns = X_test.loc[X_test.index, 'Returns']\nreturns.iloc[0] = 0\ncumulative_stock_returns = (returns + 1).cumprod()\naccuracy = (X_test['Actual_Signal'] == X_test['Predicted_Signal']).mean()\nprint(f'Accuracy: {accuracy}')\n# Calculate cumulative returns for the market\n#market_data['Returns'] = market_data['Close'].pct_change()\n#cumulative_market_returns = (market_data['Returns'].fillna(0) + 1).cumprod()\n\nplt.figure(figsize=(10,5))\n\nplt.plot(cumulative_strategy_returns, label='Strategy Returns')\n#plt.plot(cumulative_market_returns, label='Market Returns')\nplt.plot(cumulative_stock_returns, label='Stock Returns')\nplt.legend();\n\nAccuracy: 0.9473684210526315\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10,5))\nplt.plot(X_test['Predicted_Price'], label='Predicted Price')\nplt.plot(X_test['Close'], label='Actual Price')\nplt.legend();"
  },
  {
    "objectID": "posts/final-project/axiao_research.html#abstract",
    "href": "posts/final-project/axiao_research.html#abstract",
    "title": "Applying Machine Learning to Stock Price Prediction",
    "section": "",
    "text": "What problem did we address?\nWhat approach(es) did we use to address it?\nWhat are the big-picture results?"
  },
  {
    "objectID": "posts/final-project/axiao_research.html#introduction",
    "href": "posts/final-project/axiao_research.html#introduction",
    "title": "Applying Machine Learning to Stock Price Prediction",
    "section": "",
    "text": "Prompt: “Your introduction should describe the big-picture problem that you aimed to address in your project. What’s the problem to be solved, and why is it important? Who has tried solving this problem already, and what did they do? I would expect most introductions to reference no fewer than 2 scholarly studies that attempted similar tasks, although 5 is probably a better target.”\n\nIn this blog post, we train machine learning models on historical stock market data to predict future stock price movements. This is a highly popular problem to address because of the potential for significant monetary gain. This is an important problem societally because stock markets are mechanisms of price discovery: they answer the question “What is a company worth?” Finding the right answer to that question allows society to correctly allocate more or less capital (money) to that company. On an individual level, this is an important problem to us as the authors because it’s the problem for all quant trading: making a profitable model.\nAn enormous body of literature within and without computer science exists for stock market prediction. Among the papers most relevant to our work are Bhandari et al. (2022) and Zhang (2022).\nGunduz (2021) applies LSTM and ensemble learning (Light-GBM) models to predict the hourly directions of eight banking stocks in Borsa Istanbul. He achieved up to maximum success rate of 0.685 using individual features of bank stocks and LSTM.\nBhandari et al. (2022) apply single-layer and multi-layer LSTM models to the problem of predicting the S&P 500, the index of the largest 500 publicly traded companies in America. Their single-layer LTSM model with 150 neurons is their best performing specification. Their set of predicted values have an average correlation coefficient of 0.9976 with actual S&P index values.\nZhang (2022) finds the LSTM network model does not perform better than other models when applied to a short forecasting horizon (1 to 10 days). Zhang’s “other models” are linear regression, eXtreme gradient boosting (XGBoost), last value, and moving average.\nWe take some of the “best practices” we observe in the above papers, specifically benchmarking with last value and calculating accuracy with R, RMSE, and MAPE. Unlike the mentioned papers, we will be focusing on single stocks and attempting to build a model that outperforms the last value benchmark."
  },
  {
    "objectID": "posts/final-project/axiao_research.html#values",
    "href": "posts/final-project/axiao_research.html#values",
    "title": "Applying Machine Learning to Stock Price Prediction",
    "section": "",
    "text": "Who are the potential users of your project? Who, other than your users, might still be affected by your project?\nWho benefits from technology that solves the problem you address?\nWho could be harmed from technology that solves the problem you well address?\nWhat is your personal reason for working on this problem?\nBased on your reflection, would the world be a more equitable, just, joyful, peaceful, or sustainable place based on the technology that you implemented?\n\n\nimport yfinance as yf\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom yahoofinancials import YahooFinancials as YF\n\n\n# Define the ticker and the time period\nticker = 'TSLA'\nstart_date = '2019-01-01'\nend_date = '2020-01-01'\n\n# Fetch TSLA data\ntsla = yf.download(ticker, start=start_date, end=end_date)\n\nprint(tsla.head())\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n                 Open       High        Low      Close  Adj Close     Volume\nDate                                                                        \n2019-01-02  20.406668  21.008667  19.920000  20.674667  20.674667  174879000\n2019-01-03  20.466667  20.626667  19.825333  20.024000  20.024000  104478000\n2019-01-04  20.400000  21.200001  20.181999  21.179333  21.179333  110911500\n2019-01-07  21.448000  22.449333  21.183332  22.330667  22.330667  113268000\n2019-01-08  22.797333  22.934000  21.801332  22.356667  22.356667  105127500\n\n\n\n# Moving Average \nshort_window = 40\nlong_window = 100\n\ntsla['Short_MAvg'] = tsla['Close'].rolling(window=short_window, min_periods=1).mean()\ntsla['Long_MAvg'] = tsla['Close'].rolling(window=long_window, min_periods=1).mean()\n\n\n# Basic trading signal based on crossover \n\n# Create a 'Signal' column\ntsla['Signal'] = 0\ntsla['Signal'] = np.where(tsla['Short_MAvg'] &gt; tsla['Long_MAvg'], 1, 0)\n\n\n# Generate trading orders\ntsla['Position'] = tsla['Signal'].diff()\n\n\n# Basic Back Test\n\n# Plot the closing prices and moving averages\nplt.figure(figsize=(14,7))\nplt.plot(tsla['Close'], label='Close Price', alpha=0.5)\nplt.plot(tsla['Short_MAvg'], label=f'{short_window}-Day MA', alpha=0.75)\nplt.plot(tsla['Long_MAvg'], label=f'{long_window}-Day MA', alpha=0.75)\n\n# Plot buy signals\nplt.plot(tsla[tsla['Position'] == 1].index, tsla['Short_MAvg'][tsla['Position'] == 1], '^', markersize=10, color='g', lw=0, label='Buy Signal')\n\n# Plot sell signals\nplt.plot(tsla[tsla['Position'] == -1].index, tsla['Short_MAvg'][tsla['Position'] == -1], 'v', markersize=10, color='r', lw=0, label='Sell Signal')\n\nplt.title('AAPL Stock Price and Moving Average Crossovers')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\ntsla['Std_Dev'] = tsla['Close'].rolling(window=short_window, min_periods=1).std()\n\n# Calculate the z-score\ntsla['Z_Score'] = (tsla['Close'] - tsla['Short_MAvg']) / tsla['Std_Dev']\n\n\n# Define thresholds for buying and selling\nthreshold_buy = -1.5  # Buy signal threshold\nthreshold_sell = 1.5  # Sell signal threshold\n\n# Generate signals\ntsla['Signal'] = 0\ntsla['Signal'][tsla['Z_Score'] &gt; threshold_sell] = -1  # Sell signal\ntsla['Signal'][tsla['Z_Score'] &lt; threshold_buy] = 1  # Buy signal\n\n# Generate trading positions\ntsla['Position'] = tsla['Signal'].replace(0, np.nan).ffill().fillna(0)\n\nC:\\Users\\james\\AppData\\Local\\Temp\\ipykernel_70572\\3954209935.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  tsla['Signal'][tsla['Z_Score'] &gt; threshold_sell] = -1  # Sell signal\nC:\\Users\\james\\AppData\\Local\\Temp\\ipykernel_70572\\3954209935.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  tsla['Signal'][tsla['Z_Score'] &lt; threshold_buy] = 1  # Buy signal\n\n\n\n# Plot the results\nplt.figure(figsize=(14,7))\nplt.plot(tsla['Close'], label='Close Price', alpha=0.5)\nplt.plot(tsla['Short_MAvg'], label='Moving Average', alpha=0.75)\nplt.fill_between(tsla.index, tsla['Short_MAvg'] - tsla['Std_Dev'], tsla['Short_MAvg'] + tsla['Std_Dev'], color='gray', alpha=0.3, label='Standard Deviation Range')\n\n# Highlight the buy and sell signals\nplt.plot(tsla[tsla['Position'] == 1].index, tsla['Close'][tsla['Position'] == 1], '^', markersize=10, color='g', lw=0, label='Buy Signal')\nplt.plot(tsla[tsla['Position'] == -1].index, tsla['Close'][tsla['Position'] == -1], 'v', markersize=10, color='r', lw=0, label='Sell Signal')\n\nplt.title(f'{ticker} Stock Price and Mean Reversion Signals')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/final-project/axiao_research.html#model-analysis",
    "href": "posts/final-project/axiao_research.html#model-analysis",
    "title": "Applying Machine Learning to Stock Price Prediction",
    "section": "",
    "text": "from sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n\nticker = 'TSLA'\nstart_date = '2010-01-01'\nend_date = '2020-01-01'\n\nmodel = LogisticRegression()\n\ndata = yf.download(ticker, start=start_date, end=end_date)\n\n# Calculate moving averages and std\ndata['SMA_20'] = data['Close'].rolling(window=20).mean()\ndata['SMA_50'] = data['Close'].rolling(window=50).mean()\ndata['Std_Dev'] = data['Close'].rolling(window=20).std()\n\n# Calculate the z-score\ndata['Z_Score'] = (data['Close'] - data['SMA_20']) / data['Std_Dev']\n\n# Calculate RSI\ndelta = data['Close'].diff()\nup = delta.clip(lower=0)\ndown = -1 * delta.clip(upper=0)\nema_up = up.ewm(com=13, adjust=False).mean()\nema_down = down.ewm(com=13, adjust=False).mean()\nrs = ema_up / ema_down\n\ndata['RSI'] = 100 - (100 / (1 + rs))\n\n# Calculate the daily returns\ndata['Returns'] = data['Close'].pct_change()\n\n# Drop any NaNs\ndata.dropna(inplace=True)\n\n# If stock price goes up or down\ndata['Target'] = (data['Returns'].shift(-1) &gt; 0).astype(int)\n\n# Wanted features for X and y\nfeatures = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'Returns']\nX = data[features]\ny = data['Target']\n\n# Split data into first 80% and last 20%\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\n# Calculate cumulative strategy returns on test data\nX_test['Predicted_Signal'] = y_pred\nX_test['Strategy_Returns'] = X_test['Returns'] * X_test['Predicted_Signal'].shift(1)\ncumulative_strategy_returns = (X_test['Strategy_Returns'] + 1).cumprod()\n\n# Calculate cumulative returns for the market\nspy = yf.download('SPY', start=X_test.index[0].date().strftime('%Y-%m-%d'), end=end_date)\nspy['Returns'] = spy['Close'].pct_change()\ncumulative_market_returns = (spy['Returns'] + 1).cumprod()\n\nplt.figure(figsize=(10,5))\nplt.plot(cumulative_strategy_returns, label='Strategy Returns')\nplt.plot(cumulative_market_returns, label='Market Returns')\nplt.legend()\nplt.show()\n\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n\n\nAccuracy: 0.4946695095948827\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\ndef train(model, ticker, start_date, end_date):\n    data = yf.download(ticker, start=start_date, end=end_date)\n    \n    # Calculate moving averages and std\n    data['SMA_20'] = data['Close'].rolling(window=20).mean()\n    data['SMA_50'] = data['Close'].rolling(window=50).mean()\n    data['Std_Dev'] = data['Close'].rolling(window=20).std()\n\n    # Calculate the z-score\n    data['Z_Score'] = (data['Close'] - data['SMA_20']) / data['Std_Dev']\n\n    # Calculate RSI\n    delta = data['Close'].diff()\n    up = delta.clip(lower=0)\n    down = -1 * delta.clip(upper=0)\n    ema_up = up.ewm(com=13, adjust=False).mean()\n    ema_down = down.ewm(com=13, adjust=False).mean()\n    rs = ema_up / ema_down\n\n    data['RSI'] = 100 - (100 / (1 + rs))\n\n    # Calculate the daily returns\n    data['Returns'] = data['Close'].pct_change()\n\n    # Drop any NaNs\n    data.dropna(inplace=True)\n\n    # If stock price goes up or down\n    data['Target'] = (data['Returns'].shift(-1) &gt; 0).astype(int)\n\n    # Wanted features for X and y\n    features = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'Returns']\n    X = data[features]\n    y = data['Target']\n\n    # Split data into first 80% and last 20%\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(\"Accuracy:\", accuracy)\n\n    # Calculate cumulative strategy returns on test data\n    X_test['Predicted_Signal'] = y_pred\n    X_test['Strategy_Returns'] = X_test['Returns'] * X_test['Predicted_Signal'].shift(1)\n    cumulative_strategy_returns = (X_test['Strategy_Returns'] + 1).cumprod()\n\n    # Calculate cumulative returns for the market\n    spy = yf.download('SPY', start=X_test.index[0].date().strftime('%Y-%m-%d'), end=end_date)\n    spy['Returns'] = spy['Close'].pct_change()\n    cumulative_market_returns = (spy['Returns'] + 1).cumprod()\n    \n    plt.figure(figsize=(10,5))\n    plt.plot(cumulative_strategy_returns, label='Strategy Returns')\n    plt.plot(cumulative_market_returns, label='Market Returns')\n    plt.legend()\n    plt.show()\n\n\n\nCode\n%load_ext autoreload\n%autoreload 2\nfrom strategy import MeanReversion\n\n\n\nticker = 'TSLA'\nmarket = 'SPY'\nstart = '2014-01-01'\nend = '2024-01-01'\nMR = MeanReversion(ticker, start, end, market)\n\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n\n\n\nRF = RandomForestClassifier(n_estimators=100, random_state=42)\nX_test = MR.evaluate(model=RF)\n\nAccuracy: 0.5263157894736842\n\n\n\n\n\n\n\n\n\n\nLR = LogisticRegression()\nX_test = MR.evaluate(model=LR)\n\nAccuracy: 0.5041322314049587\n\n\n\n\n\n\n\n\n\n\nSVM = SVC()\nX_test = MR.evaluate(model = SVM)\n\nAccuracy: 0.5337552742616034\n\n\n\n\n\n\n\n\n\nNote: Different models work better for different stocks.\nBelow, we calculate the “risk-free rate,” used as a reference for all returns in financial markets. It is considered the return on an investment that is considered to have zero risk (short-term US treasuries). We use the risk-free rate to calculate the Sharpe ratio, which is a widely-used measure of an investment or a strategy’s “risk-adjusted” performance.\n\ndef deannualize(annual_rate, periods=365):\n    return (1 + annual_rate) ** (1/periods) - 1\n\ndef get_risk_free_rate(start_date, end_date):\n    # download 3-month us treasury bills rates\n    annualized = yf.download('^IRX', start_date, end_date)['Close']\n    annualized = annualized / 100\n    \n    # de-annualize\n    daily = annualized.apply(deannualize)\n\n    # create dataframe\n    return pd.DataFrame({\"annualized\": annualized, \"daily\": daily})"
  },
  {
    "objectID": "posts/final-project/axiao_research.html#pytrends",
    "href": "posts/final-project/axiao_research.html#pytrends",
    "title": "Applying Machine Learning to Stock Price Prediction",
    "section": "",
    "text": "from pytrends.request import TrendReq\nfrom pytrends import dailydata\n\n\npytrends = TrendReq(hl = 'en-US', tz=360)\n\n\npytrends.build_payload(kw_list=['Microsoft', 'Tesla', 'Apple'], timeframe='2010-01-01 2020-01-01')"
  },
  {
    "objectID": "posts/final-project/axiao_research.html#basket-analysis",
    "href": "posts/final-project/axiao_research.html#basket-analysis",
    "title": "Applying Machine Learning to Stock Price Prediction",
    "section": "",
    "text": "tickers = ['XOM', 'CVX', 'COP', 'NEE', 'SO', 'EOG', 'DUK', 'MPC', 'SLB', 'PSX']\n\ndef prepare_data(tickers, start_date, end_date):\n    '''\n    Combines data of all tickers into a single dataframe for X_train. X_test is a list of dataframes for each ticker.\n    '''\n    X_train_list = []\n    y_train_list = []\n    X_test_list = []\n    y_test_list = []\n    for t in tickers:\n        data = yf.download(t, start=start_date, end=end_date)\n        # Calculate moving averages and std\n        data['SMA_20'] = data['Close'].rolling(window=20).mean()\n        data['SMA_50'] = data['Close'].rolling(window=50).mean()\n        data['Std_Dev'] = data['Close'].rolling(window=20).std()\n\n        # Calculate the z-score\n        data['Z_Score'] = (data['Close'] - data['SMA_20']) / data['Std_Dev']\n\n        # Calculate RSI\n        delta = data['Close'].diff()\n        up = delta.clip(lower=0)\n        down = -1 * delta.clip(upper=0)\n        ema_up = up.ewm(com=13, adjust=False).mean()\n        ema_down = down.ewm(com=13, adjust=False).mean()\n        rs = ema_up / ema_down\n\n        data['RSI'] = 100 - (100 / (1 + rs))\n\n        # Calculate the daily returns\n        data['Returns'] = data['Close'].pct_change()\n\n        # Drop any NaNs\n        data.dropna(inplace=True)\n\n        # If stock price goes up or down\n        data['Target'] = (data['Returns'].shift(-1) &gt; 0).astype(int)\n        data['Ticker'] = t\n        features = ['Ticker', 'SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'Returns']\n        X = data[features]\n        y = data['Target']\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n        X_train_list.append(X_train)\n        y_train_list.append(y_train)\n        X_test_list.append(X_test)\n        y_test_list.append(y_test)\n\n    return pd.concat(X_train_list, ignore_index=True), pd.concat(y_train_list, ignore_index=True), X_test_list, y_test_list\n\n\ndef evaluate(model, X_test_, y_test_, features, market_data):\n    '''\n    Compares returns to the market for a single ticker.\n    '''\n    X_test = X_test_.copy()\n    y_test = y_test_.copy()\n    \n    y_pred = model.predict(X_test[features])\n    accuracy = accuracy_score(y_test, y_pred)\n    print(f\"{X_test.Ticker.iloc[0]} Accuracy:\", accuracy)\n\n    # Calculate cumulative strategy returns on test data\n    X_test['Predicted_Signal'] = y_pred\n    X_test['Strategy_Returns'] = X_test['Returns'] * X_test['Predicted_Signal'].shift(1)\n    cumulative_strategy_returns = (X_test['Strategy_Returns'].fillna(0) + 1).cumprod()\n    returns = X_test.loc[X_test.index, 'Returns']\n    returns.iloc[0] = 0\n    cumulative_stock_returns = (returns + 1).cumprod()\n\n    # Calculate cumulative returns for the market\n    market_data['Returns'] = market_data['Close'].pct_change()\n    #cumulative_market_returns = (market_data['Returns'].fillna(0) + 1).cumprod()\n\n    plt.figure(figsize=(10,5))\n    plt.plot(cumulative_strategy_returns, label='Strategy Returns', alpha=0.5)\n    #plt.plot(cumulative_market_returns, label='Market Returns')\n    plt.plot(cumulative_stock_returns, label='Stock Returns', alpha=0.5)\n    plt.title(f'{X_test.Ticker.iloc[0]} Returns')\n    plt.legend()\n    plt.show()\n\n    return X_test\n\n\nstart = '2014-01-01'\nend = '2024-01-01'\nX_train, y_train, X_test, y_test = prepare_data(tickers, start, end)\n\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n[*********************100%%**********************]  1 of 1 completed\n\n\n\nfrom sklearn.svm import LinearSVC\n\n#model = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel = SVC()\n#model = LogisticRegression()\nfeatures = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'Returns']\nmodel = model.fit(X_train[features], y_train)\n\nmarket_data = yf.download('SPY', start=start, end=end)\nX_test_vec = []\n\nfor i in range(10):\n    market = market_data[market_data.index &gt;= X_test[i].index[0]].copy()\n    X_test_vec.append(evaluate(model, X_test[i], y_test[i], features, market_data=market))\n\n[*********************100%%**********************]  1 of 1 completed\n\n\nXOM Accuracy: 0.52834008097166\nCVX Accuracy: 0.48380566801619435\nCOP Accuracy: 0.5323886639676113\nNEE Accuracy: 0.48380566801619435\nSO Accuracy: 0.4939271255060729\nEOG Accuracy: 0.5425101214574899\nDUK Accuracy: 0.5182186234817814\nMPC Accuracy: 0.5303643724696356\nSLB Accuracy: 0.520242914979757\nPSX Accuracy: 0.5161943319838057\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nX_test_vec[0]['Predicted_Signal'].mean()\n\n0.5040485829959515"
  },
  {
    "objectID": "posts/final-project/axiao_research.html#feature-selection",
    "href": "posts/final-project/axiao_research.html#feature-selection",
    "title": "Applying Machine Learning to Stock Price Prediction",
    "section": "",
    "text": "from sklearn.feature_selection import RFECV\nfrom sklearn.svm import LinearSVC\nfeatures = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI']\nrfe = RFECV(LinearSVC(dual='auto'), cv = 5)\n\n\nrfe = rfe.fit(X_train[features], y_train)\n\n\nrfe.ranking_\n\narray([2, 3, 1, 1, 4])\n\n\n\nmarket_data = yf.download('SPY', start=start, end=end)\nmarket = market_data[market_data.index &gt;= X_test[i].index[0]].copy()\nX_test_vec = []\nfor i in range(10):\n    X_test_vec.append(evaluate(rfe, X_test[i], y_test[i], features, market_data=market))\n\n[*********************100%%**********************]  1 of 1 completed\n\n\nXOM Accuracy: 0.5182186234817814\nCVX Accuracy: 0.46963562753036436\nCOP Accuracy: 0.5222672064777328\nNEE Accuracy: 0.4898785425101215\nSO Accuracy: 0.4939271255060729\nEOG Accuracy: 0.46963562753036436\nDUK Accuracy: 0.5141700404858299\nMPC Accuracy: 0.5323886639676113\nSLB Accuracy: 0.5242914979757085\nPSX Accuracy: 0.5323886639676113"
  },
  {
    "objectID": "posts/final-project/axiao_research.html#lstm",
    "href": "posts/final-project/axiao_research.html#lstm",
    "title": "Applying Machine Learning to Stock Price Prediction",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\nfrom torch.autograd import Variable \n\n\n%load_ext autoreload\n%autoreload 2\nfrom lstm_model import LSTMModel\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\nticker = 'XOM'\nstart_date = '2014-01-01'\nend_date = '2024-01-01'\n\n#model = LogisticRegression()\n\ndata = yf.download(ticker, start=start_date, end=end_date)\n\n# Calculate moving averages and std\ndata['SMA_20'] = data['Close'].rolling(window=20).mean()\ndata['SMA_50'] = data['Close'].rolling(window=50).mean()\ndata['Std_Dev'] = data['Close'].rolling(window=20).std()\n\n# Calculate the z-score\ndata['Z_Score'] = (data['Close'] - data['SMA_20']) / data['Std_Dev']\n\n# Calculate RSI\ndelta = data['Close'].diff()\nup = delta.clip(lower=0)\ndown = -1 * delta.clip(upper=0)\nema_up = up.ewm(com=13, adjust=False).mean()\nema_down = down.ewm(com=13, adjust=False).mean()\nrs = ema_up / ema_down\n\ndata['RSI'] = 100 - (100 / (1 + rs))\n\n# Calculate the daily returns\ndata['Returns'] = data['Close'].pct_change()\n\n# If stock price goes up or down\ndata['Target'] = data['Close'].shift(-1)\n\ndata.dropna(inplace=True)\n\n# Wanted features for X and y\nfeatures = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI', 'Close', 'Returns']\nX = data.loc[:, features]\ny = data.iloc[:, (data.shape[1]-1):(data.shape[1])]\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nmm1 = MinMaxScaler()\nss1 = StandardScaler()\n# mm2 = MinMaxScaler()\n# ss2 = StandardScaler()\n\nX_ss = pd.DataFrame(ss1.fit_transform(X), index=X.index, columns=X.columns)\ny_mm = pd.DataFrame(mm1.fit_transform(y), index=y.index, columns=y.columns)\n\n# Split data into first 80% and last 20%\nX_train, X_test, y_train, y_test = train_test_split(X_ss, y_mm, test_size=0.2, random_state=42, shuffle=False)\n# X_train_ss = pd.DataFrame(ss1.fit_transform(X_train), index=X_train.index, columns=X.columns)\n# y_train_mm = pd.DataFrame(mm1.fit_transform(y_train), index=y_train.index, columns=y.columns)\n# X_test_ss = pd.DataFrame(ss2.fit_transform(X_test), index=X_test.index, columns=X.columns)\n# y_test_mm = pd.DataFrame(mm2.fit_transform(y_test), index=y_test.index, columns=y.columns)\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\nfeatures_ = ['SMA_20', 'SMA_50', 'Std_Dev', 'Z_Score', 'RSI']\nX_train_tensors = Variable(torch.Tensor(np.array(X_train[features_])))\nX_test_tensors = Variable(torch.Tensor(np.array(X_test[features_])))\n\ny_train_tensors = Variable(torch.Tensor(y_train.values))\ny_test_tensors = Variable(torch.Tensor(y_test.values))\n\nX_train_final = torch.reshape(X_train_tensors,   (X_train_tensors.shape[0], 1, X_train_tensors.shape[1]))\nX_test_final = torch.reshape(X_test_tensors,  (X_test_tensors.shape[0], 1, X_test_tensors.shape[1]))\n\n\nprint(\"Training Shape\", X_train_final.shape, y_train_tensors.shape)\nprint(\"Testing Shape\", X_test_final.shape, y_test_tensors.shape) \n\nTraining Shape torch.Size([1972, 1, 5]) torch.Size([1972, 1])\nTesting Shape torch.Size([494, 1, 5]) torch.Size([494, 1])\n\n\n\nnum_epochs = 1000 #1000 epochs\nlearning_rate = 0.001 #0.001 lr\n\ninput_size = 5 #number of features\nhidden_size = 2 #number of features in hidden state\nnum_layers = 1 #number of stacked lstm layers\n\nnum_classes = 1 #number of output classes \n\n\nlstm = LSTMModel(num_classes, input_size, hidden_size, num_layers, X_train_final.shape[1]) #our lstm class \n\n\ncriterion = torch.nn.MSELoss()    # mean-squared error for regression\noptimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n\n\nfor epoch in range(num_epochs):\n  outputs = lstm.forward(X_train_final) #forward pass\n  optimizer.zero_grad() #calculate the gradient, manually setting to 0\n \n  # obtain the loss function\n  loss = criterion(outputs, y_train_tensors)\n \n  loss.backward() #calculates the loss of the loss function\n \n  optimizer.step() #improve from loss, i.e backprop\n  if epoch % 100 == 0:\n    print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item())) \n\nEpoch: 0, loss: 0.16941\nEpoch: 100, loss: 0.01794\nEpoch: 200, loss: 0.00689\nEpoch: 300, loss: 0.00291\nEpoch: 400, loss: 0.00080\nEpoch: 500, loss: 0.00038\nEpoch: 600, loss: 0.00029\nEpoch: 700, loss: 0.00026\nEpoch: 800, loss: 0.00023\nEpoch: 900, loss: 0.00022\n\n\n\ntrain_predict = lstm(X_train_final)\ndata_predict = train_predict.data.numpy()\n\n\ndata_predict = mm1.inverse_transform(data_predict) #reverse transformation\n\n\nX_train = pd.DataFrame(ss1.inverse_transform(X_train), index=X_train.index, columns=features)\n\n\nX_train['Predicted_Price'] = data_predict\nX_train['Actual_Signal'] = (X_train['Returns'].shift(-1) &gt; 0).astype(int)\nX_train['Predicted_Returns'] = X_train['Predicted_Price'].pct_change()\nX_train['Predicted_Signal'] = (X_train['Predicted_Returns'].shift(-1) &gt; 0).astype(int)\nX_train['Strategy_Returns'] = X_train['Returns'] * X_train['Predicted_Signal'].shift(1)\ncumulative_strategy_returns = (X_train['Strategy_Returns'].fillna(0) + 1).cumprod()\nreturns = X_train.loc[X_train.index, 'Returns']\nreturns.iloc[0] = 0\ncumulative_stock_returns = (returns + 1).cumprod()\naccuracy = (X_train['Actual_Signal'] == X_train['Predicted_Signal']).mean()\nprint(f'Accuracy: {accuracy}')\n# Calculate cumulative returns for the market\n#market_data['Returns'] = market_data['Close'].pct_change()\n#cumulative_market_returns = (market_data['Returns'].fillna(0) + 1).cumprod()\n\nplt.figure(figsize=(10,5))\nplt.plot(cumulative_strategy_returns, label='Strategy Returns', alpha=0.5)\n#plt.plot(cumulative_market_returns, label='Market Returns')\nplt.plot(cumulative_stock_returns, label='Stock Returns', alpha=0.5)\nplt.legend();\n\nAccuracy: 0.9523326572008114\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10,5))\nplt.plot(X_train['Predicted_Price'], label='Predicted Price')\nplt.plot(X_train['Close'], label='Actual Price')\nplt.legend();\n\n\n\n\n\n\n\n\n\ntest_predict = lstm(X_test_final)\ndata_predict = test_predict.data.numpy()\n\n\ndata_predict = mm1.inverse_transform(data_predict)\nX_test = pd.DataFrame(ss1.inverse_transform(X_test), index=X_test.index, columns=features)\n\n\nX_test['Actual_Signal'] = (X_test['Returns'].shift(-1) &gt; 0).astype(int)\nX_test['Predicted_Price'] = data_predict\nX_test['Predicted_Returns'] = X_test['Predicted_Price'].pct_change()\nX_test['Predicted_Signal'] = (X_test['Predicted_Returns'].shift(-1) &gt; 0).astype(int)\nX_test['Strategy_Returns'] = X_test['Returns'] * X_test['Predicted_Signal'].shift(1)\ncumulative_strategy_returns = (X_test['Strategy_Returns'].fillna(0) + 1).cumprod()\nreturns = X_test.loc[X_test.index, 'Returns']\nreturns.iloc[0] = 0\ncumulative_stock_returns = (returns + 1).cumprod()\naccuracy = (X_test['Actual_Signal'] == X_test['Predicted_Signal']).mean()\nprint(f'Accuracy: {accuracy}')\n# Calculate cumulative returns for the market\n#market_data['Returns'] = market_data['Close'].pct_change()\n#cumulative_market_returns = (market_data['Returns'].fillna(0) + 1).cumprod()\n\nplt.figure(figsize=(10,5))\n\nplt.plot(cumulative_strategy_returns, label='Strategy Returns')\n#plt.plot(cumulative_market_returns, label='Market Returns')\nplt.plot(cumulative_stock_returns, label='Stock Returns')\nplt.legend();\n\nAccuracy: 0.9473684210526315\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10,5))\nplt.plot(X_test['Predicted_Price'], label='Predicted Price')\nplt.plot(X_test['Close'], label='Actual Price')\nplt.legend();"
  },
  {
    "objectID": "posts/final-project/prelim-analysis.html",
    "href": "posts/final-project/prelim-analysis.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "#Importing the data and storing it in variable df_all\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\ndf_all = pd.read_csv(\"crox-early-data.csv\")\n\ndf_all = df_all.dropna()\n\ndf_train, df_test = train_test_split(df_all, test_size=0.3, random_state=1)\nX_train = df_train.drop(columns=['Close Higher'])\ny_train = df_train['Close Higher']\n\nX_test = df_test.drop(columns=['Close Higher'])\ny_test = df_test['Close Higher']\n\nX_train.head()\n\n\ndf_all.groupby(\"Close Higher\").mean(numeric_only=True)\n\n\n#Choosing features\nfrom sklearn.linear_model import LogisticRegression\n\nLR = LogisticRegression()\n\ncols = [\"Previous GT\", \"TTM PE\"]\n\nLR.fit(X_train[cols], y_train)\nLR.score(X_train[cols], y_train)\nLR.coef_[0]\n\n\ndef linear_score(w, x0, x1):\n    return w[0]*x0 + w[1]*x1\n\n\n#Predict makes binary predictions for data using a supplied score function with weights w and a supplied threshold. Taken from lecture notes from week 2.\n#We begin with a 0 threshold but later on test others to find an optimal threshold\n\nt = 0\n\ndef predict(score_fun, w, threshold, df):\n    \"\"\"\n    make binary predictions for data df using a supplied score function with weights w and supplied threshold. \n    \"\"\"\n    scores = score_fun(w, df[\"Previous GT\"], df[\"TTM PE\"])\n    return 1*(scores &gt; threshold)\n\ndf_train[\"decision\"] = predict(linear_score, LR.coef_[0], t, df_all)\n(df_train[\"decision\"] == df_train[\"Close Higher\"]).mean()\n\n\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\niterations = 200\npredictions = []\nfor i in range(iterations):\n    threshold = (-iterations/2)+(i)\n    df_train[\"decision\"] = predict(linear_score, LR.coef_[0], threshold, df_train)\n    predictions.append((threshold, (df_train[\"decision\"] == df_train[\"Close Higher\"]).mean()))\n\n\npredictions_df = pd.DataFrame(data=predictions)\npredictions_df.columns =['Threshold', 'Accuracy']\n\nsns.relplot(data=predictions_df, x=\"Threshold\", y=\"Accuracy\")\n\nt = predictions_df['Threshold'][predictions_df['Accuracy'].idxmax()]\n\npredictions_df['Threshold'][predictions_df['Accuracy'].idxmax()], predictions_df['Accuracy'].max()\n\n\ndf_test[\"decision\"] = predict(linear_score, LR.coef_[0], t, df_test)\n(df_test[\"decision\"] == df_test[\"Close Higher\"]).mean()"
  },
  {
    "objectID": "posts/kernel-machines/kernel-machines.html",
    "href": "posts/kernel-machines/kernel-machines.html",
    "title": "Sparse Kernel Machines",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom kernel import KernelLogisticRegression\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')"
  },
  {
    "objectID": "posts/kernel-machines/kernel-machines.html#introduction",
    "href": "posts/kernel-machines/kernel-machines.html#introduction",
    "title": "Sparse Kernel Machines",
    "section": "Introduction",
    "text": "Introduction"
  },
  {
    "objectID": "posts/kernel-machines/kernel-machines.html#part-a-implementation",
    "href": "posts/kernel-machines/kernel-machines.html#part-a-implementation",
    "title": "Sparse Kernel Machines",
    "section": "Part A: Implementation",
    "text": "Part A: Implementation\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    # X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    X = X - X.mean(dim = 0, keepdim = True)\n    return X, y\n\n\ndef plot_classification_data(X, y, ax):\n    assert X.shape[1] == 2, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -1, vmax = 2, alpha = 0.8, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nX, y = classification_data(n_points = 10, noise = 0.4)\nplot_classification_data(X, y, ax)\n\n\n\n\n\n\n\n\n\ndef rbf_kernel(X_1, X_2, gamma):\n    return torch.exp(-gamma*torch.cdist(X_1, X_2)**2)\n\nKR = KernelLogisticRegression(rbf_kernel, lam = 0.1, gamma = 0.1)\nKR.fit(X, y, m_epochs = 1000, lr = 0.0001)\n\ntorch.Size([10])\ntorch.Size([10])\n\n\nRuntimeError: The size of tensor a (10) must match the size of tensor b (2) at non-singleton dimension 0\n\n\n\n\n\nNameError: name 'plt' is not defined"
  },
  {
    "objectID": "posts/kernel-machines/kernel-machines.html#part-b-experiments",
    "href": "posts/kernel-machines/kernel-machines.html#part-b-experiments",
    "title": "Sparse Kernel Machines",
    "section": "Part B: Experiments",
    "text": "Part B: Experiments"
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/newton-method/newton-method.html",
    "href": "posts/newton-method/newton-method.html",
    "title": "Newton’s Method for Logistic Regression",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom newton import LogisticRegression, GradientDescentOptimizer, NewtonOptimizer\nimport torch\nfrom matplotlib import pyplot as plt\nLink to newton.py in Github: https://github.com/jjr2024/github_jjr2024.github.io/blob/main/posts/newton-method/newton.py"
  },
  {
    "objectID": "posts/newton-method/newton-method.html#introduction",
    "href": "posts/newton-method/newton-method.html#introduction",
    "title": "Newton’s Method for Logistic Regression",
    "section": "Introduction",
    "text": "Introduction\nIn this blog post, I implement Newton’s method, an optimizer technique that’s an alternative to standard gradient descent. I run three experiments and find that Newton’s method works best with low numbers of observations, high noise, and relatively low alphas (learning rates). Then I consider the computational cost of Newton’s method vs. gradient descent and consider the impact of \\(p\\), the number of features, on the comparison between the two methods."
  },
  {
    "objectID": "posts/newton-method/newton-method.html#part-a-implement-newtonoptimizer",
    "href": "posts/newton-method/newton-method.html#part-a-implement-newtonoptimizer",
    "title": "Newton’s Method for Logistic Regression",
    "section": "Part A: Implement NewtonOptimizer",
    "text": "Part A: Implement NewtonOptimizer\nIn this section, we define functions that we will use in our experiments. classification_data generates data for a classication problem: n_points controls the number of observations generated. Noise determines the difficulty of the problem. p_dims controls the number of features.\nTrain() is a function to train our LR model. We can control the maximum number of iterations (max_iterations) and the learning rate (a).\nplot_perceptron_data and draw_line allow us to plot our data and our weight vector respectively to visualize our results.\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\n\nLR = LogisticRegression() \nopt = NewtonOptimizer(LR)\n\nloss_vec = []\n\ndef train(X, y, max_iterations = 1000, a = 0.1):\n    for _ in range(max_iterations):\n        loss = LR.loss(X, y) \n        loss_vec.append(loss)\n        opt.step(X, y, a)\n\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)"
  },
  {
    "objectID": "posts/newton-method/newton-method.html#part-b-perform-experiments",
    "href": "posts/newton-method/newton-method.html#part-b-perform-experiments",
    "title": "Newton’s Method for Logistic Regression",
    "section": "Part B: Perform Experiments",
    "text": "Part B: Perform Experiments\n\nExperiment 1: Appropriate Alpha -&gt; Converge to Correct Weight Vector\nIn the below code, we generate our classification data and calculate the loss vector when we use our Newton optimizer. The last item in the loss vector is near-zero.\n\nLR = LogisticRegression() \nopt = NewtonOptimizer(LR)\n\nloss_vec = []\n\nX, y = classification_data(n_points = 100, noise = 0.1, p_dims = 2)\n\ntrain(X,y,1000,a=0.9)\n\nprint(\"Last Loss: \" + str(loss_vec[len(loss_vec)-1]))\n\nLast Loss: tensor(6.9629e-05)\n\n\nIn the below code, we save the loss vector we just computed into loss_vec2 and then compute, on the same classification data, the loss vector when we use standard gradient descent. Then we visualize both loss vectors.\n\nLR2 = LR\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nloss_vec2 = loss_vec\nloss_vec = []\n\n#Standard gradient descent\nfor _ in range(1000):\n    loss = LR.loss(X, y) \n    loss_vec.append(loss)\n    opt.step(X, y, 0.9, 0.0)\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.plot(loss_vec2, color = \"blue\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec2)), loss_vec2, color = \"blue\")\nlabs = plt.gca().set(xlabel = \"Iteration (Updates Only)\", ylabel = \"Loss\")\n\n\n\n\n\n\n\n\nThere are two key takeaways for the above graph of the loss vectors.\nFirst, we see that with alpha = 0.9, we eventually converge to a weight vector that produces a near-zero loss. Second, while the blue line (loss vector with Newton’s Method) and the grey line (loss vector with standard gradient descent) have marked differences in early iterations, they eventually converge to a zero/near-zero loss.\n\nfig, ax = plt.subplots(1, 1)\nplot_perceptron_data(X, y, ax)\n\ndraw_line(LR.w, 0, 1, ax, color = \"black\", linestyle = \"dashed\", label = \"weight vector w\")\ndraw_line(LR2.w, 0, 1, ax, color = \"blue\", linestyle = \"dashed\", label = \"weight vector w\")\n\n\n\n\n\n\n\n\nWe see above that standard gradient descent (black line) and Newton’s method (blue line) have converged to very similar weight vectors. Both successfully separate our data into two categories.\n\n\nExperiment 2: Faster Convergence\nBelow, we run an experiment comparing standard gradient descent to the Newton’s method in a context in which the latter has far faster convergence than the former. We set the number of observations to be very low (6) and the noise to be high (1).\n\nX, y = classification_data(n_points = 6, noise = 1, p_dims = 2)\n\nLR = LogisticRegression() \nopt = NewtonOptimizer(LR)\n\nloss_vec = []\n\nmax_iterations = 100\ntrain(X,y,max_iterations,a=0.5)\nloss_vec2 = loss_vec\nLR2 = LR\nprint(\"Last Loss: \" + str(loss_vec[len(loss_vec)-1]))\n\nLast Loss: tensor(nan)\n\n\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nloss_vec = []\n\n#Standard gradient descent\nfor _ in range(max_iterations):\n    loss = LR.loss(X, y) \n    loss_vec.append(loss)\n    opt.step(X, y, 0.5, 0.0)\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.plot(loss_vec2, color = \"blue\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec2)), loss_vec2, color = \"blue\")\nlabs = plt.gca().set(xlabel = \"Iteration (Updates Only)\", ylabel = \"Loss\")\n\n\n\n\n\n\n\n\nNewton’s method (blue line) converges far faster than standard gradient descent (grey line) when we have a very small number of data points and very high noise.\n\n\nExperiment 3: Failure to Converge\nWhen alpha is too high, Newton’s method fails to converge. In the below experiment, we create classification data with just 6 observations and set alpha = 15.\n\nX, y = classification_data(n_points = 6, noise = 0.3, p_dims = 2)\n\nLR = LogisticRegression() \nopt = NewtonOptimizer(LR)\n\nloss_vec = []\nalpha = 15\n\nmax_iterations = 100\nfor _ in range(max_iterations):\n    loss = LR.loss(X, y) \n    loss_vec.append(loss)\n    opt.step(X, y, alpha)\n    \n\nprint(\"Matrix Inverse of Hessian: \" + str(torch.linalg.inv(LR.hessian(X))))\nprint(\"Final Weights: \" + str(LR.w))\nprint(\"Sigmoid(Scores): \" + str(torch.sigmoid(LR.score(X))))\nprint(\"Log of One Less Sigmoid(Scores): \" + str(torch.log(1 - torch.sigmoid(LR.score(X)))))\nprint(\"Last Loss: \" + str(loss_vec[len(loss_vec)-1]))\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Iteration (Updates Only)\", ylabel = \"Loss\")\n\n_LinAlgError: linalg.inv: The diagonal element 1 is zero, the inversion could not be completed because the input matrix is singular.\n\n\nWith a high alpha relative to the number of datapoints, we get a strange loss vector and fail to converge. Above the chart is a series of print statements illustrating the problem. Our matrix inverse of the hessian ends up with values with extremely high absolute values because, multiplied by a high alpha, these values are embedded into the weights, which become increasingly large with each iteration.\nThe weights are enormous, leading to extremely high scores, which translate into extremely negative values when we calculate the log of (1 - sigmoid(scores)). Those extremely negative values are treated by Python as negative infinity. By attempting to run calculations with these -inf values, we end up with NaNs."
  },
  {
    "objectID": "posts/newton-method/newton-method.html#part-c-operation-counting",
    "href": "posts/newton-method/newton-method.html#part-c-operation-counting",
    "title": "Newton’s Method for Logistic Regression",
    "section": "Part C: Operation Counting",
    "text": "Part C: Operation Counting\n\nProblem\nWe are given the following:\n\nComputational units for each calculation\n\n\\(L = c\\)\n\\(\\nabla L = 2c\\)\n\\(Hessian = pc\\)\nInvert a \\(p \\times p\\) matrix \\(= k_1p^\\gamma\\)\nNewton’s method matrix-vector multiplication \\(= k_2p^2\\)\n\nNewton’s method converges in \\(t_{nm}\\) steps\nGradient descent converges in \\(t_{gd}\\) steps\n\nWe want to know how much smaller \\(t_{nm}\\) needs to be than \\(t_{gd}\\) for Newton’s method to require fewer computational units to complete. We also want to know if using Newton’s method ever pays off if \\(p\\) is very large.\n\n\nSolution\nFirst we set up a very abstract equation. Then we unravel the abstracted components of the equation.\n\nTotal computational cost of Newton’s Method \\(=\\) Total computational cost of gradient descent\n\nFor both sides of the equation, total computational cost = number of iterations (i.e., steps) times cost per iteration.\n\n\\(t_{nm} (c + 2c + pc + k_1p^\\gamma + k_2p^2) = t_{gd} (c + 2c)\\)\n\\(t_{nm} (3c + pc + k_1p^\\gamma + k_2p^2) = t_{gd} (3c)\\)\n\\(t_{nm} = t_{gd}*\\frac{3c}{3c + pc + k_1p^\\gamma + k_2p^2}\\)\n\nWith this equation, we know \\(t_{nm}\\) must be \\(\\frac{3c}{3c + pc + k_1p^\\gamma + k_2p^2}\\) of \\(t_{gd}\\) for the computational costs to be equal. So \\(t_{nm}\\) needs to be less than \\(\\frac{3c}{3c + pc + k_1p^\\gamma + k_2p^2}\\) of \\(t_{gd}\\) for Newton’s method to be computationally cheaper than gradient descent.\nThe fraction shows that as p increases in size, the denominator of the right-hand-side fraction in step 4 increases. So the entire fraction shrinks in value. In other words, \\(t_{nm}\\) must become an even smaller percentage of \\(t_{gd}\\). Therefore, when \\(p\\) becomes very large, it seems unlikely that using Newton’s method will pay off."
  },
  {
    "objectID": "posts/newton-method/newton-method.html#part-d-writing",
    "href": "posts/newton-method/newton-method.html#part-d-writing",
    "title": "Newton’s Method for Logistic Regression",
    "section": "Part D: Writing",
    "text": "Part D: Writing\nIn this post, I found Newton’s method was a computationally expensive but faster (in some cases) alternative to gradient descent. The number of observations, the difficulty of the classification problem (noise), and the learning rate all impacted the relative performance of Newton’s method vs. gradient descent.\nI continued to grapple with linear algebra and programming implementations of it in this post. Implementing Newton’s method wasn’t painless, but it went about as smoothly as implementing logistic regression. The similarity of this post with perceptron and logistic regression implementation helped significantly in making that happen. I gained more familiarity with analyzing the components of my model via the analysis in experiment 3. It helped significantly with my understanding of this model to break down, step-by-step, why the loss became NaN."
  }
]